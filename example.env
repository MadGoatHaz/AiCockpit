# --- AiCockpit (ACP) Backend Environment Configuration ---
# Copy this file to .env and modify it with your local settings.
# Lines starting with # are comments.

# --- General Application Settings ---
# DEBUG_MODE=True # Set to True for development (enables more logging, reload) or False for production.
# APP_NAME="AiCockpit Backend" # Usually not changed from config.py
# APP_VERSION="0.1.0-alpha"  # Usually not changed from config.py
# APP_PORT=8000 # Port for Uvicorn to listen on if running main.py directly

# --- Module Enable/Disable Flags ---
# Set to True or False to enable/disable specific modules.
# ENABLE_LLM_MODULE=True
# ENABLE_AGENT_MODULE=True
# ENABLE_WORK_SESSION_MODULE=True
# ENABLE_WORK_BOARD_MODULE=True
# ENABLE_SYSTEM_MODULE=True

# --- Paths ---
# It's highly recommended to use absolute paths for these directories in your .env file,
# especially if running ACP from different locations or as a service.
# Replace "/path/to/..." with actual absolute paths on your system.
# If using relative paths, they will be resolved relative to the current working
# directory where the ACP backend process is started.
#
# Default paths (if not overridden here) are in ~/.acp/
# To override, uncomment and set your desired paths:
#
# MODELS_DIR="/path/to/your/llm_models"              # Directory where GGUF models are stored
# WORK_SESSIONS_DIR="/path/to/your/acp_work_sessions"  # Directory to store work session data
# LOG_DIR="/path/to/your/acp_logs"                     # Directory for application log files
# TEMP_DIR="/path/to/your/acp_temp"                    # Directory for temporary files

# --- LLM Service Backend Configuration (Llama.cpp) ---
# LLM_BACKEND_TYPE="llama_cpp" # Currently "llama_cpp" or "pie" (pie is placeholder)

# Optional: Default GGUF model path if no model_path or model_id is specified in API load requests.
# Often, you'll specify the model per API call, so this can be left blank.
# LLAMA_CPP_MODEL_PATH="" # e.g., "/path/to/your/llm_models/your_default_model.gguf"

# Llama.cpp specific parameters (these are defaults from config.py, override as needed)
# LLAMA_CPP_N_GPU_LAYERS=0    # Number of layers to offload to GPU (0 for CPU, -1 for all if possible)
# LLAMA_CPP_N_CTX=2048        # Context size
# LLAMA_CPP_N_BATCH=512       # Batch size for prompt processing
# LLAMA_CPP_CHAT_FORMAT="llama-2" # Chat format (e.g., "llama-2", "chatml", "mistral-instruct", "vicuna"). Refer to llama-cpp-python docs for more.
# LLAMA_CPP_VERBOSE=False     # Enable verbose logging from llama.cpp

# --- Agent Configuration ---
# SMOLAGENTS_DEFAULT_MODEL="default_llm" # Placeholder, maps to a loaded LLM ID

# --- API Keys (Illustrative - DO NOT COMMIT ACTUAL KEYS TO VERSION CONTROL) ---
# If any integrated tools require API keys, they would be configured here.
# For example, if you had a web search tool that uses Serper API:
# SERPER_API_KEY="your_serper_api_key_here"

# Add any other environment-specific variables your application might need.
