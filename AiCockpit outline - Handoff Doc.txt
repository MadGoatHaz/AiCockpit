AiCockpit outline - Handoff Doc

Ok, I got really far with a project in an AI chat and it was lost. I have all of my code, but the discussion and history and outline of the project was not updated before the chat was lost. I am going to upload you the raw project and I want us to work together to rebuild the project outline from understanding the code and any questions you would have for me to fill in the blanks. ----  

g@g-X12ULTRA:~/Ai/AiCockpit$ ls -R
.:
acp_backend      install_acp.sh  project.toml    temp
acp_install.log  logs            pyproject.toml  tests
example.env      pdm.lock        README.md       work_sessions

./acp_backend:
config.py  __init__.py   main.py  __pycache__
core       llm_backends  models   routers

./acp_backend/core:
agent_executor.py  __init__.py     __pycache__         test_session_handler.py
fs_manager.py      llm_manager.py  session_handler.py

./acp_backend/core/__pycache__:
fs_manager.cpython-313.pyc  session_handler.cpython-313.pyc
__init__.cpython-313.pyc

./acp_backend/llm_backends:
base.py  __init__.py  llama_cpp.py  pie.py

./acp_backend/models:
agent_models.py  __init__.py    __pycache__           work_session_models.py
common.py        llm_models.py  work_board_models.py

./acp_backend/models/__pycache__:
agent_models.cpython-313.pyc  llm_models.cpython-313.pyc
common.cpython-313.pyc        work_board_models.cpython-313.pyc
__init__.cpython-313.pyc      work_session_models.cpython-313.pyc

./acp_backend/__pycache__:
config.cpython-313.pyc  __init__.cpython-313.pyc

./acp_backend/routers:
agents.py    llm_service.py  work_board.py
__init__.py  system.py       work_sessions.py

./logs:

./temp:

./tests:
e2e  __init__.py  integration  __pycache__  test_syspath.py  unit

./tests/e2e:
__init__.py

./tests/integration:
__init__.py

./tests/__pycache__:
__init__.cpython-313.pyc  test_syspath.cpython-313-pytest-8.3.5.pyc

./tests/unit:
core  __init__.py  models  __pycache__  test_work_session_models.py

./tests/unit/core:
__init__.py  __pycache__  test_fs_manager.py  test_session_handler.py

./tests/unit/core/__pycache__:
__init__.cpython-313.pyc
test_fs_manager.cpython-313-pytest-8.3.5.pyc
test_session_handler.cpython-313-pytest-8.3.5.pyc

./tests/unit/models:
__init__.py  test_agent_models.py   test_llm_models.py
__pycache__  test_common_models.py  test_work_board_models.py

./tests/unit/models/__pycache__:
__init__.cpython-313.pyc
test_agent_models.cpython-313-pytest-8.3.5.pyc
test_common_models.cpython-313-pytest-8.3.5.pyc
test_llm_models.cpython-313-pytest-8.3.5.pyc
test_work_board_models.cpython-313-pytest-8.3.5.pyc

./tests/unit/__pycache__:
__init__.cpython-313.pyc  test_work_session_models.cpython-313-pytest-8.3.5.pyc

./work_sessions:


-------

example.env

# --- AiCockpit (ACP) Backend Environment Configuration ---
# Copy this file to .env and modify it with your local settings.
# Lines starting with # are comments.

# --- General Application Settings ---
# DEBUG_MODE=True # Set to True for development (enables more logging, reload) or False for production.
# APP_NAME="AiCockpit Backend" # Usually not changed from config.py
# APP_VERSION="0.1.0-alpha"  # Usually not changed from config.py
# APP_PORT=8000 # Port for Uvicorn to listen on if running main.py directly

# --- Module Enable/Disable Flags ---
# Set to True or False to enable/disable specific modules.
# ENABLE_LLM_MODULE=True
# ENABLE_AGENT_MODULE=True
# ENABLE_WORK_SESSION_MODULE=True
# ENABLE_WORK_BOARD_MODULE=True
# ENABLE_SYSTEM_MODULE=True

# --- Paths ---
# It's highly recommended to use absolute paths for these directories in your .env file,
# especially if running ACP from different locations or as a service.
# Replace "/path/to/..." with actual absolute paths on your system.
# If using relative paths, they will be resolved relative to the current working
# directory where the ACP backend process is started.
#
# Default paths (if not overridden here) are in ~/.acp/
# To override, uncomment and set your desired paths:
#
# MODELS_DIR="/path/to/your/llm_models"              # Directory where GGUF models are stored
# WORK_SESSIONS_DIR="/path/to/your/acp_work_sessions"  # Directory to store work session data
# LOG_DIR="/path/to/your/acp_logs"                     # Directory for application log files
# TEMP_DIR="/path/to/your/acp_temp"                    # Directory for temporary files

# --- LLM Service Backend Configuration (Llama.cpp) ---
# LLM_BACKEND_TYPE="llama_cpp" # Currently "llama_cpp" or "pie" (pie is placeholder)

# Optional: Default GGUF model path if no model_path or model_id is specified in API load requests.
# Often, you'll specify the model per API call, so this can be left blank.
# LLAMA_CPP_MODEL_PATH="" # e.g., "/path/to/your/llm_models/your_default_model.gguf"

# Llama.cpp specific parameters (these are defaults from config.py, override as needed)
# LLAMA_CPP_N_GPU_LAYERS=0    # Number of layers to offload to GPU (0 for CPU, -1 for all if possible)
# LLAMA_CPP_N_CTX=2048        # Context size
# LLAMA_CPP_N_BATCH=512       # Batch size for prompt processing
# LLAMA_CPP_CHAT_FORMAT="llama-2" # Chat format (e.g., "llama-2", "chatml", "mistral-instruct", "vicuna"). Refer to llama-cpp-python docs for more.
# LLAMA_CPP_VERBOSE=False     # Enable verbose logging from llama.cpp

# --- Agent Configuration ---
# SMOLAGENTS_DEFAULT_MODEL="default_llm" # Placeholder, maps to a loaded LLM ID

# --- API Keys (Illustrative - DO NOT COMMIT ACTUAL KEYS TO VERSION CONTROL) ---
# If any integrated tools require API keys, they would be configured here.
# For example, if you had a web search tool that uses Serper API:
# SERPER_API_KEY="your_serper_api_key_here"

# Add any other environment-specific variables your application might need.
------

install_acp.sh

#!/bin/bash

# AiCockpit (ACP) Backend Installation Script (Draft v0.3)
# This script automates the setup of the ACP backend on Linux systems.
# It aims for idempotency and improved error handling.

# Exit on error and pipefail
set -e
set -o pipefail

# --- Configuration ---
MIN_PYTHON_VERSION_MAJOR=3
MIN_PYTHON_VERSION_MINOR=10 # Requires Python 3.10+
VENV_DIR=".venv-acp"      # Name of the virtual environment directory
INSTALL_LOG_FILE="acp_install.log"

# --- Helper Functions ---
_log_to_file() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" >> "$INSTALL_LOG_FILE"
}

_print_msg() {
    echo -e "\033[0;32mINSTALLER:\033[0m $1" | tee -a "$INSTALL_LOG_FILE" # Green
}
_print_error() {
    echo -e "\033[0;31mINSTALLER ERROR:\033[0m $1" | tee -a "$INSTALL_LOG_FILE" >&2 # Red
}
_print_warning() {
    echo -e "\033[0;33mINSTALLER WARNING:\033[0m $1" | tee -a "$INSTALL_LOG_FILE" >&2 # Yellow
}
_print_info() {
    echo -e "\033[0;34mINSTALLER INFO:\033[0m $1" | tee -a "$INSTALL_LOG_FILE" # Blue
}

_command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Cleanup function to be called on exit
_cleanup() {
    _log_to_file "Script exiting."
    # Deactivate virtual environment if this script activated it and `deactivate` command exists
    if [ "$SCRIPT_ACTIVATED_VENV" = true ] && _command_exists deactivate; then
        _print_info "Deactivating virtual environment for script session."
        deactivate
    fi
}
trap _cleanup EXIT # Register cleanup function

_check_python_version() {
    _log_to_file "Checking Python version."
    if ! _command_exists python3; then
        _print_error "Python 3 is not installed."
        _log_to_file "Python 3 check failed: not installed."
        return 1
    fi
    local python_major=$(python3 -c 'import sys; print(sys.version_info.major)')
    local python_minor=$(python3 -c 'import sys; print(sys.version_info.minor)')
    if [ "$python_major" -lt "$MIN_PYTHON_VERSION_MAJOR" ] || ([ "$python_major" -eq "$MIN_PYTHON_VERSION_MAJOR" ] && [ "$python_minor" -lt "$MIN_PYTHON_VERSION_MINOR" ]); then
        _print_error "Installed Python version is $python_major.$python_minor. ACP requires Python $MIN_PYTHON_VERSION_MAJOR.$MIN_PYTHON_VERSION_MINOR or higher."
        _log_to_file "Python version check failed: $python_major.$python_minor is less than required $MIN_PYTHON_VERSION_MAJOR.$MIN_PYTHON_VERSION_MINOR."
        return 1
    fi
    _print_msg "Python version $python_major.$python_minor found."
    _log_to_file "Python version check passed: $python_major.$python_minor."
    return 0
}

_prompt_user_yes_no() {
    local prompt_message="$1"
    local default_answer="${2:-N}" # Default to No if not specified
    local answer
    _log_to_file "Prompting user (yes/no): $prompt_message [Default: $default_answer]"
    while true; do
        read -r -p "$prompt_message [y/N]: " answer
        answer="${answer:-$default_answer}"
        case "$answer" in
            [Yy]* ) _log_to_file "User answered yes."; return 0;; # Yes
            [Nn]* ) _log_to_file "User answered no."; return 1;; # No
            * ) echo "Please answer yes (y) or no (n).";;
        esac
    done
}

_prompt_user_input() {
    local prompt_message="$1"
    local default_value="$2"
    local user_input
    _log_to_file "Prompting user for input: $prompt_message [Default: $default_value]"
    read -r -p "$prompt_message [$default_value]: " user_input
    local result="${user_input:-$default_value}"
    _log_to_file "User input: $result"
    echo "$result"
}

_install_package_manager_pkg() {
    local pkg_name="$1"
    local cmd_to_check="${2:-$pkg_name}" # Command to check for existence
    local apt_pkg_name="${3:-$pkg_name}" # Actual package name for apt
    _log_to_file "Checking system package: $cmd_to_check (apt name: $apt_pkg_name)."

    if ! _command_exists "$cmd_to_check"; then
        _print_warning "$cmd_to_check is not installed."
        if _prompt_user_yes_no "Attempt to install $apt_pkg_name using 'sudo apt install -y $apt_pkg_name'?" "N"; then
            _print_msg "Attempting to install $apt_pkg_name..."
            _log_to_file "User agreed to install $apt_pkg_name via apt."
            # Capture output for logging/debugging
            local install_log_tmp
            install_log_tmp=$(mktemp)
            if sudo apt update > "$install_log_tmp" 2>&1 && sudo apt install -y "$apt_pkg_name" >> "$install_log_tmp" 2>&1; then
                _print_msg "$apt_pkg_name installed successfully."
                _log_to_file "$apt_pkg_name installed via apt. Log: $(cat "$install_log_tmp")"
                rm "$install_log_tmp"
            else
                _print_error "Failed to install $apt_pkg_name. Please check the output below and install it manually."
                _print_error "--- APT Log ---"
                cat "$install_log_tmp" >&2 # Print apt log to stderr
                _print_error "--- End APT Log ---"
                _log_to_file "$apt_pkg_name apt installation failed. Log: $(cat "$install_log_tmp")"
                rm "$install_log_tmp"
                return 1
            fi
        else
            _print_error "$cmd_to_check is required. Please install $apt_pkg_name manually and re-run this script."
            _log_to_file "$cmd_to_check package check failed: User declined apt installation."
            return 1
        fi
    fi
    _print_msg "$cmd_to_check found."
    _log_to_file "$cmd_to_check package check passed."
    return 0
}

_detect_gpu_and_set_cmake_args() {
    _print_info "Detecting GPU for llama-cpp-python compilation..."
    _log_to_file "Starting GPU detection for CMAKE_ARGS."
    local llama_cmake_args=""

    if _command_exists nvidia-smi; then
        _print_msg "NVIDIA GPU detected."
        _log_to_file "NVIDIA GPU detected via nvidia-smi."
        if _prompt_user_yes_no "Attempt to build llama-cpp-python with CUDA support?" "Y"; then
            llama_cmake_args="-DGGML_CUDA=on"
        fi
    elif _command_exists rocm-smi || _command_exists rocminfo; then
        _print_msg "AMD GPU detected."
        _log_to_file "AMD GPU detected via rocm-smi or rocminfo."
        if _prompt_user_yes_no "Attempt to build llama-cpp-python with ROCm/HIP support?" "Y"; then
            llama_cmake_args="-DGGML_HIPBLAS=on -DGGML_ROCM_NO_EVENT=on"
        fi
    elif [[ "$(uname -s)" == "Darwin" ]] && _command_exists system_profiler && system_profiler SPDisplaysDataType 2>/dev/null | grep -q "Metal Family: Supported"; then
        _print_msg "Apple Metal GPU detected."
        _log_to_file "Apple Metal GPU detected."
        if _prompt_user_yes_no "Attempt to build llama-cpp-python with Metal support?" "Y"; then
            llama_cmake_args="-DGGML_METAL=on"
        fi
    else
        _print_warning "No NVIDIA or AMD GPU detected, or Metal not confirmed."
        _log_to_file "No known GPU detected for accelerated build."
    fi

    if [ -z "$llama_cmake_args" ]; then
        _print_msg "Proceeding with CPU-only build for llama-cpp-python."
        _log_to_file "CMAKE_ARGS set for CPU-only build (empty)."
    else
        _print_msg "Using CMAKE_ARGS for GPU: $llama_cmake_args"
        _log_to_file "CMAKE_ARGS set for GPU: $llama_cmake_args."
    fi
    echo "$llama_cmake_args"
}

_update_env_file() {
    local env_file_path="$1"
    local key_to_update="$2"
    local new_value="$3"
    local comment_char="#"
    _log_to_file "Updating .env file '$env_file_path' for key '$key_to_update' with value '$new_value'."

    local escaped_new_value
    escaped_new_value=$(printf '%s\n' "$new_value" | sed 's:[][\\/.^$*]:\\&:g') # Escape for sed

    if grep -q "^\s*${comment_char}\?\s*${key_to_update}=" "$env_file_path"; then
        sed -i -E "s|^\s*${comment_char}?\s*(${key_to_update}=).*|\1\"${escaped_new_value}\"|" "$env_file_path"
        _print_msg "Updated $key_to_update in $env_file_path"
    else
        echo "${key_to_update}=\"${new_value}\"" >> "$env_file_path"
        _print_msg "Added $key_to_update to $env_file_path"
    fi
    _log_to_file ".env update for '$key_to_update' complete."
}

# --- Main Installation Logic ---
main() {
    echo "" > "$INSTALL_LOG_FILE" # Clear log file at start
    _print_msg "Starting AiCockpit (ACP) Backend Installation (v0.3)..."
    _log_to_file "Installation script started."
    echo

    PROJECT_ROOT=$(pwd)
    _print_msg "Project root: $PROJECT_ROOT"
    _log_to_file "Project root set to $PROJECT_ROOT."

    local force_reinstall_deps=false
    local reconfigure_env=false

    if [ -d "$VENV_DIR" ]; then
        _print_warning "Existing virtual environment '$VENV_DIR' detected."
        if _prompt_user_yes_no "Do you want to (U)pdate dependencies or (F)orce a full reinstall of dependencies?" "U"; then
            # User chose Update or Force
             if [[ $(echo "$REPLY" | tr '[:upper:]' '[:lower:]') == "f" ]]; then # Check if user typed F/f for force
                force_reinstall_deps=true
                _print_info "Forcing reinstallation of dependencies."
             else
                _print_info "Will attempt to update dependencies."
             fi
        else
            _print_info "Skipping dependency modification for existing venv."
        fi
        if _prompt_user_yes_no "Do you want to (R)econfigure .env paths?" "N"; then
            reconfigure_env=true
        fi
    fi


    # --- 1. Prerequisite Checks ---
    _print_msg "--- Step 1: Checking Prerequisites ---"
    if ! _check_python_version; then exit 1; fi
    if ! _install_package_manager_pkg "pip3" "pip3" "python3-pip"; then exit 1; fi
    if ! _install_package_manager_pkg "venv" "python3-venv" "python3-venv"; then exit 1; fi # Check for venv module
    _install_package_manager_pkg "git" "git" "git" # Optional
    _install_package_manager_pkg "cmake" "cmake" "cmake" # Optional for llama.cpp
    _install_package_manager_pkg "g++" "g++ C++ compiler" "g++" # Optional for llama.cpp

    # --- 2. Setup Python Virtual Environment ---
    _print_msg "--- Step 2: Setting up Python Virtual Environment ---"
    if [ "$force_reinstall_deps" = true ] && [ -d "$VENV_DIR" ]; then
        _print_warning "Force reinstall selected. Removing existing virtual environment '$VENV_DIR'."
        rm -rf "$VENV_DIR"
        _log_to_file "Removed existing venv due to force reinstall."
    fi

    if [ -d "$VENV_DIR" ]; then
        _print_msg "Virtual environment '$VENV_DIR' already exists. Using it."
    else
        _print_msg "Creating virtual environment in '$VENV_DIR'..."
        if ! python3 -m venv "$VENV_DIR"; then
            _print_error "Failed to create virtual environment."
            _log_to_file "Venv creation failed."
            exit 1
        fi
        _print_msg "Virtual environment created."
        _log_to_file "Venv created at $VENV_DIR."
    fi
    
    _print_msg "Activating virtual environment for installation..."
    # shellcheck source=/dev/null
    source "$VENV_DIR/bin/activate" || { _print_error "Failed to activate virtual environment."; exit 1; }
    SCRIPT_ACTIVATED_VENV=true # Flag for cleanup
    _log_to_file "Venv activated for script session."


    # --- 3. Install/Update PDM and Project Dependencies ---
    _print_msg "--- Step 3: Installing Dependencies using PDM ---"
    if ! _command_exists pdm; then
        _print_msg "PDM not found. Installing PDM via pip into the virtual environment..."
        if ! python -m pip install "pdm>=1.14"; then # Changed from pdm>=2 to pdm>=1.14 as per original, assuming 1.14 is a typo and meant 2.x or a specific 1.x
             _print_error "Failed to install PDM. Please try: python -m pip install pdm"
             _log_to_file "PDM installation failed."
             exit 1
        fi
        _log_to_file "PDM installed via pip."
    fi
    _print_msg "PDM found: $(pdm --version)"
    _log_to_file "PDM version: $(pdm --version)."

    if [ ! -f "pyproject.toml" ]; then
        _print_error "pyproject.toml not found in $PROJECT_ROOT. Run script from project root."
        _log_to_file "pyproject.toml not found."
        exit 1
    fi

    LLAMA_CPP_CMAKE_ARGS=""
    if [ "$force_reinstall_deps" = true ] || ! pdm list --graph | grep -q "llama-cpp-python"; then # Check if llama-cpp-python is already installed
        LLAMA_CPP_CMAKE_ARGS=$(_detect_gpu_and_set_cmake_args)
    else
        _print_info "llama-cpp-python seems to be installed. To change its build options (e.g., GPU), use force reinstall."
        _log_to_file "Skipping GPU detection for llama-cpp-python as it seems installed."
    fi
    
    _print_msg "Attempting to install/update project dependencies with PDM..."
    _print_info "This may take some time, especially for llama-cpp-python compilation."
    
    PDM_INSTALL_CMD="pdm install --no-self" # --no-self is for older PDM, newer is just `pdm install` or `pdm sync`
    # For modern PDM, `pdm sync` is often preferred to strictly sync with lock file.
    # `pdm install` might try to update pdm.lock if pyproject.toml changed.
    # Assuming current PDM version where `pdm install` is the standard sync/install command.

    if [ "$force_reinstall_deps" = true ]; then
        _print_info "Forcing reinstallation of dependencies (PDM will sync/clean)."
        # PDM handles this with `pdm sync --clean` or `pdm install --clean` if available,
        # or by simply re-running `pdm install` after venv removal.
        # The venv removal earlier handles the "clean" part for force_reinstall_deps.
    fi

    if ! env CMAKE_ARGS="$LLAMA_CPP_CMAKE_ARGS" $PDM_INSTALL_CMD; then
        _print_error "Failed to install/update project dependencies with PDM (GPU attempt: $LLAMA_CPP_CMAKE_ARGS)."
        _log_to_file "PDM install failed with CMAKE_ARGS='$LLAMA_CPP_CMAKE_ARGS'."
        if [ -n "$LLAMA_CPP_CMAKE_ARGS" ]; then
            _print_warning "GPU build for llama-cpp-python failed."
            if _prompt_user_yes_no "Attempt a CPU-only build of llama-cpp-python?" "Y"; then
                _log_to_file "User opted for CPU fallback for llama-cpp-python."
                _print_msg "Attempting CPU-only build using '$PDM_INSTALL_CMD'..."
                if ! $PDM_INSTALL_CMD; then # No CMAKE_ARGS for default CPU
                    _print_error "CPU-only build also failed. Please check PDM/pip output for errors."
                    _log_to_file "PDM install failed (CPU fallback)."
                    exit 1
                fi
            else
                _print_error "Installation aborted by user after GPU build failure."
                _log_to_file "User aborted after GPU build failure."
                exit 1
            fi
        else
            _print_error "Installation failed. Please check PDM/pip output for errors."
            _log_to_file "PDM install failed (initial attempt, no specific CMAKE_ARGS)."
            exit 1
        fi
    fi
    _print_msg "Project dependencies installed/updated successfully."
    _log_to_file "PDM install successful."

    # --- 4. Create/Update .env Configuration File ---
    _print_msg "--- Step 4: Configuring Environment (.env file) ---"
    if [ ! -f ".env" ]; then
        if [ -f "example.env" ]; then
            _print_msg "Copying example.env to .env..."
            cp "example.env" ".env"
            _log_to_file "Copied example.env to .env."
        else
            _print_warning "example.env not found. Creating a minimal .env."
            touch ".env" # Create empty .env
            _log_to_file "Created empty .env as example.env was missing."
        fi
        reconfigure_env=true # Force path configuration for new .env
    elif [ "$reconfigure_env" = false ]; then # Only prompt if not already forced by new venv or user choice
         _print_msg ".env file already exists."
         if ! _prompt_user_yes_no "Do you want to review/update directory paths in .env?" "N"; then
            reconfigure_env=false # User chose not to reconfigure
         else
            reconfigure_env=true
         fi
    fi
    
    if [ "$reconfigure_env" = true ]; then
        _print_info "Please provide paths for ACP directories. Press Enter for defaults shown."
        PYTHON_IN_VENV="$VENV_DIR/bin/python"

        # Ensure acp_backend.config can be imported to get defaults.
        # This requires dependencies to be installed.
        DEFAULT_MODELS_DIR=$($PYTHON_IN_VENV -c "from acp_backend.config import settings; print(settings.MODELS_DIR)")
        DEFAULT_SESSIONS_DIR=$($PYTHON_IN_VENV -c "from acp_backend.config import settings; print(settings.WORK_SESSIONS_DIR)")
        DEFAULT_LOG_DIR=$($PYTHON_IN_VENV -c "from acp_backend.config import settings; print(settings.LOG_DIR)")
        DEFAULT_TEMP_DIR=$($PYTHON_IN_VENV -c "from acp_backend.config import settings; print(settings.TEMP_DIR)")
        DEFAULT_APP_PORT=$($PYTHON_IN_VENV -c "from acp_backend.config import settings; print(settings.APP_PORT)")


        USER_MODELS_DIR=$(_prompt_user_input "Path for LLM MODELS_DIR" "$DEFAULT_MODELS_DIR")
        USER_SESSIONS_DIR=$(_prompt_user_input "Path for WORK_SESSIONS_DIR" "$DEFAULT_SESSIONS_DIR")
        USER_LOG_DIR=$(_prompt_user_input "Path for LOG_DIR" "$DEFAULT_LOG_DIR")
        USER_TEMP_DIR=$(_prompt_user_input "Path for TEMP_DIR" "$DEFAULT_TEMP_DIR")
        USER_APP_PORT=$(_prompt_user_input "Port for the application server" "$DEFAULT_APP_PORT")


        _update_env_file ".env" "MODELS_DIR" "$USER_MODELS_DIR"
        _update_env_file ".env" "WORK_SESSIONS_DIR" "$USER_SESSIONS_DIR"
        _update_env_file ".env" "LOG_DIR" "$USER_LOG_DIR"
        _update_env_file ".env" "TEMP_DIR" "$USER_TEMP_DIR"
        _update_env_file ".env" "APP_PORT" "$USER_APP_PORT"

        _print_msg ".env file configured."
        _log_to_file ".env paths configured by user."
    else
        _print_msg "Skipping .env path reconfiguration."
        _log_to_file "Skipped .env path reconfiguration."
    fi

    # --- 5. Create ACP Application Directories (using final settings from .env/defaults) ---
    _print_msg "--- Step 5: Creating Application Directories ---"
    PYTHON_IN_VENV="$VENV_DIR/bin/python" 
    # Reload settings from .env by re-running the config import snippet
    MODELS_DIR_FINAL=$($PYTHON_IN_VENV -c "from acp_backend.config import settings; print(settings.MODELS_DIR)")
    SESSIONS_DIR_FINAL=$($PYTHON_IN_VENV -c "from acp_backend.config import settings; print(settings.WORK_SESSIONS_DIR)")
    LOG_DIR_FINAL=$($PYTHON_IN_VENV -c "from acp_backend.config import settings; print(settings.LOG_DIR)")
    TEMP_DIR_FINAL=$($PYTHON_IN_VENV -c "from acp_backend.config import settings; print(settings.TEMP_DIR)")

    for dir_path_var_name in MODELS_DIR_FINAL SESSIONS_DIR_FINAL LOG_DIR_FINAL TEMP_DIR_FINAL; do
        eval "local dir_path=\$$dir_path_var_name" # Get value of the variable
        if [ -n "$dir_path" ]; then # Check if path is not empty
            if mkdir -p "$dir_path"; then
                _print_msg "Ensured directory exists: $dir_path"
                _log_to_file "Ensured directory: $dir_path"
            else
                _print_error "Failed to create directory: $dir_path. Check permissions."
                _log_to_file "Failed to create directory: $dir_path"
                # Consider if this should be a fatal error
            fi
        else
            _print_warning "Directory path for $dir_path_var_name is not set. Skipping creation."
            _log_to_file "Directory path for $dir_path_var_name not set."
        fi
    done

    # --- Final Instructions ---
    echo
    _print_msg "--- AiCockpit (ACP) Backend Installation/Update Complete! ---"
    _print_msg "Log file for this session: $PROJECT_ROOT/$INSTALL_LOG_FILE"
    _print_msg "To activate the virtual environment in your current shell session, run:"
    _print_msg "  source $PROJECT_ROOT/$VENV_DIR/bin/activate"
    _print_msg "Ensure you have GGUF models in your MODELS_DIR: $MODELS_DIR_FINAL"
    APP_PORT_FINAL=$($PYTHON_IN_VENV -c "from acp_backend.config import settings; print(settings.APP_PORT)")
    _print_msg "To run the application (after activating venv):"
    _print_msg "  pdm run dev  (runs on port 8000 as per script)"
    _print_msg "  or directly: python acp_backend/main.py (runs on port $APP_PORT_FINAL as per .env/defaults)"
    echo
    _log_to_file "Installation script finished successfully."
}

# --- Script Execution ---
SCRIPT_ACTIVATED_VENV=false # Global flag for cleanup
main "$@"
--------

pdm.lock

# This file is @generated by PDM.
# It is not intended for manual editing.

[metadata]
groups = ["default", "dev"]
strategy = ["inherit_metadata"]
lock_version = "4.5.0"
content_hash = "sha256:8b279f31f8d6c91e6bfa03857555463ccc0d040bdfb940dbfba61f79347b6e6b"

[[metadata.targets]]
requires_python = ">=3.10"

[[package]]
name = "aiofiles"
version = "24.1.0"
requires_python = ">=3.8"
summary = "File support for asyncio."
groups = ["default"]
files = [
    {file = "aiofiles-24.1.0-py3-none-any.whl", hash = "sha256:b4ec55f4195e3eb5d7abd1bf7e061763e864dd4954231fb8539a0ef8bb8260e5"},
    {file = "aiofiles-24.1.0.tar.gz", hash = "sha256:22a075c9e5a3810f0c2e48f3008c94d68c65d763b9b03857924c99e57355166c"},
]

[[package]]
name = "annotated-types"
version = "0.7.0"
requires_python = ">=3.8"
summary = "Reusable constraint types to use with typing.Annotated"
groups = ["default"]
dependencies = [
    "typing-extensions>=4.0.0; python_version < \"3.9\"",
]
files = [
    {file = "annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53"},
    {file = "annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89"},
]

[[package]]
name = "anyio"
version = "4.9.0"
requires_python = ">=3.9"
summary = "High level compatibility layer for multiple asynchronous event loop implementations"
groups = ["default"]
dependencies = [
    "exceptiongroup>=1.0.2; python_version < \"3.11\"",
    "idna>=2.8",
    "sniffio>=1.1",
    "typing-extensions>=4.5; python_version < \"3.13\"",
]
files = [
    {file = "anyio-4.9.0-py3-none-any.whl", hash = "sha256:9f76d541cad6e36af7beb62e978876f3b41e3e04f2c1fbf0884604c0a9c4d93c"},
    {file = "anyio-4.9.0.tar.gz", hash = "sha256:673c0c244e15788651a4ff38710fea9675823028a6f08a5eda409e0c9840a028"},
]

[[package]]
name = "black"
version = "25.1.0"
requires_python = ">=3.9"
summary = "The uncompromising code formatter."
groups = ["dev"]
dependencies = [
    "click>=8.0.0",
    "mypy-extensions>=0.4.3",
    "packaging>=22.0",
    "pathspec>=0.9.0",
    "platformdirs>=2",
    "tomli>=1.1.0; python_version < \"3.11\"",
    "typing-extensions>=4.0.1; python_version < \"3.11\"",
]
files = [
    {file = "black-25.1.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:759e7ec1e050a15f89b770cefbf91ebee8917aac5c20483bc2d80a6c3a04df32"},
    {file = "black-25.1.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:0e519ecf93120f34243e6b0054db49c00a35f84f195d5bce7e9f5cfc578fc2da"},
    {file = "black-25.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:055e59b198df7ac0b7efca5ad7ff2516bca343276c466be72eb04a3bcc1f82d7"},
    {file = "black-25.1.0-cp310-cp310-win_amd64.whl", hash = "sha256:db8ea9917d6f8fc62abd90d944920d95e73c83a5ee3383493e35d271aca872e9"},
    {file = "black-25.1.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:a39337598244de4bae26475f77dda852ea00a93bd4c728e09eacd827ec929df0"},
    {file = "black-25.1.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:96c1c7cd856bba8e20094e36e0f948718dc688dba4a9d78c3adde52b9e6c2299"},
    {file = "black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:bce2e264d59c91e52d8000d507eb20a9aca4a778731a08cfff7e5ac4a4bb7096"},
    {file = "black-25.1.0-cp311-cp311-win_amd64.whl", hash = "sha256:172b1dbff09f86ce6f4eb8edf9dede08b1fce58ba194c87d7a4f1a5aa2f5b3c2"},
    {file = "black-25.1.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:4b60580e829091e6f9238c848ea6750efed72140b91b048770b64e74fe04908b"},
    {file = "black-25.1.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:1e2978f6df243b155ef5fa7e558a43037c3079093ed5d10fd84c43900f2d8ecc"},
    {file = "black-25.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:3b48735872ec535027d979e8dcb20bf4f70b5ac75a8ea99f127c106a7d7aba9f"},
    {file = "black-25.1.0-cp312-cp312-win_amd64.whl", hash = "sha256:ea0213189960bda9cf99be5b8c8ce66bb054af5e9e861249cd23471bd7b0b3ba"},
    {file = "black-25.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:8f0b18a02996a836cc9c9c78e5babec10930862827b1b724ddfe98ccf2f2fe4f"},
    {file = "black-25.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:afebb7098bfbc70037a053b91ae8437c3857482d3a690fefc03e9ff7aa9a5fd3"},
    {file = "black-25.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:030b9759066a4ee5e5aca28c3c77f9c64789cdd4de8ac1df642c40b708be6171"},
    {file = "black-25.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:a22f402b410566e2d1c950708c77ebf5ebd5d0d88a6a2e87c86d9fb48afa0d18"},
    {file = "black-25.1.0-py3-none-any.whl", hash = "sha256:95e8176dae143ba9097f351d174fdaf0ccd29efb414b362ae3fd72bf0f710717"},
    {file = "black-25.1.0.tar.gz", hash = "sha256:33496d5cd1222ad73391352b4ae8da15253c5de89b93a80b3e2c8d9a19ec2666"},
]

[[package]]
name = "certifi"
version = "2025.4.26"
requires_python = ">=3.6"
summary = "Python package for providing Mozilla's CA Bundle."
groups = ["default"]
files = [
    {file = "certifi-2025.4.26-py3-none-any.whl", hash = "sha256:30350364dfe371162649852c63336a15c70c6510c2ad5015b21c2345311805f3"},
    {file = "certifi-2025.4.26.tar.gz", hash = "sha256:0a816057ea3cdefcef70270d2c515e4506bbc954f417fa5ade2021213bb8f0c6"},
]

[[package]]
name = "cfgv"
version = "3.4.0"
requires_python = ">=3.8"
summary = "Validate configuration and produce human readable error messages."
groups = ["dev"]
files = [
    {file = "cfgv-3.4.0-py2.py3-none-any.whl", hash = "sha256:b7265b1f29fd3316bfcd2b330d63d024f2bfd8bcb8b0272f8e19a504856c48f9"},
    {file = "cfgv-3.4.0.tar.gz", hash = "sha256:e52591d4c5f5dead8e0f673fb16db7949d2cfb3f7da4582893288f0ded8fe560"},
]

[[package]]
name = "charset-normalizer"
version = "3.4.2"
requires_python = ">=3.7"
summary = "The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet."
groups = ["default"]
files = [
    {file = "charset_normalizer-3.4.2-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:7c48ed483eb946e6c04ccbe02c6b4d1d48e51944b6db70f697e089c193404941"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b2d318c11350e10662026ad0eb71bb51c7812fc8590825304ae0bdd4ac283acd"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:9cbfacf36cb0ec2897ce0ebc5d08ca44213af24265bd56eca54bee7923c48fd6"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:18dd2e350387c87dabe711b86f83c9c78af772c748904d372ade190b5c7c9d4d"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8075c35cd58273fee266c58c0c9b670947c19df5fb98e7b66710e04ad4e9ff86"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:5bf4545e3b962767e5c06fe1738f951f77d27967cb2caa64c28be7c4563e162c"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:7a6ab32f7210554a96cd9e33abe3ddd86732beeafc7a28e9955cdf22ffadbab0"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:b33de11b92e9f75a2b545d6e9b6f37e398d86c3e9e9653c4864eb7e89c5773ef"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-musllinux_1_2_ppc64le.whl", hash = "sha256:8755483f3c00d6c9a77f490c17e6ab0c8729e39e6390328e42521ef175380ae6"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-musllinux_1_2_s390x.whl", hash = "sha256:68a328e5f55ec37c57f19ebb1fdc56a248db2e3e9ad769919a58672958e8f366"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:21b2899062867b0e1fde9b724f8aecb1af14f2778d69aacd1a5a1853a597a5db"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-win32.whl", hash = "sha256:e8082b26888e2f8b36a042a58307d5b917ef2b1cacab921ad3323ef91901c71a"},
    {file = "charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl", hash = "sha256:f69a27e45c43520f5487f27627059b64aaf160415589230992cec34c5e18a509"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:be1e352acbe3c78727a16a455126d9ff83ea2dfdcbc83148d2982305a04714c2"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:aa88ca0b1932e93f2d961bf3addbb2db902198dca337d88c89e1559e066e7645"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d524ba3f1581b35c03cb42beebab4a13e6cdad7b36246bd22541fa585a56cccd"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:28a1005facc94196e1fb3e82a3d442a9d9110b8434fc1ded7a24a2983c9888d8"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fdb20a30fe1175ecabed17cbf7812f7b804b8a315a25f24678bcdf120a90077f"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0f5d9ed7f254402c9e7d35d2f5972c9bbea9040e99cd2861bd77dc68263277c7"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:efd387a49825780ff861998cd959767800d54f8308936b21025326de4b5a42b9"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:f0aa37f3c979cf2546b73e8222bbfa3dc07a641585340179d768068e3455e544"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:e70e990b2137b29dc5564715de1e12701815dacc1d056308e2b17e9095372a82"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:0c8c57f84ccfc871a48a47321cfa49ae1df56cd1d965a09abe84066f6853b9c0"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:6b66f92b17849b85cad91259efc341dce9c1af48e2173bf38a85c6329f1033e5"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-win32.whl", hash = "sha256:daac4765328a919a805fa5e2720f3e94767abd632ae410a9062dff5412bae65a"},
    {file = "charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl", hash = "sha256:e53efc7c7cee4c1e70661e2e112ca46a575f90ed9ae3fef200f2a25e954f4b28"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:0c29de6a1a95f24b9a1aa7aefd27d2487263f00dfd55a77719b530788f75cff7"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cddf7bd982eaa998934a91f69d182aec997c6c468898efe6679af88283b498d3"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:fcbe676a55d7445b22c10967bceaaf0ee69407fbe0ece4d032b6eb8d4565982a"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d41c4d287cfc69060fa91cae9683eacffad989f1a10811995fa309df656ec214"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4e594135de17ab3866138f496755f302b72157d115086d100c3f19370839dd3a"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:cf713fe9a71ef6fd5adf7a79670135081cd4431c2943864757f0fa3a65b1fafd"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:a370b3e078e418187da8c3674eddb9d983ec09445c99a3a263c2011993522981"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:a955b438e62efdf7e0b7b52a64dc5c3396e2634baa62471768a64bc2adb73d5c"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:7222ffd5e4de8e57e03ce2cef95a4c43c98fcb72ad86909abdfc2c17d227fc1b"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:bee093bf902e1d8fc0ac143c88902c3dfc8941f7ea1d6a8dd2bcb786d33db03d"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:dedb8adb91d11846ee08bec4c8236c8549ac721c245678282dcb06b221aab59f"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-win32.whl", hash = "sha256:db4c7bf0e07fc3b7d89ac2a5880a6a8062056801b83ff56d8464b70f65482b6c"},
    {file = "charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl", hash = "sha256:5a9979887252a82fefd3d3ed2a8e3b937a7a809f65dcb1e068b090e165bbe99e"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:926ca93accd5d36ccdabd803392ddc3e03e6d4cd1cf17deff3b989ab8e9dbcf0"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:eba9904b0f38a143592d9fc0e19e2df0fa2e41c3c3745554761c5f6447eedabf"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3fddb7e2c84ac87ac3a947cb4e66d143ca5863ef48e4a5ecb83bd48619e4634e"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:98f862da73774290f251b9df8d11161b6cf25b599a66baf087c1ffe340e9bfd1"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6c9379d65defcab82d07b2a9dfbfc2e95bc8fe0ebb1b176a3190230a3ef0e07c"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e635b87f01ebc977342e2697d05b56632f5f879a4f15955dfe8cef2448b51691"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:1c95a1e2902a8b722868587c0e1184ad5c55631de5afc0eb96bc4b0d738092c0"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:ef8de666d6179b009dce7bcb2ad4c4a779f113f12caf8dc77f0162c29d20490b"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:32fc0341d72e0f73f80acb0a2c94216bd704f4f0bce10aedea38f30502b271ff"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:289200a18fa698949d2b39c671c2cc7a24d44096784e76614899a7ccf2574b7b"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4a476b06fbcf359ad25d34a057b7219281286ae2477cc5ff5e3f70a246971148"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-win32.whl", hash = "sha256:aaeeb6a479c7667fbe1099af9617c83aaca22182d6cf8c53966491a0f1b7ffb7"},
    {file = "charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl", hash = "sha256:aa6af9e7d59f9c12b33ae4e9450619cf2488e2bbe9b44030905877f0b2324980"},
    {file = "charset_normalizer-3.4.2-py3-none-any.whl", hash = "sha256:7f56930ab0abd1c45cd15be65cc741c28b1c9a34876ce8c17a2fa107810c0af0"},
    {file = "charset_normalizer-3.4.2.tar.gz", hash = "sha256:5baececa9ecba31eff645232d59845c07aa030f0c81ee70184a90d35099a0e63"},
]

[[package]]
name = "click"
version = "8.2.1"
requires_python = ">=3.10"
summary = "Composable command line interface toolkit"
groups = ["default", "dev"]
dependencies = [
    "colorama; platform_system == \"Windows\"",
]
files = [
    {file = "click-8.2.1-py3-none-any.whl", hash = "sha256:61a3265b914e850b85317d0b3109c7f8cd35a670f963866005d6ef1d5175a12b"},
    {file = "click-8.2.1.tar.gz", hash = "sha256:27c491cc05d968d271d5a1db13e3b5a184636d9d930f148c50b038f0d0646202"},
]

[[package]]
name = "colorama"
version = "0.4.6"
requires_python = "!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7"
summary = "Cross-platform colored terminal text."
groups = ["default", "dev"]
marker = "sys_platform == \"win32\" or platform_system == \"Windows\""
files = [
    {file = "colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6"},
    {file = "colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44"},
]

[[package]]
name = "diskcache"
version = "5.6.3"
requires_python = ">=3"
summary = "Disk Cache -- Disk and file backed persistent cache."
groups = ["default"]
files = [
    {file = "diskcache-5.6.3-py3-none-any.whl", hash = "sha256:5e31b2d5fbad117cc363ebaf6b689474db18a1f6438bc82358b024abd4c2ca19"},
    {file = "diskcache-5.6.3.tar.gz", hash = "sha256:2c3a3fa2743d8535d832ec61c2054a1641f41775aa7c556758a109941e33e4fc"},
]

[[package]]
name = "distlib"
version = "0.3.9"
summary = "Distribution utilities"
groups = ["dev"]
files = [
    {file = "distlib-0.3.9-py2.py3-none-any.whl", hash = "sha256:47f8c22fd27c27e25a65601af709b38e4f0a45ea4fc2e710f65755fa8caaaf87"},
    {file = "distlib-0.3.9.tar.gz", hash = "sha256:a60f20dea646b8a33f3e7772f74dc0b2d0772d2837ee1342a00645c81edf9403"},
]

[[package]]
name = "exceptiongroup"
version = "1.3.0"
requires_python = ">=3.7"
summary = "Backport of PEP 654 (exception groups)"
groups = ["default", "dev"]
marker = "python_version < \"3.11\""
dependencies = [
    "typing-extensions>=4.6.0; python_version < \"3.13\"",
]
files = [
    {file = "exceptiongroup-1.3.0-py3-none-any.whl", hash = "sha256:4d111e6e0c13d0644cad6ddaa7ed0261a0b36971f6d23e7ec9b4b9097da78a10"},
    {file = "exceptiongroup-1.3.0.tar.gz", hash = "sha256:b241f5885f560bc56a59ee63ca4c6a8bfa46ae4ad651af316d4e81817bb9fd88"},
]

[[package]]
name = "fastapi"
version = "0.115.12"
requires_python = ">=3.8"
summary = "FastAPI framework, high performance, easy to learn, fast to code, ready for production"
groups = ["default"]
dependencies = [
    "pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4",
    "starlette<0.47.0,>=0.40.0",
    "typing-extensions>=4.8.0",
]
files = [
    {file = "fastapi-0.115.12-py3-none-any.whl", hash = "sha256:e94613d6c05e27be7ffebdd6ea5f388112e5e430c8f7d6494a9d1d88d43e814d"},
    {file = "fastapi-0.115.12.tar.gz", hash = "sha256:1e2c2a2646905f9e83d32f04a3f86aff4a286669c6c950ca95b5fd68c2602681"},
]

[[package]]
name = "filelock"
version = "3.18.0"
requires_python = ">=3.9"
summary = "A platform independent file lock."
groups = ["default", "dev"]
files = [
    {file = "filelock-3.18.0-py3-none-any.whl", hash = "sha256:c401f4f8377c4464e6db25fff06205fd89bdd83b65eb0488ed1b160f780e21de"},
    {file = "filelock-3.18.0.tar.gz", hash = "sha256:adbc88eabb99d2fec8c9c1b229b171f18afa655400173ddc653d5d01501fb9f2"},
]

[[package]]
name = "fsspec"
version = "2025.5.1"
requires_python = ">=3.9"
summary = "File-system specification"
groups = ["default"]
files = [
    {file = "fsspec-2025.5.1-py3-none-any.whl", hash = "sha256:24d3a2e663d5fc735ab256263c4075f374a174c3410c0b25e5bd1970bceaa462"},
    {file = "fsspec-2025.5.1.tar.gz", hash = "sha256:2e55e47a540b91843b755e83ded97c6e897fa0942b11490113f09e9c443c2475"},
]

[[package]]
name = "h11"
version = "0.16.0"
requires_python = ">=3.8"
summary = "A pure-Python, bring-your-own-I/O implementation of HTTP/1.1"
groups = ["default"]
files = [
    {file = "h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86"},
    {file = "h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1"},
]

[[package]]
name = "hf-xet"
version = "1.1.2"
requires_python = ">=3.8"
summary = "Fast transfer of large files with the Hugging Face Hub."
groups = ["default"]
marker = "platform_machine == \"x86_64\" or platform_machine == \"amd64\" or platform_machine == \"arm64\" or platform_machine == \"aarch64\""
files = [
    {file = "hf_xet-1.1.2-cp37-abi3-macosx_10_12_x86_64.whl", hash = "sha256:dfd1873fd648488c70735cb60f7728512bca0e459e61fcd107069143cd798469"},
    {file = "hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl", hash = "sha256:29b584983b2d977c44157d9241dcf0fd50acde0b7bff8897fe4386912330090d"},
    {file = "hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6b29ac84298147fe9164cc55ad994ba47399f90b5d045b0b803b99cf5f06d8ec"},
    {file = "hf_xet-1.1.2-cp37-abi3-manylinux_2_28_aarch64.whl", hash = "sha256:d921ba32615676e436a0d15e162331abc9ed43d440916b1d836dc27ce1546173"},
    {file = "hf_xet-1.1.2-cp37-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:d9b03c34e13c44893ab6e8fea18ee8d2a6878c15328dd3aabedbdd83ee9f2ed3"},
    {file = "hf_xet-1.1.2-cp37-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:01b18608955b3d826307d37da8bd38b28a46cd2d9908b3a3655d1363274f941a"},
    {file = "hf_xet-1.1.2-cp37-abi3-win_amd64.whl", hash = "sha256:3562902c81299b09f3582ddfb324400c6a901a2f3bc854f83556495755f4954c"},
    {file = "hf_xet-1.1.2.tar.gz", hash = "sha256:3712d6d4819d3976a1c18e36db9f503e296283f9363af818f50703506ed63da3"},
]

[[package]]
name = "httpcore"
version = "1.0.9"
requires_python = ">=3.8"
summary = "A minimal low-level HTTP client."
groups = ["default"]
dependencies = [
    "certifi",
    "h11>=0.16",
]
files = [
    {file = "httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55"},
    {file = "httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8"},
]

[[package]]
name = "httptools"
version = "0.6.4"
requires_python = ">=3.8.0"
summary = "A collection of framework independent HTTP protocol utils."
groups = ["default"]
files = [
    {file = "httptools-0.6.4-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:3c73ce323711a6ffb0d247dcd5a550b8babf0f757e86a52558fe5b86d6fefcc0"},
    {file = "httptools-0.6.4-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:345c288418f0944a6fe67be8e6afa9262b18c7626c3ef3c28adc5eabc06a68da"},
    {file = "httptools-0.6.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:deee0e3343f98ee8047e9f4c5bc7cedbf69f5734454a94c38ee829fb2d5fa3c1"},
    {file = "httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ca80b7485c76f768a3bc83ea58373f8db7b015551117375e4918e2aa77ea9b50"},
    {file = "httptools-0.6.4-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:90d96a385fa941283ebd231464045187a31ad932ebfa541be8edf5b3c2328959"},
    {file = "httptools-0.6.4-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:59e724f8b332319e2875efd360e61ac07f33b492889284a3e05e6d13746876f4"},
    {file = "httptools-0.6.4-cp310-cp310-win_amd64.whl", hash = "sha256:c26f313951f6e26147833fc923f78f95604bbec812a43e5ee37f26dc9e5a686c"},
    {file = "httptools-0.6.4-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:f47f8ed67cc0ff862b84a1189831d1d33c963fb3ce1ee0c65d3b0cbe7b711069"},
    {file = "httptools-0.6.4-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:0614154d5454c21b6410fdf5262b4a3ddb0f53f1e1721cfd59d55f32138c578a"},
    {file = "httptools-0.6.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f8787367fbdfccae38e35abf7641dafc5310310a5987b689f4c32cc8cc3ee975"},
    {file = "httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:40b0f7fe4fd38e6a507bdb751db0379df1e99120c65fbdc8ee6c1d044897a636"},
    {file = "httptools-0.6.4-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:40a5ec98d3f49904b9fe36827dcf1aadfef3b89e2bd05b0e35e94f97c2b14721"},
    {file = "httptools-0.6.4-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:dacdd3d10ea1b4ca9df97a0a303cbacafc04b5cd375fa98732678151643d4988"},
    {file = "httptools-0.6.4-cp311-cp311-win_amd64.whl", hash = "sha256:288cd628406cc53f9a541cfaf06041b4c71d751856bab45e3702191f931ccd17"},
    {file = "httptools-0.6.4-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:df017d6c780287d5c80601dafa31f17bddb170232d85c066604d8558683711a2"},
    {file = "httptools-0.6.4-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:85071a1e8c2d051b507161f6c3e26155b5c790e4e28d7f236422dbacc2a9cc44"},
    {file = "httptools-0.6.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:69422b7f458c5af875922cdb5bd586cc1f1033295aa9ff63ee196a87519ac8e1"},
    {file = "httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:16e603a3bff50db08cd578d54f07032ca1631450ceb972c2f834c2b860c28ea2"},
    {file = "httptools-0.6.4-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:ec4f178901fa1834d4a060320d2f3abc5c9e39766953d038f1458cb885f47e81"},
    {file = "httptools-0.6.4-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:f9eb89ecf8b290f2e293325c646a211ff1c2493222798bb80a530c5e7502494f"},
    {file = "httptools-0.6.4-cp312-cp312-win_amd64.whl", hash = "sha256:db78cb9ca56b59b016e64b6031eda5653be0589dba2b1b43453f6e8b405a0970"},
    {file = "httptools-0.6.4-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:ade273d7e767d5fae13fa637f4d53b6e961fb7fd93c7797562663f0171c26660"},
    {file = "httptools-0.6.4-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:856f4bc0478ae143bad54a4242fccb1f3f86a6e1be5548fecfd4102061b3a083"},
    {file = "httptools-0.6.4-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:322d20ea9cdd1fa98bd6a74b77e2ec5b818abdc3d36695ab402a0de8ef2865a3"},
    {file = "httptools-0.6.4-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4d87b29bd4486c0093fc64dea80231f7c7f7eb4dc70ae394d70a495ab8436071"},
    {file = "httptools-0.6.4-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:342dd6946aa6bda4b8f18c734576106b8a31f2fe31492881a9a160ec84ff4bd5"},
    {file = "httptools-0.6.4-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4b36913ba52008249223042dca46e69967985fb4051951f94357ea681e1f5dc0"},
    {file = "httptools-0.6.4-cp313-cp313-win_amd64.whl", hash = "sha256:28908df1b9bb8187393d5b5db91435ccc9c8e891657f9cbb42a2541b44c82fc8"},
    {file = "httptools-0.6.4.tar.gz", hash = "sha256:4e93eee4add6493b59a5c514da98c939b244fce4a0d8879cd3f466562f4b7d5c"},
]

[[package]]
name = "httpx"
version = "0.28.1"
requires_python = ">=3.8"
summary = "The next generation HTTP client."
groups = ["default"]
dependencies = [
    "anyio",
    "certifi",
    "httpcore==1.*",
    "idna",
]
files = [
    {file = "httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad"},
    {file = "httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc"},
]

[[package]]
name = "huggingface-hub"
version = "0.32.0"
requires_python = ">=3.8.0"
summary = "Client library to download and publish models, datasets and other repos on the huggingface.co hub"
groups = ["default"]
dependencies = [
    "filelock",
    "fsspec>=2023.5.0",
    "hf-xet<2.0.0,>=1.1.2; platform_machine == \"x86_64\" or platform_machine == \"amd64\" or platform_machine == \"arm64\" or platform_machine == \"aarch64\"",
    "packaging>=20.9",
    "pyyaml>=5.1",
    "requests",
    "tqdm>=4.42.1",
    "typing-extensions>=3.7.4.3",
]
files = [
    {file = "huggingface_hub-0.32.0-py3-none-any.whl", hash = "sha256:e56e94109649ce6ebdb59b4e393ee3543ec0eca2eab4f41b269e1d885c88d08c"},
    {file = "huggingface_hub-0.32.0.tar.gz", hash = "sha256:dd66c9365ea43049ec9b939bdcdb21a0051e1bd70026fc50304e4fb1bb6a15ba"},
]

[[package]]
name = "identify"
version = "2.6.12"
requires_python = ">=3.9"
summary = "File identification library for Python"
groups = ["dev"]
files = [
    {file = "identify-2.6.12-py2.py3-none-any.whl", hash = "sha256:ad9672d5a72e0d2ff7c5c8809b62dfa60458626352fb0eb7b55e69bdc45334a2"},
    {file = "identify-2.6.12.tar.gz", hash = "sha256:d8de45749f1efb108badef65ee8386f0f7bb19a7f26185f74de6367bffbaf0e6"},
]

[[package]]
name = "idna"
version = "3.10"
requires_python = ">=3.6"
summary = "Internationalized Domain Names in Applications (IDNA)"
groups = ["default"]
files = [
    {file = "idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3"},
    {file = "idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9"},
]

[[package]]
name = "iniconfig"
version = "2.1.0"
requires_python = ">=3.8"
summary = "brain-dead simple config-ini parsing"
groups = ["dev"]
files = [
    {file = "iniconfig-2.1.0-py3-none-any.whl", hash = "sha256:9deba5723312380e77435581c6bf4935c94cbfab9b1ed33ef8d238ea168eb760"},
    {file = "iniconfig-2.1.0.tar.gz", hash = "sha256:3abbd2e30b36733fee78f9c7f7308f2d0050e88f0087fd25c2645f63c773e1c7"},
]

[[package]]
name = "jinja2"
version = "3.1.6"
requires_python = ">=3.7"
summary = "A very fast and expressive template engine."
groups = ["default"]
dependencies = [
    "MarkupSafe>=2.0",
]
files = [
    {file = "jinja2-3.1.6-py3-none-any.whl", hash = "sha256:85ece4451f492d0c13c5dd7c13a64681a86afae63a5f347908daf103ce6d2f67"},
    {file = "jinja2-3.1.6.tar.gz", hash = "sha256:0137fb05990d35f1275a587e9aee6d56da821fc83491a0fb838183be43f66d6d"},
]

[[package]]
name = "llama-cpp-python"
version = "0.3.9"
requires_python = ">=3.8"
summary = "Python bindings for the llama.cpp library"
groups = ["default"]
dependencies = [
    "diskcache>=5.6.1",
    "jinja2>=2.11.3",
    "numpy>=1.20.0",
    "typing-extensions>=4.5.0",
]
files = [
    {file = "llama_cpp_python-0.3.9.tar.gz", hash = "sha256:a3a985f558385e2f5de5b663f4e9b0817506d6af98122450142cd98e79216370"},
]

[[package]]
name = "markdown-it-py"
version = "3.0.0"
requires_python = ">=3.8"
summary = "Python port of markdown-it. Markdown parsing, done right!"
groups = ["default"]
dependencies = [
    "mdurl~=0.1",
]
files = [
    {file = "markdown-it-py-3.0.0.tar.gz", hash = "sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb"},
    {file = "markdown_it_py-3.0.0-py3-none-any.whl", hash = "sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1"},
]

[[package]]
name = "markupsafe"
version = "3.0.2"
requires_python = ">=3.9"
summary = "Safely add untrusted strings to HTML/XML markup."
groups = ["default"]
files = [
    {file = "MarkupSafe-3.0.2-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:7e94c425039cde14257288fd61dcfb01963e658efbc0ff54f5306b06054700f8"},
    {file = "MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:9e2d922824181480953426608b81967de705c3cef4d1af983af849d7bd619158"},
    {file = "MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:38a9ef736c01fccdd6600705b09dc574584b89bea478200c5fbf112a6b0d5579"},
    {file = "MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bbcb445fa71794da8f178f0f6d66789a28d7319071af7a496d4d507ed566270d"},
    {file = "MarkupSafe-3.0.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:57cb5a3cf367aeb1d316576250f65edec5bb3be939e9247ae594b4bcbc317dfb"},
    {file = "MarkupSafe-3.0.2-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:3809ede931876f5b2ec92eef964286840ed3540dadf803dd570c3b7e13141a3b"},
    {file = "MarkupSafe-3.0.2-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:e07c3764494e3776c602c1e78e298937c3315ccc9043ead7e685b7f2b8d47b3c"},
    {file = "MarkupSafe-3.0.2-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:b424c77b206d63d500bcb69fa55ed8d0e6a3774056bdc4839fc9298a7edca171"},
    {file = "MarkupSafe-3.0.2-cp310-cp310-win32.whl", hash = "sha256:fcabf5ff6eea076f859677f5f0b6b5c1a51e70a376b0579e0eadef8db48c6b50"},
    {file = "MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl", hash = "sha256:6af100e168aa82a50e186c82875a5893c5597a0c1ccdb0d8b40240b1f28b969a"},
    {file = "MarkupSafe-3.0.2-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:9025b4018f3a1314059769c7bf15441064b2207cb3f065e6ea1e7359cb46db9d"},
    {file = "MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:93335ca3812df2f366e80509ae119189886b0f3c2b81325d39efdb84a1e2ae93"},
    {file = "MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2cb8438c3cbb25e220c2ab33bb226559e7afb3baec11c4f218ffa7308603c832"},
    {file = "MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a123e330ef0853c6e822384873bef7507557d8e4a082961e1defa947aa59ba84"},
    {file = "MarkupSafe-3.0.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1e084f686b92e5b83186b07e8a17fc09e38fff551f3602b249881fec658d3eca"},
    {file = "MarkupSafe-3.0.2-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:d8213e09c917a951de9d09ecee036d5c7d36cb6cb7dbaece4c71a60d79fb9798"},
    {file = "MarkupSafe-3.0.2-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:5b02fb34468b6aaa40dfc198d813a641e3a63b98c2b05a16b9f80b7ec314185e"},
    {file = "MarkupSafe-3.0.2-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:0bff5e0ae4ef2e1ae4fdf2dfd5b76c75e5c2fa4132d05fc1b0dabcd20c7e28c4"},
    {file = "MarkupSafe-3.0.2-cp311-cp311-win32.whl", hash = "sha256:6c89876f41da747c8d3677a2b540fb32ef5715f97b66eeb0c6b66f5e3ef6f59d"},
    {file = "MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl", hash = "sha256:70a87b411535ccad5ef2f1df5136506a10775d267e197e4cf531ced10537bd6b"},
    {file = "MarkupSafe-3.0.2-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:9778bd8ab0a994ebf6f84c2b949e65736d5575320a17ae8984a77fab08db94cf"},
    {file = "MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:846ade7b71e3536c4e56b386c2a47adf5741d2d8b94ec9dc3e92e5e1ee1e2225"},
    {file = "MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1c99d261bd2d5f6b59325c92c73df481e05e57f19837bdca8413b9eac4bd8028"},
    {file = "MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e17c96c14e19278594aa4841ec148115f9c7615a47382ecb6b82bd8fea3ab0c8"},
    {file = "MarkupSafe-3.0.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:88416bd1e65dcea10bc7569faacb2c20ce071dd1f87539ca2ab364bf6231393c"},
    {file = "MarkupSafe-3.0.2-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:2181e67807fc2fa785d0592dc2d6206c019b9502410671cc905d132a92866557"},
    {file = "MarkupSafe-3.0.2-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:52305740fe773d09cffb16f8ed0427942901f00adedac82ec8b67752f58a1b22"},
    {file = "MarkupSafe-3.0.2-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:ad10d3ded218f1039f11a75f8091880239651b52e9bb592ca27de44eed242a48"},
    {file = "MarkupSafe-3.0.2-cp312-cp312-win32.whl", hash = "sha256:0f4ca02bea9a23221c0182836703cbf8930c5e9454bacce27e767509fa286a30"},
    {file = "MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl", hash = "sha256:8e06879fc22a25ca47312fbe7c8264eb0b662f6db27cb2d3bbbc74b1df4b9b87"},
    {file = "MarkupSafe-3.0.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:ba9527cdd4c926ed0760bc301f6728ef34d841f405abf9d4f959c478421e4efd"},
    {file = "MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f8b3d067f2e40fe93e1ccdd6b2e1d16c43140e76f02fb1319a05cf2b79d99430"},
    {file = "MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:569511d3b58c8791ab4c2e1285575265991e6d8f8700c7be0e88f86cb0672094"},
    {file = "MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:15ab75ef81add55874e7ab7055e9c397312385bd9ced94920f2802310c930396"},
    {file = "MarkupSafe-3.0.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f3818cb119498c0678015754eba762e0d61e5b52d34c8b13d770f0719f7b1d79"},
    {file = "MarkupSafe-3.0.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:cdb82a876c47801bb54a690c5ae105a46b392ac6099881cdfb9f6e95e4014c6a"},
    {file = "MarkupSafe-3.0.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:cabc348d87e913db6ab4aa100f01b08f481097838bdddf7c7a84b7575b7309ca"},
    {file = "MarkupSafe-3.0.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:444dcda765c8a838eaae23112db52f1efaf750daddb2d9ca300bcae1039adc5c"},
    {file = "MarkupSafe-3.0.2-cp313-cp313-win32.whl", hash = "sha256:bcf3e58998965654fdaff38e58584d8937aa3096ab5354d493c77d1fdd66d7a1"},
    {file = "MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:e6a2a455bd412959b57a172ce6328d2dd1f01cb2135efda2e4576e8a23fa3b0f"},
    {file = "MarkupSafe-3.0.2-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:b5a6b3ada725cea8a5e634536b1b01c30bcdcd7f9c6fff4151548d5bf6b3a36c"},
    {file = "MarkupSafe-3.0.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:a904af0a6162c73e3edcb969eeeb53a63ceeb5d8cf642fade7d39e7963a22ddb"},
    {file = "MarkupSafe-3.0.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4aa4e5faecf353ed117801a068ebab7b7e09ffb6e1d5e412dc852e0da018126c"},
    {file = "MarkupSafe-3.0.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c0ef13eaeee5b615fb07c9a7dadb38eac06a0608b41570d8ade51c56539e509d"},
    {file = "MarkupSafe-3.0.2-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d16a81a06776313e817c951135cf7340a3e91e8c1ff2fac444cfd75fffa04afe"},
    {file = "MarkupSafe-3.0.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:6381026f158fdb7c72a168278597a5e3a5222e83ea18f543112b2662a9b699c5"},
    {file = "MarkupSafe-3.0.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:3d79d162e7be8f996986c064d1c7c817f6df3a77fe3d6859f6f9e7be4b8c213a"},
    {file = "MarkupSafe-3.0.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:131a3c7689c85f5ad20f9f6fb1b866f402c445b220c19fe4308c0b147ccd2ad9"},
    {file = "MarkupSafe-3.0.2-cp313-cp313t-win32.whl", hash = "sha256:ba8062ed2cf21c07a9e295d5b8a2a5ce678b913b45fdf68c32d95d6c1291e0b6"},
    {file = "MarkupSafe-3.0.2-cp313-cp313t-win_amd64.whl", hash = "sha256:e444a31f8db13eb18ada366ab3cf45fd4b31e4db1236a4448f68778c1d1a5a2f"},
    {file = "markupsafe-3.0.2.tar.gz", hash = "sha256:ee55d3edf80167e48ea11a923c7386f4669df67d7994554387f84e7d8b0a2bf0"},
]

[[package]]
name = "mdurl"
version = "0.1.2"
requires_python = ">=3.7"
summary = "Markdown URL utilities"
groups = ["default"]
files = [
    {file = "mdurl-0.1.2-py3-none-any.whl", hash = "sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8"},
    {file = "mdurl-0.1.2.tar.gz", hash = "sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba"},
]

[[package]]
name = "mypy-extensions"
version = "1.1.0"
requires_python = ">=3.8"
summary = "Type system extensions for programs checked with the mypy type checker."
groups = ["dev"]
files = [
    {file = "mypy_extensions-1.1.0-py3-none-any.whl", hash = "sha256:1be4cccdb0f2482337c4743e60421de3a356cd97508abadd57d47403e94f5505"},
    {file = "mypy_extensions-1.1.0.tar.gz", hash = "sha256:52e68efc3284861e772bbcd66823fde5ae21fd2fdb51c62a211403730b916558"},
]

[[package]]
name = "nodeenv"
version = "1.9.1"
requires_python = "!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7"
summary = "Node.js virtual environment builder"
groups = ["dev"]
files = [
    {file = "nodeenv-1.9.1-py2.py3-none-any.whl", hash = "sha256:ba11c9782d29c27c70ffbdda2d7415098754709be8a7056d79a737cd901155c9"},
    {file = "nodeenv-1.9.1.tar.gz", hash = "sha256:6ec12890a2dab7946721edbfbcd91f3319c6ccc9aec47be7c7e6b7011ee6645f"},
]

[[package]]
name = "numpy"
version = "2.2.6"
requires_python = ">=3.10"
summary = "Fundamental package for array computing in Python"
groups = ["default"]
files = [
    {file = "numpy-2.2.6-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:b412caa66f72040e6d268491a59f2c43bf03eb6c96dd8f0307829feb7fa2b6fb"},
    {file = "numpy-2.2.6-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:8e41fd67c52b86603a91c1a505ebaef50b3314de0213461c7a6e99c9a3beff90"},
    {file = "numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl", hash = "sha256:37e990a01ae6ec7fe7fa1c26c55ecb672dd98b19c3d0e1d1f326fa13cb38d163"},
    {file = "numpy-2.2.6-cp310-cp310-macosx_14_0_x86_64.whl", hash = "sha256:5a6429d4be8ca66d889b7cf70f536a397dc45ba6faeb5f8c5427935d9592e9cf"},
    {file = "numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:efd28d4e9cd7d7a8d39074a4d44c63eda73401580c5c76acda2ce969e0a38e83"},
    {file = "numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fc7b73d02efb0e18c000e9ad8b83480dfcd5dfd11065997ed4c6747470ae8915"},
    {file = "numpy-2.2.6-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:74d4531beb257d2c3f4b261bfb0fc09e0f9ebb8842d82a7b4209415896adc680"},
    {file = "numpy-2.2.6-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:8fc377d995680230e83241d8a96def29f204b5782f371c532579b4f20607a289"},
    {file = "numpy-2.2.6-cp310-cp310-win32.whl", hash = "sha256:b093dd74e50a8cba3e873868d9e93a85b78e0daf2e98c6797566ad8044e8363d"},
    {file = "numpy-2.2.6-cp310-cp310-win_amd64.whl", hash = "sha256:f0fd6321b839904e15c46e0d257fdd101dd7f530fe03fd6359c1ea63738703f3"},
    {file = "numpy-2.2.6-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:f9f1adb22318e121c5c69a09142811a201ef17ab257a1e66ca3025065b7f53ae"},
    {file = "numpy-2.2.6-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:c820a93b0255bc360f53eca31a0e676fd1101f673dda8da93454a12e23fc5f7a"},
    {file = "numpy-2.2.6-cp311-cp311-macosx_14_0_arm64.whl", hash = "sha256:3d70692235e759f260c3d837193090014aebdf026dfd167834bcba43e30c2a42"},
    {file = "numpy-2.2.6-cp311-cp311-macosx_14_0_x86_64.whl", hash = "sha256:481b49095335f8eed42e39e8041327c05b0f6f4780488f61286ed3c01368d491"},
    {file = "numpy-2.2.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b64d8d4d17135e00c8e346e0a738deb17e754230d7e0810ac5012750bbd85a5a"},
    {file = "numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ba10f8411898fc418a521833e014a77d3ca01c15b0c6cdcce6a0d2897e6dbbdf"},
    {file = "numpy-2.2.6-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:bd48227a919f1bafbdda0583705e547892342c26fb127219d60a5c36882609d1"},
    {file = "numpy-2.2.6-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:9551a499bf125c1d4f9e250377c1ee2eddd02e01eac6644c080162c0c51778ab"},
    {file = "numpy-2.2.6-cp311-cp311-win32.whl", hash = "sha256:0678000bb9ac1475cd454c6b8c799206af8107e310843532b04d49649c717a47"},
    {file = "numpy-2.2.6-cp311-cp311-win_amd64.whl", hash = "sha256:e8213002e427c69c45a52bbd94163084025f533a55a59d6f9c5b820774ef3303"},
    {file = "numpy-2.2.6-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:41c5a21f4a04fa86436124d388f6ed60a9343a6f767fced1a8a71c3fbca038ff"},
    {file = "numpy-2.2.6-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:de749064336d37e340f640b05f24e9e3dd678c57318c7289d222a8a2f543e90c"},
    {file = "numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl", hash = "sha256:894b3a42502226a1cac872f840030665f33326fc3dac8e57c607905773cdcde3"},
    {file = "numpy-2.2.6-cp312-cp312-macosx_14_0_x86_64.whl", hash = "sha256:71594f7c51a18e728451bb50cc60a3ce4e6538822731b2933209a1f3614e9282"},
    {file = "numpy-2.2.6-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f2618db89be1b4e05f7a1a847a9c1c0abd63e63a1607d892dd54668dd92faf87"},
    {file = "numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fd83c01228a688733f1ded5201c678f0c53ecc1006ffbc404db9f7a899ac6249"},
    {file = "numpy-2.2.6-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:37c0ca431f82cd5fa716eca9506aefcabc247fb27ba69c5062a6d3ade8cf8f49"},
    {file = "numpy-2.2.6-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:fe27749d33bb772c80dcd84ae7e8df2adc920ae8297400dabec45f0dedb3f6de"},
    {file = "numpy-2.2.6-cp312-cp312-win32.whl", hash = "sha256:4eeaae00d789f66c7a25ac5f34b71a7035bb474e679f410e5e1a94deb24cf2d4"},
    {file = "numpy-2.2.6-cp312-cp312-win_amd64.whl", hash = "sha256:c1f9540be57940698ed329904db803cf7a402f3fc200bfe599334c9bd84a40b2"},
    {file = "numpy-2.2.6-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:0811bb762109d9708cca4d0b13c4f67146e3c3b7cf8d34018c722adb2d957c84"},
    {file = "numpy-2.2.6-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:287cc3162b6f01463ccd86be154f284d0893d2b3ed7292439ea97eafa8170e0b"},
    {file = "numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl", hash = "sha256:f1372f041402e37e5e633e586f62aa53de2eac8d98cbfb822806ce4bbefcb74d"},
    {file = "numpy-2.2.6-cp313-cp313-macosx_14_0_x86_64.whl", hash = "sha256:55a4d33fa519660d69614a9fad433be87e5252f4b03850642f88993f7b2ca566"},
    {file = "numpy-2.2.6-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f92729c95468a2f4f15e9bb94c432a9229d0d50de67304399627a943201baa2f"},
    {file = "numpy-2.2.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1bc23a79bfabc5d056d106f9befb8d50c31ced2fbc70eedb8155aec74a45798f"},
    {file = "numpy-2.2.6-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:e3143e4451880bed956e706a3220b4e5cf6172ef05fcc397f6f36a550b1dd868"},
    {file = "numpy-2.2.6-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:b4f13750ce79751586ae2eb824ba7e1e8dba64784086c98cdbbcc6a42112ce0d"},
    {file = "numpy-2.2.6-cp313-cp313-win32.whl", hash = "sha256:5beb72339d9d4fa36522fc63802f469b13cdbe4fdab4a288f0c441b74272ebfd"},
    {file = "numpy-2.2.6-cp313-cp313-win_amd64.whl", hash = "sha256:b0544343a702fa80c95ad5d3d608ea3599dd54d4632df855e4c8d24eb6ecfa1c"},
    {file = "numpy-2.2.6-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0bca768cd85ae743b2affdc762d617eddf3bcf8724435498a1e80132d04879e6"},
    {file = "numpy-2.2.6-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:fc0c5673685c508a142ca65209b4e79ed6740a4ed6b2267dbba90f34b0b3cfda"},
    {file = "numpy-2.2.6-cp313-cp313t-macosx_14_0_arm64.whl", hash = "sha256:5bd4fc3ac8926b3819797a7c0e2631eb889b4118a9898c84f585a54d475b7e40"},
    {file = "numpy-2.2.6-cp313-cp313t-macosx_14_0_x86_64.whl", hash = "sha256:fee4236c876c4e8369388054d02d0e9bb84821feb1a64dd59e137e6511a551f8"},
    {file = "numpy-2.2.6-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e1dda9c7e08dc141e0247a5b8f49cf05984955246a327d4c48bda16821947b2f"},
    {file = "numpy-2.2.6-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f447e6acb680fd307f40d3da4852208af94afdfab89cf850986c3ca00562f4fa"},
    {file = "numpy-2.2.6-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:389d771b1623ec92636b0786bc4ae56abafad4a4c513d36a55dce14bd9ce8571"},
    {file = "numpy-2.2.6-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:8e9ace4a37db23421249ed236fdcdd457d671e25146786dfc96835cd951aa7c1"},
    {file = "numpy-2.2.6-cp313-cp313t-win32.whl", hash = "sha256:038613e9fb8c72b0a41f025a7e4c3f0b7a1b5d768ece4796b674c8f3fe13efff"},
    {file = "numpy-2.2.6-cp313-cp313t-win_amd64.whl", hash = "sha256:6031dd6dfecc0cf9f668681a37648373bddd6421fff6c66ec1624eed0180ee06"},
    {file = "numpy-2.2.6-pp310-pypy310_pp73-macosx_10_15_x86_64.whl", hash = "sha256:0b605b275d7bd0c640cad4e5d30fa701a8d59302e127e5f79138ad62762c3e3d"},
    {file = "numpy-2.2.6-pp310-pypy310_pp73-macosx_14_0_x86_64.whl", hash = "sha256:7befc596a7dc9da8a337f79802ee8adb30a552a94f792b9c9d18c840055907db"},
    {file = "numpy-2.2.6-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ce47521a4754c8f4593837384bd3424880629f718d87c5d44f8ed763edd63543"},
    {file = "numpy-2.2.6-pp310-pypy310_pp73-win_amd64.whl", hash = "sha256:d042d24c90c41b54fd506da306759e06e568864df8ec17ccc17e9e884634fd00"},
    {file = "numpy-2.2.6.tar.gz", hash = "sha256:e29554e2bef54a90aa5cc07da6ce955accb83f21ab5de01a62c8478897b264fd"},
]

[[package]]
name = "packaging"
version = "25.0"
requires_python = ">=3.8"
summary = "Core utilities for Python packages"
groups = ["default", "dev"]
files = [
    {file = "packaging-25.0-py3-none-any.whl", hash = "sha256:29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484"},
    {file = "packaging-25.0.tar.gz", hash = "sha256:d443872c98d677bf60f6a1f2f8c1cb748e8fe762d2bf9d3148b5599295b0fc4f"},
]

[[package]]
name = "pathspec"
version = "0.12.1"
requires_python = ">=3.8"
summary = "Utility library for gitignore style pattern matching of file paths."
groups = ["dev"]
files = [
    {file = "pathspec-0.12.1-py3-none-any.whl", hash = "sha256:a0d503e138a4c123b27490a4f7beda6a01c6f288df0e4a8b79c7eb0dc7b4cc08"},
    {file = "pathspec-0.12.1.tar.gz", hash = "sha256:a482d51503a1ab33b1c67a6c3813a26953dbdc71c31dacaef9a838c4e29f5712"},
]

[[package]]
name = "pillow"
version = "11.2.1"
requires_python = ">=3.9"
summary = "Python Imaging Library (Fork)"
groups = ["default"]
files = [
    {file = "pillow-11.2.1-cp310-cp310-macosx_10_10_x86_64.whl", hash = "sha256:d57a75d53922fc20c165016a20d9c44f73305e67c351bbc60d1adaf662e74047"},
    {file = "pillow-11.2.1-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:127bf6ac4a5b58b3d32fc8289656f77f80567d65660bc46f72c0d77e6600cc95"},
    {file = "pillow-11.2.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b4ba4be812c7a40280629e55ae0b14a0aafa150dd6451297562e1764808bbe61"},
    {file = "pillow-11.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c8bd62331e5032bc396a93609982a9ab6b411c05078a52f5fe3cc59234a3abd1"},
    {file = "pillow-11.2.1-cp310-cp310-manylinux_2_28_aarch64.whl", hash = "sha256:562d11134c97a62fe3af29581f083033179f7ff435f78392565a1ad2d1c2c45c"},
    {file = "pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl", hash = "sha256:c97209e85b5be259994eb5b69ff50c5d20cca0f458ef9abd835e262d9d88b39d"},
    {file = "pillow-11.2.1-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:0c3e6d0f59171dfa2e25d7116217543310908dfa2770aa64b8f87605f8cacc97"},
    {file = "pillow-11.2.1-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:cc1c3bc53befb6096b84165956e886b1729634a799e9d6329a0c512ab651e579"},
    {file = "pillow-11.2.1-cp310-cp310-win32.whl", hash = "sha256:312c77b7f07ab2139924d2639860e084ec2a13e72af54d4f08ac843a5fc9c79d"},
    {file = "pillow-11.2.1-cp310-cp310-win_amd64.whl", hash = "sha256:9bc7ae48b8057a611e5fe9f853baa88093b9a76303937449397899385da06fad"},
    {file = "pillow-11.2.1-cp310-cp310-win_arm64.whl", hash = "sha256:2728567e249cdd939f6cc3d1f049595c66e4187f3c34078cbc0a7d21c47482d2"},
    {file = "pillow-11.2.1-cp311-cp311-macosx_10_10_x86_64.whl", hash = "sha256:35ca289f712ccfc699508c4658a1d14652e8033e9b69839edf83cbdd0ba39e70"},
    {file = "pillow-11.2.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:e0409af9f829f87a2dfb7e259f78f317a5351f2045158be321fd135973fff7bf"},
    {file = "pillow-11.2.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d4e5c5edee874dce4f653dbe59db7c73a600119fbea8d31f53423586ee2aafd7"},
    {file = "pillow-11.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b93a07e76d13bff9444f1a029e0af2964e654bfc2e2c2d46bfd080df5ad5f3d8"},
    {file = "pillow-11.2.1-cp311-cp311-manylinux_2_28_aarch64.whl", hash = "sha256:e6def7eed9e7fa90fde255afaf08060dc4b343bbe524a8f69bdd2a2f0018f600"},
    {file = "pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl", hash = "sha256:8f4f3724c068be008c08257207210c138d5f3731af6c155a81c2b09a9eb3a788"},
    {file = "pillow-11.2.1-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:a0a6709b47019dff32e678bc12c63008311b82b9327613f534e496dacaefb71e"},
    {file = "pillow-11.2.1-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:f6b0c664ccb879109ee3ca702a9272d877f4fcd21e5eb63c26422fd6e415365e"},
    {file = "pillow-11.2.1-cp311-cp311-win32.whl", hash = "sha256:cc5d875d56e49f112b6def6813c4e3d3036d269c008bf8aef72cd08d20ca6df6"},
    {file = "pillow-11.2.1-cp311-cp311-win_amd64.whl", hash = "sha256:0f5c7eda47bf8e3c8a283762cab94e496ba977a420868cb819159980b6709193"},
    {file = "pillow-11.2.1-cp311-cp311-win_arm64.whl", hash = "sha256:4d375eb838755f2528ac8cbc926c3e31cc49ca4ad0cf79cff48b20e30634a4a7"},
    {file = "pillow-11.2.1-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:78afba22027b4accef10dbd5eed84425930ba41b3ea0a86fa8d20baaf19d807f"},
    {file = "pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:78092232a4ab376a35d68c4e6d5e00dfd73454bd12b230420025fbe178ee3b0b"},
    {file = "pillow-11.2.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:25a5f306095c6780c52e6bbb6109624b95c5b18e40aab1c3041da3e9e0cd3e2d"},
    {file = "pillow-11.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0c7b29dbd4281923a2bfe562acb734cee96bbb129e96e6972d315ed9f232bef4"},
    {file = "pillow-11.2.1-cp312-cp312-manylinux_2_28_aarch64.whl", hash = "sha256:3e645b020f3209a0181a418bffe7b4a93171eef6c4ef6cc20980b30bebf17b7d"},
    {file = "pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl", hash = "sha256:b2dbea1012ccb784a65349f57bbc93730b96e85b42e9bf7b01ef40443db720b4"},
    {file = "pillow-11.2.1-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:da3104c57bbd72948d75f6a9389e6727d2ab6333c3617f0a89d72d4940aa0443"},
    {file = "pillow-11.2.1-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:598174aef4589af795f66f9caab87ba4ff860ce08cd5bb447c6fc553ffee603c"},
    {file = "pillow-11.2.1-cp312-cp312-win32.whl", hash = "sha256:1d535df14716e7f8776b9e7fee118576d65572b4aad3ed639be9e4fa88a1cad3"},
    {file = "pillow-11.2.1-cp312-cp312-win_amd64.whl", hash = "sha256:14e33b28bf17c7a38eede290f77db7c664e4eb01f7869e37fa98a5aa95978941"},
    {file = "pillow-11.2.1-cp312-cp312-win_arm64.whl", hash = "sha256:21e1470ac9e5739ff880c211fc3af01e3ae505859392bf65458c224d0bf283eb"},
    {file = "pillow-11.2.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:fdec757fea0b793056419bca3e9932eb2b0ceec90ef4813ea4c1e072c389eb28"},
    {file = "pillow-11.2.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:b0e130705d568e2f43a17bcbe74d90958e8a16263868a12c3e0d9c8162690830"},
    {file = "pillow-11.2.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:7bdb5e09068332578214cadd9c05e3d64d99e0e87591be22a324bdbc18925be0"},
    {file = "pillow-11.2.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d189ba1bebfbc0c0e529159631ec72bb9e9bc041f01ec6d3233d6d82eb823bc1"},
    {file = "pillow-11.2.1-cp313-cp313-manylinux_2_28_aarch64.whl", hash = "sha256:191955c55d8a712fab8934a42bfefbf99dd0b5875078240943f913bb66d46d9f"},
    {file = "pillow-11.2.1-cp313-cp313-manylinux_2_28_x86_64.whl", hash = "sha256:ad275964d52e2243430472fc5d2c2334b4fc3ff9c16cb0a19254e25efa03a155"},
    {file = "pillow-11.2.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:750f96efe0597382660d8b53e90dd1dd44568a8edb51cb7f9d5d918b80d4de14"},
    {file = "pillow-11.2.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:fe15238d3798788d00716637b3d4e7bb6bde18b26e5d08335a96e88564a36b6b"},
    {file = "pillow-11.2.1-cp313-cp313-win32.whl", hash = "sha256:3fe735ced9a607fee4f481423a9c36701a39719252a9bb251679635f99d0f7d2"},
    {file = "pillow-11.2.1-cp313-cp313-win_amd64.whl", hash = "sha256:74ee3d7ecb3f3c05459ba95eed5efa28d6092d751ce9bf20e3e253a4e497e691"},
    {file = "pillow-11.2.1-cp313-cp313-win_arm64.whl", hash = "sha256:5119225c622403afb4b44bad4c1ca6c1f98eed79db8d3bc6e4e160fc6339d66c"},
    {file = "pillow-11.2.1-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:8ce2e8411c7aaef53e6bb29fe98f28cd4fbd9a1d9be2eeea434331aac0536b22"},
    {file = "pillow-11.2.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:9ee66787e095127116d91dea2143db65c7bb1e232f617aa5957c0d9d2a3f23a7"},
    {file = "pillow-11.2.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9622e3b6c1d8b551b6e6f21873bdcc55762b4b2126633014cea1803368a9aa16"},
    {file = "pillow-11.2.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:63b5dff3a68f371ea06025a1a6966c9a1e1ee452fc8020c2cd0ea41b83e9037b"},
    {file = "pillow-11.2.1-cp313-cp313t-manylinux_2_28_aarch64.whl", hash = "sha256:31df6e2d3d8fc99f993fd253e97fae451a8db2e7207acf97859732273e108406"},
    {file = "pillow-11.2.1-cp313-cp313t-manylinux_2_28_x86_64.whl", hash = "sha256:062b7a42d672c45a70fa1f8b43d1d38ff76b63421cbbe7f88146b39e8a558d91"},
    {file = "pillow-11.2.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:4eb92eca2711ef8be42fd3f67533765d9fd043b8c80db204f16c8ea62ee1a751"},
    {file = "pillow-11.2.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:f91ebf30830a48c825590aede79376cb40f110b387c17ee9bd59932c961044f9"},
    {file = "pillow-11.2.1-cp313-cp313t-win32.whl", hash = "sha256:e0b55f27f584ed623221cfe995c912c61606be8513bfa0e07d2c674b4516d9dd"},
    {file = "pillow-11.2.1-cp313-cp313t-win_amd64.whl", hash = "sha256:36d6b82164c39ce5482f649b437382c0fb2395eabc1e2b1702a6deb8ad647d6e"},
    {file = "pillow-11.2.1-cp313-cp313t-win_arm64.whl", hash = "sha256:225c832a13326e34f212d2072982bb1adb210e0cc0b153e688743018c94a2681"},
    {file = "pillow-11.2.1-pp310-pypy310_pp73-macosx_10_15_x86_64.whl", hash = "sha256:9b7b0d4fd2635f54ad82785d56bc0d94f147096493a79985d0ab57aedd563156"},
    {file = "pillow-11.2.1-pp310-pypy310_pp73-macosx_11_0_arm64.whl", hash = "sha256:aa442755e31c64037aa7c1cb186e0b369f8416c567381852c63444dd666fb772"},
    {file = "pillow-11.2.1-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f0d3348c95b766f54b76116d53d4cb171b52992a1027e7ca50c81b43b9d9e363"},
    {file = "pillow-11.2.1-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:85d27ea4c889342f7e35f6d56e7e1cb345632ad592e8c51b693d7b7556043ce0"},
    {file = "pillow-11.2.1-pp310-pypy310_pp73-manylinux_2_28_aarch64.whl", hash = "sha256:bf2c33d6791c598142f00c9c4c7d47f6476731c31081331664eb26d6ab583e01"},
    {file = "pillow-11.2.1-pp310-pypy310_pp73-manylinux_2_28_x86_64.whl", hash = "sha256:e616e7154c37669fc1dfc14584f11e284e05d1c650e1c0f972f281c4ccc53193"},
    {file = "pillow-11.2.1-pp310-pypy310_pp73-win_amd64.whl", hash = "sha256:39ad2e0f424394e3aebc40168845fee52df1394a4673a6ee512d840d14ab3013"},
    {file = "pillow-11.2.1-pp311-pypy311_pp73-macosx_10_15_x86_64.whl", hash = "sha256:80f1df8dbe9572b4b7abdfa17eb5d78dd620b1d55d9e25f834efdbee872d3aed"},
    {file = "pillow-11.2.1-pp311-pypy311_pp73-macosx_11_0_arm64.whl", hash = "sha256:ea926cfbc3957090becbcbbb65ad177161a2ff2ad578b5a6ec9bb1e1cd78753c"},
    {file = "pillow-11.2.1-pp311-pypy311_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:738db0e0941ca0376804d4de6a782c005245264edaa253ffce24e5a15cbdc7bd"},
    {file = "pillow-11.2.1-pp311-pypy311_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9db98ab6565c69082ec9b0d4e40dd9f6181dab0dd236d26f7a50b8b9bfbd5076"},
    {file = "pillow-11.2.1-pp311-pypy311_pp73-manylinux_2_28_aarch64.whl", hash = "sha256:036e53f4170e270ddb8797d4c590e6dd14d28e15c7da375c18978045f7e6c37b"},
    {file = "pillow-11.2.1-pp311-pypy311_pp73-manylinux_2_28_x86_64.whl", hash = "sha256:14f73f7c291279bd65fda51ee87affd7c1e097709f7fdd0188957a16c264601f"},
    {file = "pillow-11.2.1-pp311-pypy311_pp73-win_amd64.whl", hash = "sha256:208653868d5c9ecc2b327f9b9ef34e0e42a4cdd172c2988fd81d62d2bc9bc044"},
    {file = "pillow-11.2.1.tar.gz", hash = "sha256:a64dd61998416367b7ef979b73d3a85853ba9bec4c2925f74e588879a58716b6"},
]

[[package]]
name = "platformdirs"
version = "4.3.8"
requires_python = ">=3.9"
summary = "A small Python package for determining appropriate platform-specific dirs, e.g. a `user data dir`."
groups = ["dev"]
files = [
    {file = "platformdirs-4.3.8-py3-none-any.whl", hash = "sha256:ff7059bb7eb1179e2685604f4aaf157cfd9535242bd23742eadc3c13542139b4"},
    {file = "platformdirs-4.3.8.tar.gz", hash = "sha256:3d512d96e16bcb959a814c9f348431070822a6496326a4be0911c40b5a74c2bc"},
]

[[package]]
name = "pluggy"
version = "1.6.0"
requires_python = ">=3.9"
summary = "plugin and hook calling mechanisms for python"
groups = ["dev"]
files = [
    {file = "pluggy-1.6.0-py3-none-any.whl", hash = "sha256:e920276dd6813095e9377c0bc5566d94c932c33b27a3e3945d8389c374dd4746"},
    {file = "pluggy-1.6.0.tar.gz", hash = "sha256:7dcc130b76258d33b90f61b658791dede3486c3e6bfb003ee5c9bfb396dd22f3"},
]

[[package]]
name = "pre-commit"
version = "4.2.0"
requires_python = ">=3.9"
summary = "A framework for managing and maintaining multi-language pre-commit hooks."
groups = ["dev"]
dependencies = [
    "cfgv>=2.0.0",
    "identify>=1.0.0",
    "nodeenv>=0.11.1",
    "pyyaml>=5.1",
    "virtualenv>=20.10.0",
]
files = [
    {file = "pre_commit-4.2.0-py2.py3-none-any.whl", hash = "sha256:a009ca7205f1eb497d10b845e52c838a98b6cdd2102a6c8e4540e94ee75c58bd"},
    {file = "pre_commit-4.2.0.tar.gz", hash = "sha256:601283b9757afd87d40c4c4a9b2b5de9637a8ea02eaff7adc2d0fb4e04841146"},
]

[[package]]
name = "pydantic"
version = "2.11.5"
requires_python = ">=3.9"
summary = "Data validation using Python type hints"
groups = ["default"]
dependencies = [
    "annotated-types>=0.6.0",
    "pydantic-core==2.33.2",
    "typing-extensions>=4.12.2",
    "typing-inspection>=0.4.0",
]
files = [
    {file = "pydantic-2.11.5-py3-none-any.whl", hash = "sha256:f9c26ba06f9747749ca1e5c94d6a85cb84254577553c8785576fd38fa64dc0f7"},
    {file = "pydantic-2.11.5.tar.gz", hash = "sha256:7f853db3d0ce78ce8bbb148c401c2cdd6431b3473c0cdff2755c7690952a7b7a"},
]

[[package]]
name = "pydantic-core"
version = "2.33.2"
requires_python = ">=3.9"
summary = "Core functionality for Pydantic validation and serialization"
groups = ["default"]
dependencies = [
    "typing-extensions!=4.7.0,>=4.6.0",
]
files = [
    {file = "pydantic_core-2.33.2-cp310-cp310-macosx_10_12_x86_64.whl", hash = "sha256:2b3d326aaef0c0399d9afffeb6367d5e26ddc24d351dbc9c636840ac355dc5d8"},
    {file = "pydantic_core-2.33.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:0e5b2671f05ba48b94cb90ce55d8bdcaaedb8ba00cc5359f6810fc918713983d"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0069c9acc3f3981b9ff4cdfaf088e98d83440a4c7ea1bc07460af3d4dc22e72d"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:d53b22f2032c42eaaf025f7c40c2e3b94568ae077a606f006d206a463bc69572"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:0405262705a123b7ce9f0b92f123334d67b70fd1f20a9372b907ce1080c7ba02"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4b25d91e288e2c4e0662b8038a28c6a07eaac3e196cfc4ff69de4ea3db992a1b"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6bdfe4b3789761f3bcb4b1ddf33355a71079858958e3a552f16d5af19768fef2"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:efec8db3266b76ef9607c2c4c419bdb06bf335ae433b80816089ea7585816f6a"},
    {file = "pydantic_core-2.33.2-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:031c57d67ca86902726e0fae2214ce6770bbe2f710dc33063187a68744a5ecac"},
    {file = "pydantic_core-2.33.2-cp310-cp310-musllinux_1_1_armv7l.whl", hash = "sha256:f8de619080e944347f5f20de29a975c2d815d9ddd8be9b9b7268e2e3ef68605a"},
    {file = "pydantic_core-2.33.2-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:73662edf539e72a9440129f231ed3757faab89630d291b784ca99237fb94db2b"},
    {file = "pydantic_core-2.33.2-cp310-cp310-win32.whl", hash = "sha256:0a39979dcbb70998b0e505fb1556a1d550a0781463ce84ebf915ba293ccb7e22"},
    {file = "pydantic_core-2.33.2-cp310-cp310-win_amd64.whl", hash = "sha256:b0379a2b24882fef529ec3b4987cb5d003b9cda32256024e6fe1586ac45fc640"},
    {file = "pydantic_core-2.33.2-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:4c5b0a576fb381edd6d27f0a85915c6daf2f8138dc5c267a57c08a62900758c7"},
    {file = "pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:e799c050df38a639db758c617ec771fd8fb7a5f8eaaa4b27b101f266b216a246"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:dc46a01bf8d62f227d5ecee74178ffc448ff4e5197c756331f71efcc66dc980f"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:a144d4f717285c6d9234a66778059f33a89096dfb9b39117663fd8413d582dcc"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:73cf6373c21bc80b2e0dc88444f41ae60b2f070ed02095754eb5a01df12256de"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3dc625f4aa79713512d1976fe9f0bc99f706a9dee21dfd1810b4bbbf228d0e8a"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:881b21b5549499972441da4758d662aeea93f1923f953e9cbaff14b8b9565aef"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:bdc25f3681f7b78572699569514036afe3c243bc3059d3942624e936ec93450e"},
    {file = "pydantic_core-2.33.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:fe5b32187cbc0c862ee201ad66c30cf218e5ed468ec8dc1cf49dec66e160cc4d"},
    {file = "pydantic_core-2.33.2-cp311-cp311-musllinux_1_1_armv7l.whl", hash = "sha256:bc7aee6f634a6f4a95676fcb5d6559a2c2a390330098dba5e5a5f28a2e4ada30"},
    {file = "pydantic_core-2.33.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:235f45e5dbcccf6bd99f9f472858849f73d11120d76ea8707115415f8e5ebebf"},
    {file = "pydantic_core-2.33.2-cp311-cp311-win32.whl", hash = "sha256:6368900c2d3ef09b69cb0b913f9f8263b03786e5b2a387706c5afb66800efd51"},
    {file = "pydantic_core-2.33.2-cp311-cp311-win_amd64.whl", hash = "sha256:1e063337ef9e9820c77acc768546325ebe04ee38b08703244c1309cccc4f1bab"},
    {file = "pydantic_core-2.33.2-cp311-cp311-win_arm64.whl", hash = "sha256:6b99022f1d19bc32a4c2a0d544fc9a76e3be90f0b3f4af413f87d38749300e65"},
    {file = "pydantic_core-2.33.2-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:a7ec89dc587667f22b6a0b6579c249fca9026ce7c333fc142ba42411fa243cdc"},
    {file = "pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:3c6db6e52c6d70aa0d00d45cdb9b40f0433b96380071ea80b09277dba021ddf7"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4e61206137cbc65e6d5256e1166f88331d3b6238e082d9f74613b9b765fb9025"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:eb8c529b2819c37140eb51b914153063d27ed88e3bdc31b71198a198e921e011"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c52b02ad8b4e2cf14ca7b3d918f3eb0ee91e63b3167c32591e57c4317e134f8f"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:96081f1605125ba0855dfda83f6f3df5ec90c61195421ba72223de35ccfb2f88"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8f57a69461af2a5fa6e6bbd7a5f60d3b7e6cebb687f55106933188e79ad155c1"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:572c7e6c8bb4774d2ac88929e3d1f12bc45714ae5ee6d9a788a9fb35e60bb04b"},
    {file = "pydantic_core-2.33.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:db4b41f9bd95fbe5acd76d89920336ba96f03e149097365afe1cb092fceb89a1"},
    {file = "pydantic_core-2.33.2-cp312-cp312-musllinux_1_1_armv7l.whl", hash = "sha256:fa854f5cf7e33842a892e5c73f45327760bc7bc516339fda888c75ae60edaeb6"},
    {file = "pydantic_core-2.33.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:5f483cfb75ff703095c59e365360cb73e00185e01aaea067cd19acffd2ab20ea"},
    {file = "pydantic_core-2.33.2-cp312-cp312-win32.whl", hash = "sha256:9cb1da0f5a471435a7bc7e439b8a728e8b61e59784b2af70d7c169f8dd8ae290"},
    {file = "pydantic_core-2.33.2-cp312-cp312-win_amd64.whl", hash = "sha256:f941635f2a3d96b2973e867144fde513665c87f13fe0e193c158ac51bfaaa7b2"},
    {file = "pydantic_core-2.33.2-cp312-cp312-win_arm64.whl", hash = "sha256:cca3868ddfaccfbc4bfb1d608e2ccaaebe0ae628e1416aeb9c4d88c001bb45ab"},
    {file = "pydantic_core-2.33.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:1082dd3e2d7109ad8b7da48e1d4710c8d06c253cbc4a27c1cff4fbcaa97a9e3f"},
    {file = "pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f517ca031dfc037a9c07e748cefd8d96235088b83b4f4ba8939105d20fa1dcd6"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0a9f2c9dd19656823cb8250b0724ee9c60a82f3cdf68a080979d13092a3b0fef"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:2b0a451c263b01acebe51895bfb0e1cc842a5c666efe06cdf13846c7418caa9a"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1ea40a64d23faa25e62a70ad163571c0b342b8bf66d5fa612ac0dec4f069d916"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0fb2d542b4d66f9470e8065c5469ec676978d625a8b7a363f07d9a501a9cb36a"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fdac5d6ffa1b5a83bca06ffe7583f5576555e6c8b3a91fbd25ea7780f825f7d"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:04a1a413977ab517154eebb2d326da71638271477d6ad87a769102f7c2488c56"},
    {file = "pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:c8e7af2f4e0194c22b5b37205bfb293d166a7344a5b0d0eaccebc376546d77d5"},
    {file = "pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:5c92edd15cd58b3c2d34873597a1e20f13094f59cf88068adb18947df5455b4e"},
    {file = "pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:65132b7b4a1c0beded5e057324b7e16e10910c106d43675d9bd87d4f38dde162"},
    {file = "pydantic_core-2.33.2-cp313-cp313-win32.whl", hash = "sha256:52fb90784e0a242bb96ec53f42196a17278855b0f31ac7c3cc6f5c1ec4811849"},
    {file = "pydantic_core-2.33.2-cp313-cp313-win_amd64.whl", hash = "sha256:c083a3bdd5a93dfe480f1125926afcdbf2917ae714bdb80b36d34318b2bec5d9"},
    {file = "pydantic_core-2.33.2-cp313-cp313-win_arm64.whl", hash = "sha256:e80b087132752f6b3d714f041ccf74403799d3b23a72722ea2e6ba2e892555b9"},
    {file = "pydantic_core-2.33.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:61c18fba8e5e9db3ab908620af374db0ac1baa69f0f32df4f61ae23f15e586ac"},
    {file = "pydantic_core-2.33.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:95237e53bb015f67b63c91af7518a62a8660376a6a0db19b89acc77a4d6199f5"},
    {file = "pydantic_core-2.33.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c2fc0a768ef76c15ab9238afa6da7f69895bb5d1ee83aeea2e3509af4472d0b9"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-macosx_10_12_x86_64.whl", hash = "sha256:5c4aa4e82353f65e548c476b37e64189783aa5384903bfea4f41580f255fddfa"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-macosx_11_0_arm64.whl", hash = "sha256:d946c8bf0d5c24bf4fe333af284c59a19358aa3ec18cb3dc4370080da1e8ad29"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:87b31b6846e361ef83fedb187bb5b4372d0da3f7e28d85415efa92d6125d6e6d"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:aa9d91b338f2df0508606f7009fde642391425189bba6d8c653afd80fd6bb64e"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:2058a32994f1fde4ca0480ab9d1e75a0e8c87c22b53a3ae66554f9af78f2fe8c"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-musllinux_1_1_aarch64.whl", hash = "sha256:0e03262ab796d986f978f79c943fc5f620381be7287148b8010b4097f79a39ec"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-musllinux_1_1_armv7l.whl", hash = "sha256:1a8695a8d00c73e50bff9dfda4d540b7dee29ff9b8053e38380426a85ef10052"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-musllinux_1_1_x86_64.whl", hash = "sha256:fa754d1850735a0b0e03bcffd9d4b4343eb417e47196e4485d9cca326073a42c"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-win_amd64.whl", hash = "sha256:a11c8d26a50bfab49002947d3d237abe4d9e4b5bdc8846a63537b6488e197808"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-macosx_10_12_x86_64.whl", hash = "sha256:dd14041875d09cc0f9308e37a6f8b65f5585cf2598a53aa0123df8b129d481f8"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-macosx_11_0_arm64.whl", hash = "sha256:d87c561733f66531dced0da6e864f44ebf89a8fba55f31407b00c2f7f9449593"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2f82865531efd18d6e07a04a17331af02cb7a651583c418df8266f17a63c6612"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2bfb5112df54209d820d7bf9317c7a6c9025ea52e49f46b6a2060104bba37de7"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:64632ff9d614e5eecfb495796ad51b0ed98c453e447a76bcbeeb69615079fc7e"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-musllinux_1_1_aarch64.whl", hash = "sha256:f889f7a40498cc077332c7ab6b4608d296d852182211787d4f3ee377aaae66e8"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-musllinux_1_1_armv7l.whl", hash = "sha256:de4b83bb311557e439b9e186f733f6c645b9417c84e2eb8203f3f820a4b988bf"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-musllinux_1_1_x86_64.whl", hash = "sha256:82f68293f055f51b51ea42fafc74b6aad03e70e191799430b90c13d643059ebb"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-win_amd64.whl", hash = "sha256:329467cecfb529c925cf2bbd4d60d2c509bc2fb52a20c1045bf09bb70971a9c1"},
    {file = "pydantic_core-2.33.2.tar.gz", hash = "sha256:7cb8bc3605c29176e1b105350d2e6474142d7c1bd1d9327c4a9bdb46bf827acc"},
]

[[package]]
name = "pydantic-settings"
version = "2.9.1"
requires_python = ">=3.9"
summary = "Settings management using Pydantic"
groups = ["default"]
dependencies = [
    "pydantic>=2.7.0",
    "python-dotenv>=0.21.0",
    "typing-inspection>=0.4.0",
]
files = [
    {file = "pydantic_settings-2.9.1-py3-none-any.whl", hash = "sha256:59b4f431b1defb26fe620c71a7d3968a710d719f5f4cdbbdb7926edeb770f6ef"},
    {file = "pydantic_settings-2.9.1.tar.gz", hash = "sha256:c509bf79d27563add44e8446233359004ed85066cd096d8b510f715e6ef5d268"},
]

[[package]]
name = "pygments"
version = "2.19.1"
requires_python = ">=3.8"
summary = "Pygments is a syntax highlighting package written in Python."
groups = ["default"]
files = [
    {file = "pygments-2.19.1-py3-none-any.whl", hash = "sha256:9ea1544ad55cecf4b8242fab6dd35a93bbce657034b0611ee383099054ab6d8c"},
    {file = "pygments-2.19.1.tar.gz", hash = "sha256:61c16d2a8576dc0649d9f39e089b5f02bcd27fba10d8fb4dcc28173f7a45151f"},
]

[[package]]
name = "pytest"
version = "8.3.5"
requires_python = ">=3.8"
summary = "pytest: simple powerful testing with Python"
groups = ["dev"]
dependencies = [
    "colorama; sys_platform == \"win32\"",
    "exceptiongroup>=1.0.0rc8; python_version < \"3.11\"",
    "iniconfig",
    "packaging",
    "pluggy<2,>=1.5",
    "tomli>=1; python_version < \"3.11\"",
]
files = [
    {file = "pytest-8.3.5-py3-none-any.whl", hash = "sha256:c69214aa47deac29fad6c2a4f590b9c4a9fdb16a403176fe154b79c0b4d4d820"},
    {file = "pytest-8.3.5.tar.gz", hash = "sha256:f4efe70cc14e511565ac476b57c279e12a855b11f48f212af1080ef2263d3845"},
]

[[package]]
name = "pytest-asyncio"
version = "0.26.0"
requires_python = ">=3.9"
summary = "Pytest support for asyncio"
groups = ["dev"]
dependencies = [
    "pytest<9,>=8.2",
    "typing-extensions>=4.12; python_version < \"3.10\"",
]
files = [
    {file = "pytest_asyncio-0.26.0-py3-none-any.whl", hash = "sha256:7b51ed894f4fbea1340262bdae5135797ebbe21d8638978e35d31c6d19f72fb0"},
    {file = "pytest_asyncio-0.26.0.tar.gz", hash = "sha256:c4df2a697648241ff39e7f0e4a73050b03f123f760673956cf0d72a4990e312f"},
]

[[package]]
name = "pytest-mock"
version = "3.14.0"
requires_python = ">=3.8"
summary = "Thin-wrapper around the mock package for easier use with pytest"
groups = ["dev"]
dependencies = [
    "pytest>=6.2.5",
]
files = [
    {file = "pytest-mock-3.14.0.tar.gz", hash = "sha256:2719255a1efeceadbc056d6bf3df3d1c5015530fb40cf347c0f9afac88410bd0"},
    {file = "pytest_mock-3.14.0-py3-none-any.whl", hash = "sha256:0b72c38033392a5f4621342fe11e9219ac11ec9d375f8e2a0c164539e0d70f6f"},
]

[[package]]
name = "python-dotenv"
version = "1.1.0"
requires_python = ">=3.9"
summary = "Read key-value pairs from a .env file and set them as environment variables"
groups = ["default"]
files = [
    {file = "python_dotenv-1.1.0-py3-none-any.whl", hash = "sha256:d7c01d9e2293916c18baf562d95698754b0dbbb5e74d457c45d4f6561fb9d55d"},
    {file = "python_dotenv-1.1.0.tar.gz", hash = "sha256:41f90bc6f5f177fb41f53e87666db362025010eb28f60a01c9143bfa33a2b2d5"},
]

[[package]]
name = "pyyaml"
version = "6.0.2"
requires_python = ">=3.8"
summary = "YAML parser and emitter for Python"
groups = ["default", "dev"]
files = [
    {file = "PyYAML-6.0.2-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:0a9a2848a5b7feac301353437eb7d5957887edbf81d56e903999a75a3d743086"},
    {file = "PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:29717114e51c84ddfba879543fb232a6ed60086602313ca38cce623c1d62cfbf"},
    {file = "PyYAML-6.0.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8824b5a04a04a047e72eea5cec3bc266db09e35de6bdfe34c9436ac5ee27d237"},
    {file = "PyYAML-6.0.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:7c36280e6fb8385e520936c3cb3b8042851904eba0e58d277dca80a5cfed590b"},
    {file = "PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ec031d5d2feb36d1d1a24380e4db6d43695f3748343d99434e6f5f9156aaa2ed"},
    {file = "PyYAML-6.0.2-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:936d68689298c36b53b29f23c6dbb74de12b4ac12ca6cfe0e047bedceea56180"},
    {file = "PyYAML-6.0.2-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:23502f431948090f597378482b4812b0caae32c22213aecf3b55325e049a6c68"},
    {file = "PyYAML-6.0.2-cp310-cp310-win32.whl", hash = "sha256:2e99c6826ffa974fe6e27cdb5ed0021786b03fc98e5ee3c5bfe1fd5015f42b99"},
    {file = "PyYAML-6.0.2-cp310-cp310-win_amd64.whl", hash = "sha256:a4d3091415f010369ae4ed1fc6b79def9416358877534caf6a0fdd2146c87a3e"},
    {file = "PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:cc1c1159b3d456576af7a3e4d1ba7e6924cb39de8f67111c735f6fc832082774"},
    {file = "PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:1e2120ef853f59c7419231f3bf4e7021f1b936f6ebd222406c3b60212205d2ee"},
    {file = "PyYAML-6.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5d225db5a45f21e78dd9358e58a98702a0302f2659a3c6cd320564b75b86f47c"},
    {file = "PyYAML-6.0.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5ac9328ec4831237bec75defaf839f7d4564be1e6b25ac710bd1a96321cc8317"},
    {file = "PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3ad2a3decf9aaba3d29c8f537ac4b243e36bef957511b4766cb0057d32b0be85"},
    {file = "PyYAML-6.0.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:ff3824dc5261f50c9b0dfb3be22b4567a6f938ccce4587b38952d85fd9e9afe4"},
    {file = "PyYAML-6.0.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:797b4f722ffa07cc8d62053e4cff1486fa6dc094105d13fea7b1de7d8bf71c9e"},
    {file = "PyYAML-6.0.2-cp311-cp311-win32.whl", hash = "sha256:11d8f3dd2b9c1207dcaf2ee0bbbfd5991f571186ec9cc78427ba5bd32afae4b5"},
    {file = "PyYAML-6.0.2-cp311-cp311-win_amd64.whl", hash = "sha256:e10ce637b18caea04431ce14fabcf5c64a1c61ec9c56b071a4b7ca131ca52d44"},
    {file = "PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:c70c95198c015b85feafc136515252a261a84561b7b1d51e3384e0655ddf25ab"},
    {file = "PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:ce826d6ef20b1bc864f0a68340c8b3287705cae2f8b4b1d932177dcc76721725"},
    {file = "PyYAML-6.0.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1f71ea527786de97d1a0cc0eacd1defc0985dcf6b3f17bb77dcfc8c34bec4dc5"},
    {file = "PyYAML-6.0.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9b22676e8097e9e22e36d6b7bda33190d0d400f345f23d4065d48f4ca7ae0425"},
    {file = "PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:80bab7bfc629882493af4aa31a4cfa43a4c57c83813253626916b8c7ada83476"},
    {file = "PyYAML-6.0.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:0833f8694549e586547b576dcfaba4a6b55b9e96098b36cdc7ebefe667dfed48"},
    {file = "PyYAML-6.0.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:8b9c7197f7cb2738065c481a0461e50ad02f18c78cd75775628afb4d7137fb3b"},
    {file = "PyYAML-6.0.2-cp312-cp312-win32.whl", hash = "sha256:ef6107725bd54b262d6dedcc2af448a266975032bc85ef0172c5f059da6325b4"},
    {file = "PyYAML-6.0.2-cp312-cp312-win_amd64.whl", hash = "sha256:7e7401d0de89a9a855c839bc697c079a4af81cf878373abd7dc625847d25cbd8"},
    {file = "PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba"},
    {file = "PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1"},
    {file = "PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133"},
    {file = "PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484"},
    {file = "PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5"},
    {file = "PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc"},
    {file = "PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652"},
    {file = "PyYAML-6.0.2-cp313-cp313-win32.whl", hash = "sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183"},
    {file = "PyYAML-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563"},
    {file = "pyyaml-6.0.2.tar.gz", hash = "sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e"},
]

[[package]]
name = "requests"
version = "2.32.3"
requires_python = ">=3.8"
summary = "Python HTTP for Humans."
groups = ["default"]
dependencies = [
    "certifi>=2017.4.17",
    "charset-normalizer<4,>=2",
    "idna<4,>=2.5",
    "urllib3<3,>=1.21.1",
]
files = [
    {file = "requests-2.32.3-py3-none-any.whl", hash = "sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6"},
    {file = "requests-2.32.3.tar.gz", hash = "sha256:55365417734eb18255590a9ff9eb97e9e1da868d4ccd6402399eaf68af20a760"},
]

[[package]]
name = "rich"
version = "14.0.0"
requires_python = ">=3.8.0"
summary = "Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal"
groups = ["default"]
dependencies = [
    "markdown-it-py>=2.2.0",
    "pygments<3.0.0,>=2.13.0",
    "typing-extensions<5.0,>=4.0.0; python_version < \"3.11\"",
]
files = [
    {file = "rich-14.0.0-py3-none-any.whl", hash = "sha256:1c9491e1951aac09caffd42f448ee3d04e58923ffe14993f6e83068dc395d7e0"},
    {file = "rich-14.0.0.tar.gz", hash = "sha256:82f1bc23a6a21ebca4ae0c45af9bdbc492ed20231dcb63f297d6d1021a9d5725"},
]

[[package]]
name = "ruff"
version = "0.11.11"
requires_python = ">=3.7"
summary = "An extremely fast Python linter and code formatter, written in Rust."
groups = ["dev"]
files = [
    {file = "ruff-0.11.11-py3-none-linux_armv6l.whl", hash = "sha256:9924e5ae54125ed8958a4f7de320dab7380f6e9fa3195e3dc3b137c6842a0092"},
    {file = "ruff-0.11.11-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:c8a93276393d91e952f790148eb226658dd275cddfde96c6ca304873f11d2ae4"},
    {file = "ruff-0.11.11-py3-none-macosx_11_0_arm64.whl", hash = "sha256:d6e333dbe2e6ae84cdedefa943dfd6434753ad321764fd937eef9d6b62022bcd"},
    {file = "ruff-0.11.11-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:7885d9a5e4c77b24e8c88aba8c80be9255fa22ab326019dac2356cff42089fc6"},
    {file = "ruff-0.11.11-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:1b5ab797fcc09121ed82e9b12b6f27e34859e4227080a42d090881be888755d4"},
    {file = "ruff-0.11.11-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e231ff3132c1119ece836487a02785f099a43992b95c2f62847d29bace3c75ac"},
    {file = "ruff-0.11.11-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:a97c9babe1d4081037a90289986925726b802d180cca784ac8da2bbbc335f709"},
    {file = "ruff-0.11.11-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d8c4ddcbe8a19f59f57fd814b8b117d4fcea9bee7c0492e6cf5fdc22cfa563c8"},
    {file = "ruff-0.11.11-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:6224076c344a7694c6fbbb70d4f2a7b730f6d47d2a9dc1e7f9d9bb583faf390b"},
    {file = "ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:882821fcdf7ae8db7a951df1903d9cb032bbe838852e5fc3c2b6c3ab54e39875"},
    {file = "ruff-0.11.11-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:dcec2d50756463d9df075a26a85a6affbc1b0148873da3997286caf1ce03cae1"},
    {file = "ruff-0.11.11-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:99c28505ecbaeb6594701a74e395b187ee083ee26478c1a795d35084d53ebd81"},
    {file = "ruff-0.11.11-py3-none-musllinux_1_2_i686.whl", hash = "sha256:9263f9e5aa4ff1dec765e99810f1cc53f0c868c5329b69f13845f699fe74f639"},
    {file = "ruff-0.11.11-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:64ac6f885e3ecb2fdbb71de2701d4e34526651f1e8503af8fb30d4915a3fe345"},
    {file = "ruff-0.11.11-py3-none-win32.whl", hash = "sha256:1adcb9a18802268aaa891ffb67b1c94cd70578f126637118e8099b8e4adcf112"},
    {file = "ruff-0.11.11-py3-none-win_amd64.whl", hash = "sha256:748b4bb245f11e91a04a4ff0f96e386711df0a30412b9fe0c74d5bdc0e4a531f"},
    {file = "ruff-0.11.11-py3-none-win_arm64.whl", hash = "sha256:6c51f136c0364ab1b774767aa8b86331bd8e9d414e2d107db7a2189f35ea1f7b"},
    {file = "ruff-0.11.11.tar.gz", hash = "sha256:7774173cc7c1980e6bf67569ebb7085989a78a103922fb83ef3dfe230cd0687d"},
]

[[package]]
name = "smolagents"
version = "1.16.1"
requires_python = ">=3.10"
summary = "🤗 smolagents: a barebones library for agents. Agents write python code to call tools or orchestrate other agents."
groups = ["default"]
dependencies = [
    "huggingface-hub>=0.31.2",
    "jinja2>=3.1.4",
    "pillow>=10.0.1",
    "python-dotenv",
    "requests>=2.32.3",
    "rich>=13.9.4",
]
files = [
    {file = "smolagents-1.16.1-py3-none-any.whl", hash = "sha256:21407b39c1292b0c9b326c54042e1fe88f5bed23e095f31bd75cc467cd89d083"},
    {file = "smolagents-1.16.1.tar.gz", hash = "sha256:189f61332fb71ce2e9a5fd6f9a111cdce1333b3991a04e7044c510630e338978"},
]

[[package]]
name = "sniffio"
version = "1.3.1"
requires_python = ">=3.7"
summary = "Sniff out which async library your code is running under"
groups = ["default"]
files = [
    {file = "sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2"},
    {file = "sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc"},
]

[[package]]
name = "sse-starlette"
version = "2.3.5"
requires_python = ">=3.9"
summary = "SSE plugin for Starlette"
groups = ["default"]
dependencies = [
    "anyio>=4.7.0",
    "starlette>=0.41.3",
]
files = [
    {file = "sse_starlette-2.3.5-py3-none-any.whl", hash = "sha256:251708539a335570f10eaaa21d1848a10c42ee6dc3a9cf37ef42266cdb1c52a8"},
    {file = "sse_starlette-2.3.5.tar.gz", hash = "sha256:228357b6e42dcc73a427990e2b4a03c023e2495ecee82e14f07ba15077e334b2"},
]

[[package]]
name = "starlette"
version = "0.46.2"
requires_python = ">=3.9"
summary = "The little ASGI library that shines."
groups = ["default"]
dependencies = [
    "anyio<5,>=3.6.2",
    "typing-extensions>=3.10.0; python_version < \"3.10\"",
]
files = [
    {file = "starlette-0.46.2-py3-none-any.whl", hash = "sha256:595633ce89f8ffa71a015caed34a5b2dc1c0cdb3f0f1fbd1e69339cf2abeec35"},
    {file = "starlette-0.46.2.tar.gz", hash = "sha256:7f7361f34eed179294600af672f565727419830b54b7b084efe44bb82d2fccd5"},
]

[[package]]
name = "tomli"
version = "2.2.1"
requires_python = ">=3.8"
summary = "A lil' TOML parser"
groups = ["dev"]
marker = "python_version < \"3.11\""
files = [
    {file = "tomli-2.2.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:678e4fa69e4575eb77d103de3df8a895e1591b48e740211bd1067378c69e8249"},
    {file = "tomli-2.2.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:023aa114dd824ade0100497eb2318602af309e5a55595f76b626d6d9f3b7b0a6"},
    {file = "tomli-2.2.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ece47d672db52ac607a3d9599a9d48dcb2f2f735c6c2d1f34130085bb12b112a"},
    {file = "tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6972ca9c9cc9f0acaa56a8ca1ff51e7af152a9f87fb64623e31d5c83700080ee"},
    {file = "tomli-2.2.1-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c954d2250168d28797dd4e3ac5cf812a406cd5a92674ee4c8f123c889786aa8e"},
    {file = "tomli-2.2.1-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:8dd28b3e155b80f4d54beb40a441d366adcfe740969820caf156c019fb5c7ec4"},
    {file = "tomli-2.2.1-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:e59e304978767a54663af13c07b3d1af22ddee3bb2fb0618ca1593e4f593a106"},
    {file = "tomli-2.2.1-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:33580bccab0338d00994d7f16f4c4ec25b776af3ffaac1ed74e0b3fc95e885a8"},
    {file = "tomli-2.2.1-cp311-cp311-win32.whl", hash = "sha256:465af0e0875402f1d226519c9904f37254b3045fc5084697cefb9bdde1ff99ff"},
    {file = "tomli-2.2.1-cp311-cp311-win_amd64.whl", hash = "sha256:2d0f2fdd22b02c6d81637a3c95f8cd77f995846af7414c5c4b8d0545afa1bc4b"},
    {file = "tomli-2.2.1-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:4a8f6e44de52d5e6c657c9fe83b562f5f4256d8ebbfe4ff922c495620a7f6cea"},
    {file = "tomli-2.2.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:8d57ca8095a641b8237d5b079147646153d22552f1c637fd3ba7f4b0b29167a8"},
    {file = "tomli-2.2.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4e340144ad7ae1533cb897d406382b4b6fede8890a03738ff1683af800d54192"},
    {file = "tomli-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:db2b95f9de79181805df90bedc5a5ab4c165e6ec3fe99f970d0e302f384ad222"},
    {file = "tomli-2.2.1-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:40741994320b232529c802f8bc86da4e1aa9f413db394617b9a256ae0f9a7f77"},
    {file = "tomli-2.2.1-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:400e720fe168c0f8521520190686ef8ef033fb19fc493da09779e592861b78c6"},
    {file = "tomli-2.2.1-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:02abe224de6ae62c19f090f68da4e27b10af2b93213d36cf44e6e1c5abd19fdd"},
    {file = "tomli-2.2.1-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:b82ebccc8c8a36f2094e969560a1b836758481f3dc360ce9a3277c65f374285e"},
    {file = "tomli-2.2.1-cp312-cp312-win32.whl", hash = "sha256:889f80ef92701b9dbb224e49ec87c645ce5df3fa2cc548664eb8a25e03127a98"},
    {file = "tomli-2.2.1-cp312-cp312-win_amd64.whl", hash = "sha256:7fc04e92e1d624a4a63c76474610238576942d6b8950a2d7f908a340494e67e4"},
    {file = "tomli-2.2.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:f4039b9cbc3048b2416cc57ab3bda989a6fcf9b36cf8937f01a6e731b64f80d7"},
    {file = "tomli-2.2.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:286f0ca2ffeeb5b9bd4fcc8d6c330534323ec51b2f52da063b11c502da16f30c"},
    {file = "tomli-2.2.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a92ef1a44547e894e2a17d24e7557a5e85a9e1d0048b0b5e7541f76c5032cb13"},
    {file = "tomli-2.2.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9316dc65bed1684c9a98ee68759ceaed29d229e985297003e494aa825ebb0281"},
    {file = "tomli-2.2.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e85e99945e688e32d5a35c1ff38ed0b3f41f43fad8df0bdf79f72b2ba7bc5272"},
    {file = "tomli-2.2.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:ac065718db92ca818f8d6141b5f66369833d4a80a9d74435a268c52bdfa73140"},
    {file = "tomli-2.2.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:d920f33822747519673ee656a4b6ac33e382eca9d331c87770faa3eef562aeb2"},
    {file = "tomli-2.2.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:a198f10c4d1b1375d7687bc25294306e551bf1abfa4eace6650070a5c1ae2744"},
    {file = "tomli-2.2.1-cp313-cp313-win32.whl", hash = "sha256:d3f5614314d758649ab2ab3a62d4f2004c825922f9e370b29416484086b264ec"},
    {file = "tomli-2.2.1-cp313-cp313-win_amd64.whl", hash = "sha256:a38aa0308e754b0e3c67e344754dff64999ff9b513e691d0e786265c93583c69"},
    {file = "tomli-2.2.1-py3-none-any.whl", hash = "sha256:cb55c73c5f4408779d0cf3eef9f762b9c9f147a77de7b258bef0a5628adc85cc"},
    {file = "tomli-2.2.1.tar.gz", hash = "sha256:cd45e1dc79c835ce60f7404ec8119f2eb06d38b1deba146f07ced3bbc44505ff"},
]

[[package]]
name = "tqdm"
version = "4.67.1"
requires_python = ">=3.7"
summary = "Fast, Extensible Progress Meter"
groups = ["default"]
dependencies = [
    "colorama; platform_system == \"Windows\"",
]
files = [
    {file = "tqdm-4.67.1-py3-none-any.whl", hash = "sha256:26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2"},
    {file = "tqdm-4.67.1.tar.gz", hash = "sha256:f8aef9c52c08c13a65f30ea34f4e5aac3fd1a34959879d7e59e63027286627f2"},
]

[[package]]
name = "typing-extensions"
version = "4.13.2"
requires_python = ">=3.8"
summary = "Backported and Experimental Type Hints for Python 3.8+"
groups = ["default", "dev"]
files = [
    {file = "typing_extensions-4.13.2-py3-none-any.whl", hash = "sha256:a439e7c04b49fec3e5d3e2beaa21755cadbbdc391694e28ccdd36ca4a1408f8c"},
    {file = "typing_extensions-4.13.2.tar.gz", hash = "sha256:e6c81219bd689f51865d9e372991c540bda33a0379d5573cddb9a3a23f7caaef"},
]

[[package]]
name = "typing-inspection"
version = "0.4.1"
requires_python = ">=3.9"
summary = "Runtime typing introspection tools"
groups = ["default"]
dependencies = [
    "typing-extensions>=4.12.0",
]
files = [
    {file = "typing_inspection-0.4.1-py3-none-any.whl", hash = "sha256:389055682238f53b04f7badcb49b989835495a96700ced5dab2d8feae4b26f51"},
    {file = "typing_inspection-0.4.1.tar.gz", hash = "sha256:6ae134cc0203c33377d43188d4064e9b357dba58cff3185f22924610e70a9d28"},
]

[[package]]
name = "urllib3"
version = "2.4.0"
requires_python = ">=3.9"
summary = "HTTP library with thread-safe connection pooling, file post, and more."
groups = ["default"]
files = [
    {file = "urllib3-2.4.0-py3-none-any.whl", hash = "sha256:4e16665048960a0900c702d4a66415956a584919c03361cac9f1df5c5dd7e813"},
    {file = "urllib3-2.4.0.tar.gz", hash = "sha256:414bc6535b787febd7567804cc015fee39daab8ad86268f1310a9250697de466"},
]

[[package]]
name = "uvicorn"
version = "0.34.2"
requires_python = ">=3.9"
summary = "The lightning-fast ASGI server."
groups = ["default"]
dependencies = [
    "click>=7.0",
    "h11>=0.8",
    "typing-extensions>=4.0; python_version < \"3.11\"",
]
files = [
    {file = "uvicorn-0.34.2-py3-none-any.whl", hash = "sha256:deb49af569084536d269fe0a6d67e3754f104cf03aba7c11c40f01aadf33c403"},
    {file = "uvicorn-0.34.2.tar.gz", hash = "sha256:0e929828f6186353a80b58ea719861d2629d766293b6d19baf086ba31d4f3328"},
]

[[package]]
name = "uvicorn"
version = "0.34.2"
extras = ["standard"]
requires_python = ">=3.9"
summary = "The lightning-fast ASGI server."
groups = ["default"]
dependencies = [
    "colorama>=0.4; sys_platform == \"win32\"",
    "httptools>=0.6.3",
    "python-dotenv>=0.13",
    "pyyaml>=5.1",
    "uvicorn==0.34.2",
    "uvloop!=0.15.0,!=0.15.1,>=0.14.0; (sys_platform != \"cygwin\" and sys_platform != \"win32\") and platform_python_implementation != \"PyPy\"",
    "watchfiles>=0.13",
    "websockets>=10.4",
]
files = [
    {file = "uvicorn-0.34.2-py3-none-any.whl", hash = "sha256:deb49af569084536d269fe0a6d67e3754f104cf03aba7c11c40f01aadf33c403"},
    {file = "uvicorn-0.34.2.tar.gz", hash = "sha256:0e929828f6186353a80b58ea719861d2629d766293b6d19baf086ba31d4f3328"},
]

[[package]]
name = "uvloop"
version = "0.21.0"
requires_python = ">=3.8.0"
summary = "Fast implementation of asyncio event loop on top of libuv"
groups = ["default"]
marker = "(sys_platform != \"cygwin\" and sys_platform != \"win32\") and platform_python_implementation != \"PyPy\""
files = [
    {file = "uvloop-0.21.0-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:ec7e6b09a6fdded42403182ab6b832b71f4edaf7f37a9a0e371a01db5f0cb45f"},
    {file = "uvloop-0.21.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:196274f2adb9689a289ad7d65700d37df0c0930fd8e4e743fa4834e850d7719d"},
    {file = "uvloop-0.21.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f38b2e090258d051d68a5b14d1da7203a3c3677321cf32a95a6f4db4dd8b6f26"},
    {file = "uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:87c43e0f13022b998eb9b973b5e97200c8b90823454d4bc06ab33829e09fb9bb"},
    {file = "uvloop-0.21.0-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:10d66943def5fcb6e7b37310eb6b5639fd2ccbc38df1177262b0640c3ca68c1f"},
    {file = "uvloop-0.21.0-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:67dd654b8ca23aed0a8e99010b4c34aca62f4b7fce88f39d452ed7622c94845c"},
    {file = "uvloop-0.21.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:c0f3fa6200b3108919f8bdabb9a7f87f20e7097ea3c543754cabc7d717d95cf8"},
    {file = "uvloop-0.21.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:0878c2640cf341b269b7e128b1a5fed890adc4455513ca710d77d5e93aa6d6a0"},
    {file = "uvloop-0.21.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b9fb766bb57b7388745d8bcc53a359b116b8a04c83a2288069809d2b3466c37e"},
    {file = "uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8a375441696e2eda1c43c44ccb66e04d61ceeffcd76e4929e527b7fa401b90fb"},
    {file = "uvloop-0.21.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:baa0e6291d91649c6ba4ed4b2f982f9fa165b5bbd50a9e203c416a2797bab3c6"},
    {file = "uvloop-0.21.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:4509360fcc4c3bd2c70d87573ad472de40c13387f5fda8cb58350a1d7475e58d"},
    {file = "uvloop-0.21.0-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:359ec2c888397b9e592a889c4d72ba3d6befba8b2bb01743f72fffbde663b59c"},
    {file = "uvloop-0.21.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:f7089d2dc73179ce5ac255bdf37c236a9f914b264825fdaacaded6990a7fb4c2"},
    {file = "uvloop-0.21.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:baa4dcdbd9ae0a372f2167a207cd98c9f9a1ea1188a8a526431eef2f8116cc8d"},
    {file = "uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:86975dca1c773a2c9864f4c52c5a55631038e387b47eaf56210f873887b6c8dc"},
    {file = "uvloop-0.21.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:461d9ae6660fbbafedd07559c6a2e57cd553b34b0065b6550685f6653a98c1cb"},
    {file = "uvloop-0.21.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:183aef7c8730e54c9a3ee3227464daed66e37ba13040bb3f350bc2ddc040f22f"},
    {file = "uvloop-0.21.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:bfd55dfcc2a512316e65f16e503e9e450cab148ef11df4e4e679b5e8253a5281"},
    {file = "uvloop-0.21.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:787ae31ad8a2856fc4e7c095341cccc7209bd657d0e71ad0dc2ea83c4a6fa8af"},
    {file = "uvloop-0.21.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5ee4d4ef48036ff6e5cfffb09dd192c7a5027153948d85b8da7ff705065bacc6"},
    {file = "uvloop-0.21.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f3df876acd7ec037a3d005b3ab85a7e4110422e4d9c1571d4fc89b0fc41b6816"},
    {file = "uvloop-0.21.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:bd53ecc9a0f3d87ab847503c2e1552b690362e005ab54e8a48ba97da3924c0dc"},
    {file = "uvloop-0.21.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:a5c39f217ab3c663dc699c04cbd50c13813e31d917642d459fdcec07555cc553"},
    {file = "uvloop-0.21.0.tar.gz", hash = "sha256:3bf12b0fda68447806a7ad847bfa591613177275d35b6724b1ee573faa3704e3"},
]

[[package]]
name = "virtualenv"
version = "20.31.2"
requires_python = ">=3.8"
summary = "Virtual Python Environment builder"
groups = ["dev"]
dependencies = [
    "distlib<1,>=0.3.7",
    "filelock<4,>=3.12.2",
    "importlib-metadata>=6.6; python_version < \"3.8\"",
    "platformdirs<5,>=3.9.1",
]
files = [
    {file = "virtualenv-20.31.2-py3-none-any.whl", hash = "sha256:36efd0d9650ee985f0cad72065001e66d49a6f24eb44d98980f630686243cf11"},
    {file = "virtualenv-20.31.2.tar.gz", hash = "sha256:e10c0a9d02835e592521be48b332b6caee6887f332c111aa79a09b9e79efc2af"},
]

[[package]]
name = "watchfiles"
version = "1.0.5"
requires_python = ">=3.9"
summary = "Simple, modern and high performance file watching and code reload in python."
groups = ["default"]
dependencies = [
    "anyio>=3.0.0",
]
files = [
    {file = "watchfiles-1.0.5-cp310-cp310-macosx_10_12_x86_64.whl", hash = "sha256:5c40fe7dd9e5f81e0847b1ea64e1f5dd79dd61afbedb57759df06767ac719b40"},
    {file = "watchfiles-1.0.5-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:8c0db396e6003d99bb2d7232c957b5f0b5634bbd1b24e381a5afcc880f7373fb"},
    {file = "watchfiles-1.0.5-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b551d4fb482fc57d852b4541f911ba28957d051c8776e79c3b4a51eb5e2a1b11"},
    {file = "watchfiles-1.0.5-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:830aa432ba5c491d52a15b51526c29e4a4b92bf4f92253787f9726fe01519487"},
    {file = "watchfiles-1.0.5-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a16512051a822a416b0d477d5f8c0e67b67c1a20d9acecb0aafa3aa4d6e7d256"},
    {file = "watchfiles-1.0.5-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:bfe0cbc787770e52a96c6fda6726ace75be7f840cb327e1b08d7d54eadc3bc85"},
    {file = "watchfiles-1.0.5-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d363152c5e16b29d66cbde8fa614f9e313e6f94a8204eaab268db52231fe5358"},
    {file = "watchfiles-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7ee32c9a9bee4d0b7bd7cbeb53cb185cf0b622ac761efaa2eba84006c3b3a614"},
    {file = "watchfiles-1.0.5-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:29c7fd632ccaf5517c16a5188e36f6612d6472ccf55382db6c7fe3fcccb7f59f"},
    {file = "watchfiles-1.0.5-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:8e637810586e6fe380c8bc1b3910accd7f1d3a9a7262c8a78d4c8fb3ba6a2b3d"},
    {file = "watchfiles-1.0.5-cp310-cp310-win32.whl", hash = "sha256:cd47d063fbeabd4c6cae1d4bcaa38f0902f8dc5ed168072874ea11d0c7afc1ff"},
    {file = "watchfiles-1.0.5-cp310-cp310-win_amd64.whl", hash = "sha256:86c0df05b47a79d80351cd179893f2f9c1b1cae49d96e8b3290c7f4bd0ca0a92"},
    {file = "watchfiles-1.0.5-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:237f9be419e977a0f8f6b2e7b0475ababe78ff1ab06822df95d914a945eac827"},
    {file = "watchfiles-1.0.5-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:e0da39ff917af8b27a4bdc5a97ac577552a38aac0d260a859c1517ea3dc1a7c4"},
    {file = "watchfiles-1.0.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2cfcb3952350e95603f232a7a15f6c5f86c5375e46f0bd4ae70d43e3e063c13d"},
    {file = "watchfiles-1.0.5-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:68b2dddba7a4e6151384e252a5632efcaa9bc5d1c4b567f3cb621306b2ca9f63"},
    {file = "watchfiles-1.0.5-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:95cf944fcfc394c5f9de794ce581914900f82ff1f855326f25ebcf24d5397418"},
    {file = "watchfiles-1.0.5-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ecf6cd9f83d7c023b1aba15d13f705ca7b7d38675c121f3cc4a6e25bd0857ee9"},
    {file = "watchfiles-1.0.5-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:852de68acd6212cd6d33edf21e6f9e56e5d98c6add46f48244bd479d97c967c6"},
    {file = "watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d5730f3aa35e646103b53389d5bc77edfbf578ab6dab2e005142b5b80a35ef25"},
    {file = "watchfiles-1.0.5-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:18b3bd29954bc4abeeb4e9d9cf0b30227f0f206c86657674f544cb032296acd5"},
    {file = "watchfiles-1.0.5-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:ba5552a1b07c8edbf197055bc9d518b8f0d98a1c6a73a293bc0726dce068ed01"},
    {file = "watchfiles-1.0.5-cp311-cp311-win32.whl", hash = "sha256:2f1fefb2e90e89959447bc0420fddd1e76f625784340d64a2f7d5983ef9ad246"},
    {file = "watchfiles-1.0.5-cp311-cp311-win_amd64.whl", hash = "sha256:b6e76ceb1dd18c8e29c73f47d41866972e891fc4cc7ba014f487def72c1cf096"},
    {file = "watchfiles-1.0.5-cp311-cp311-win_arm64.whl", hash = "sha256:266710eb6fddc1f5e51843c70e3bebfb0f5e77cf4f27129278c70554104d19ed"},
    {file = "watchfiles-1.0.5-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:b5eb568c2aa6018e26da9e6c86f3ec3fd958cee7f0311b35c2630fa4217d17f2"},
    {file = "watchfiles-1.0.5-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:0a04059f4923ce4e856b4b4e5e783a70f49d9663d22a4c3b3298165996d1377f"},
    {file = "watchfiles-1.0.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3e380c89983ce6e6fe2dd1e1921b9952fb4e6da882931abd1824c092ed495dec"},
    {file = "watchfiles-1.0.5-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:fe43139b2c0fdc4a14d4f8d5b5d967f7a2777fd3d38ecf5b1ec669b0d7e43c21"},
    {file = "watchfiles-1.0.5-cp312-cp312-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ee0822ce1b8a14fe5a066f93edd20aada932acfe348bede8aa2149f1a4489512"},
    {file = "watchfiles-1.0.5-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a0dbcb1c2d8f2ab6e0a81c6699b236932bd264d4cef1ac475858d16c403de74d"},
    {file = "watchfiles-1.0.5-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a2014a2b18ad3ca53b1f6c23f8cd94a18ce930c1837bd891262c182640eb40a6"},
    {file = "watchfiles-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:10f6ae86d5cb647bf58f9f655fcf577f713915a5d69057a0371bc257e2553234"},
    {file = "watchfiles-1.0.5-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:1a7bac2bde1d661fb31f4d4e8e539e178774b76db3c2c17c4bb3e960a5de07a2"},
    {file = "watchfiles-1.0.5-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:4ab626da2fc1ac277bbf752446470b367f84b50295264d2d313e28dc4405d663"},
    {file = "watchfiles-1.0.5-cp312-cp312-win32.whl", hash = "sha256:9f4571a783914feda92018ef3901dab8caf5b029325b5fe4558c074582815249"},
    {file = "watchfiles-1.0.5-cp312-cp312-win_amd64.whl", hash = "sha256:360a398c3a19672cf93527f7e8d8b60d8275119c5d900f2e184d32483117a705"},
    {file = "watchfiles-1.0.5-cp312-cp312-win_arm64.whl", hash = "sha256:1a2902ede862969077b97523987c38db28abbe09fb19866e711485d9fbf0d417"},
    {file = "watchfiles-1.0.5-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:0b289572c33a0deae62daa57e44a25b99b783e5f7aed81b314232b3d3c81a11d"},
    {file = "watchfiles-1.0.5-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:a056c2f692d65bf1e99c41045e3bdcaea3cb9e6b5a53dcaf60a5f3bd95fc9763"},
    {file = "watchfiles-1.0.5-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b9dca99744991fc9850d18015c4f0438865414e50069670f5f7eee08340d8b40"},
    {file = "watchfiles-1.0.5-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:894342d61d355446d02cd3988a7326af344143eb33a2fd5d38482a92072d9563"},
    {file = "watchfiles-1.0.5-cp313-cp313-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ab44e1580924d1ffd7b3938e02716d5ad190441965138b4aa1d1f31ea0877f04"},
    {file = "watchfiles-1.0.5-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d6f9367b132078b2ceb8d066ff6c93a970a18c3029cea37bfd7b2d3dd2e5db8f"},
    {file = "watchfiles-1.0.5-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f2e55a9b162e06e3f862fb61e399fe9f05d908d019d87bf5b496a04ef18a970a"},
    {file = "watchfiles-1.0.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0125f91f70e0732a9f8ee01e49515c35d38ba48db507a50c5bdcad9503af5827"},
    {file = "watchfiles-1.0.5-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:13bb21f8ba3248386337c9fa51c528868e6c34a707f729ab041c846d52a0c69a"},
    {file = "watchfiles-1.0.5-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:839ebd0df4a18c5b3c1b890145b5a3f5f64063c2a0d02b13c76d78fe5de34936"},
    {file = "watchfiles-1.0.5-cp313-cp313-win32.whl", hash = "sha256:4a8ec1e4e16e2d5bafc9ba82f7aaecfeec990ca7cd27e84fb6f191804ed2fcfc"},
    {file = "watchfiles-1.0.5-cp313-cp313-win_amd64.whl", hash = "sha256:f436601594f15bf406518af922a89dcaab416568edb6f65c4e5bbbad1ea45c11"},
    {file = "watchfiles-1.0.5-pp310-pypy310_pp73-macosx_10_12_x86_64.whl", hash = "sha256:f59b870db1f1ae5a9ac28245707d955c8721dd6565e7f411024fa374b5362d1d"},
    {file = "watchfiles-1.0.5-pp310-pypy310_pp73-macosx_11_0_arm64.whl", hash = "sha256:9475b0093767e1475095f2aeb1d219fb9664081d403d1dff81342df8cd707034"},
    {file = "watchfiles-1.0.5-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:fc533aa50664ebd6c628b2f30591956519462f5d27f951ed03d6c82b2dfd9965"},
    {file = "watchfiles-1.0.5-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fed1cd825158dcaae36acce7b2db33dcbfd12b30c34317a88b8ed80f0541cc57"},
    {file = "watchfiles-1.0.5.tar.gz", hash = "sha256:b7529b5dcc114679d43827d8c35a07c493ad6f083633d573d81c660abc5979e9"},
]

[[package]]
name = "websockets"
version = "15.0.1"
requires_python = ">=3.9"
summary = "An implementation of the WebSocket Protocol (RFC 6455 & 7692)"
groups = ["default"]
files = [
    {file = "websockets-15.0.1-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:d63efaa0cd96cf0c5fe4d581521d9fa87744540d4bc999ae6e08595a1014b45b"},
    {file = "websockets-15.0.1-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:ac60e3b188ec7574cb761b08d50fcedf9d77f1530352db4eef1707fe9dee7205"},
    {file = "websockets-15.0.1-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:5756779642579d902eed757b21b0164cd6fe338506a8083eb58af5c372e39d9a"},
    {file = "websockets-15.0.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0fdfe3e2a29e4db3659dbd5bbf04560cea53dd9610273917799f1cde46aa725e"},
    {file = "websockets-15.0.1-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:4c2529b320eb9e35af0fa3016c187dffb84a3ecc572bcee7c3ce302bfeba52bf"},
    {file = "websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ac1e5c9054fe23226fb11e05a6e630837f074174c4c2f0fe442996112a6de4fb"},
    {file = "websockets-15.0.1-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:5df592cd503496351d6dc14f7cdad49f268d8e618f80dce0cd5a36b93c3fc08d"},
    {file = "websockets-15.0.1-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:0a34631031a8f05657e8e90903e656959234f3a04552259458aac0b0f9ae6fd9"},
    {file = "websockets-15.0.1-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:3d00075aa65772e7ce9e990cab3ff1de702aa09be3940d1dc88d5abf1ab8a09c"},
    {file = "websockets-15.0.1-cp310-cp310-win32.whl", hash = "sha256:1234d4ef35db82f5446dca8e35a7da7964d02c127b095e172e54397fb6a6c256"},
    {file = "websockets-15.0.1-cp310-cp310-win_amd64.whl", hash = "sha256:39c1fec2c11dc8d89bba6b2bf1556af381611a173ac2b511cf7231622058af41"},
    {file = "websockets-15.0.1-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:823c248b690b2fd9303ba00c4f66cd5e2d8c3ba4aa968b2779be9532a4dad431"},
    {file = "websockets-15.0.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:678999709e68425ae2593acf2e3ebcbcf2e69885a5ee78f9eb80e6e371f1bf57"},
    {file = "websockets-15.0.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:d50fd1ee42388dcfb2b3676132c78116490976f1300da28eb629272d5d93e905"},
    {file = "websockets-15.0.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d99e5546bf73dbad5bf3547174cd6cb8ba7273062a23808ffea025ecb1cf8562"},
    {file = "websockets-15.0.1-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:66dd88c918e3287efc22409d426c8f729688d89a0c587c88971a0faa2c2f3792"},
    {file = "websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8dd8327c795b3e3f219760fa603dcae1dcc148172290a8ab15158cf85a953413"},
    {file = "websockets-15.0.1-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:8fdc51055e6ff4adeb88d58a11042ec9a5eae317a0a53d12c062c8a8865909e8"},
    {file = "websockets-15.0.1-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:693f0192126df6c2327cce3baa7c06f2a117575e32ab2308f7f8216c29d9e2e3"},
    {file = "websockets-15.0.1-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:54479983bd5fb469c38f2f5c7e3a24f9a4e70594cd68cd1fa6b9340dadaff7cf"},
    {file = "websockets-15.0.1-cp311-cp311-win32.whl", hash = "sha256:16b6c1b3e57799b9d38427dda63edcbe4926352c47cf88588c0be4ace18dac85"},
    {file = "websockets-15.0.1-cp311-cp311-win_amd64.whl", hash = "sha256:27ccee0071a0e75d22cb35849b1db43f2ecd3e161041ac1ee9d2352ddf72f065"},
    {file = "websockets-15.0.1-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:3e90baa811a5d73f3ca0bcbf32064d663ed81318ab225ee4f427ad4e26e5aff3"},
    {file = "websockets-15.0.1-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:592f1a9fe869c778694f0aa806ba0374e97648ab57936f092fd9d87f8bc03665"},
    {file = "websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:0701bc3cfcb9164d04a14b149fd74be7347a530ad3bbf15ab2c678a2cd3dd9a2"},
    {file = "websockets-15.0.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e8b56bdcdb4505c8078cb6c7157d9811a85790f2f2b3632c7d1462ab5783d215"},
    {file = "websockets-15.0.1-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0af68c55afbd5f07986df82831c7bff04846928ea8d1fd7f30052638788bc9b5"},
    {file = "websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:64dee438fed052b52e4f98f76c5790513235efaa1ef7f3f2192c392cd7c91b65"},
    {file = "websockets-15.0.1-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:d5f6b181bb38171a8ad1d6aa58a67a6aa9d4b38d0f8c5f496b9e42561dfc62fe"},
    {file = "websockets-15.0.1-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:5d54b09eba2bada6011aea5375542a157637b91029687eb4fdb2dab11059c1b4"},
    {file = "websockets-15.0.1-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:3be571a8b5afed347da347bfcf27ba12b069d9d7f42cb8c7028b5e98bbb12597"},
    {file = "websockets-15.0.1-cp312-cp312-win32.whl", hash = "sha256:c338ffa0520bdb12fbc527265235639fb76e7bc7faafbb93f6ba80d9c06578a9"},
    {file = "websockets-15.0.1-cp312-cp312-win_amd64.whl", hash = "sha256:fcd5cf9e305d7b8338754470cf69cf81f420459dbae8a3b40cee57417f4614a7"},
    {file = "websockets-15.0.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:ee443ef070bb3b6ed74514f5efaa37a252af57c90eb33b956d35c8e9c10a1931"},
    {file = "websockets-15.0.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:5a939de6b7b4e18ca683218320fc67ea886038265fd1ed30173f5ce3f8e85675"},
    {file = "websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:746ee8dba912cd6fc889a8147168991d50ed70447bf18bcda7039f7d2e3d9151"},
    {file = "websockets-15.0.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:595b6c3969023ecf9041b2936ac3827e4623bfa3ccf007575f04c5a6aa318c22"},
    {file = "websockets-15.0.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:3c714d2fc58b5ca3e285461a4cc0c9a66bd0e24c5da9911e30158286c9b5be7f"},
    {file = "websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0f3c1e2ab208db911594ae5b4f79addeb3501604a165019dd221c0bdcabe4db8"},
    {file = "websockets-15.0.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:229cf1d3ca6c1804400b0a9790dc66528e08a6a1feec0d5040e8b9eb14422375"},
    {file = "websockets-15.0.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:756c56e867a90fb00177d530dca4b097dd753cde348448a1012ed6c5131f8b7d"},
    {file = "websockets-15.0.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:558d023b3df0bffe50a04e710bc87742de35060580a293c2a984299ed83bc4e4"},
    {file = "websockets-15.0.1-cp313-cp313-win32.whl", hash = "sha256:ba9e56e8ceeeedb2e080147ba85ffcd5cd0711b89576b83784d8605a7df455fa"},
    {file = "websockets-15.0.1-cp313-cp313-win_amd64.whl", hash = "sha256:e09473f095a819042ecb2ab9465aee615bd9c2028e4ef7d933600a8401c79561"},
    {file = "websockets-15.0.1-pp310-pypy310_pp73-macosx_10_15_x86_64.whl", hash = "sha256:0c9e74d766f2818bb95f84c25be4dea09841ac0f734d1966f415e4edfc4ef1c3"},
    {file = "websockets-15.0.1-pp310-pypy310_pp73-macosx_11_0_arm64.whl", hash = "sha256:1009ee0c7739c08a0cd59de430d6de452a55e42d6b522de7aa15e6f67db0b8e1"},
    {file = "websockets-15.0.1-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:76d1f20b1c7a2fa82367e04982e708723ba0e7b8d43aa643d3dcd404d74f1475"},
    {file = "websockets-15.0.1-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f29d80eb9a9263b8d109135351caf568cc3f80b9928bccde535c235de55c22d9"},
    {file = "websockets-15.0.1-pp310-pypy310_pp73-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b359ed09954d7c18bbc1680f380c7301f92c60bf924171629c5db97febb12f04"},
    {file = "websockets-15.0.1-pp310-pypy310_pp73-win_amd64.whl", hash = "sha256:cad21560da69f4ce7658ca2cb83138fb4cf695a2ba3e475e0559e05991aa8122"},
    {file = "websockets-15.0.1-py3-none-any.whl", hash = "sha256:f7a866fbc1e97b5c617ee4116daaa09b722101d4a3c170c787450ba409f9736f"},
    {file = "websockets-15.0.1.tar.gz", hash = "sha256:82544de02076bafba038ce055ee6412d68da13ab47f0c60cab827346de828dee"},
]
------

project.toml

# This file uses PDM as the build system and dependency manager.
# Reference: [https://pdm.fming.dev](https://pdm.fming.dev)

[project]
name = "aicockpit-backend"
version = "0.1.0-alpha" # Matches APP_VERSION in config.py
description = "Backend API for AiCockpit (ACP), a platform for AI-driven workflows, LLM management, and agent execution."
authors = [
    {name = "Your Name / ACP Project", email = "your_email@example.com"},
]
dependencies = [
    "fastapi>=0.100.0", # Keep FastAPI version reasonably up-to-date
    "uvicorn[standard]>=0.20.0", # For ASGI server, [standard] includes websockets and http-tools
    "pydantic>=2.0.0", # Using Pydantic v2 features
    "pydantic-settings>=2.0.0",
    # python-dotenv is usually a sub-dependency of pydantic-settings, but explicit can be fine
    # "python-dotenv>=1.0.0",
    "sse-starlette>=1.0.0", # For Server-Sent Events
    "llama-cpp-python>=0.2.50", # Specify a recent version, check latest compatible
    "requests>=2.20.0", # General HTTP requests
    "huggingface_hub>=0.18.0", # For Hugging Face Hub interactions
    "smolagents>=0.1.0", # Placeholder version, adjust when integrating
    "httpx>=0.24.0", # For async HTTP requests (can also be used for testing)
    "aiofiles>=23.0.0" # For async file operations if needed by FastAPI (e.g. FileUploads)
]
requires-python = ">=3.10" # Recommended Python version
readme = "README.md" 
license = {text = "Proprietary"} 

[project.urls]
Homepage = "[https://github.com/your_username/aicockpit-backend](https://github.com/your_username/aicockpit-backend)" # Replace with your actual repo URL
Repository = "[https://github.com/your_username/aicockpit-backend](https://github.com/your_username/aicockpit-backend)"

# [project.optional-dependencies]
# # Example: if you have features that require extra dependencies
# # gpu_cuda = ["llama-cpp-python[cuda]"] 
# # gpu_rocm = ["llama-cpp-python[rocm]"]

[tool.pdm]
# PDM specific configurations
# For example, to specify the Python interpreter to use for the project
# python = "3.10"

[tool.pdm.dev-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.20.0",
    "black>=23.0.0", # Code formatter
    "ruff>=0.1.0",   # Linter
    "pre-commit>=3.0.0", # For managing pre-commit hooks
]

[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"

[tool.pdm.scripts]
# Define common scripts for convenience
lint = "ruff check acp_backend tests && black --check acp_backend tests"
format = "ruff --fix acp_backend tests && black acp_backend tests"
test = "pytest"
run = "uvicorn acp_backend.main:app --reload --port 8000"
dev = "uvicorn acp_backend.main:app --reload --port 8000 --log-level debug" 

[tool.ruff]
# Ruff linter configuration ([https://beta.ruff.rs/docs/configuration/](https://beta.ruff.rs/docs/configuration/))
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # Pyflakes
    "I",  # isort
    "C",  # flake8-comprehensions
    "B",  # flake8-bugbear
]
ignore = [
    "E501", # Line too long, handled by black
]
line-length = 120 

[tool.ruff.isort]
known-first-party = ["acp_backend"]

[tool.black]
# Black code formatter configuration
line-length = 120
# target-version = ['py310'] 

[tool.pytest.ini_options]
minversion = "6.0"
addopts = "-ra -q" 
testpaths = [
    "tests",
]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
pythonpath = ["."] # <-- ADDED THIS LINE


-------

pyproject.toml

# This file uses PDM as the build system and dependency manager.
# Reference: [https://pdm.fming.dev](https://pdm.fming.dev)

[project]
name = "aicockpit-backend"
version = "0.1.0-alpha" # Matches APP_VERSION in config.py
description = "Backend API for AiCockpit (ACP), a platform for AI-driven workflows, LLM management, and agent execution."
authors = [
    {name = "Your Name / ACP Project", email = "your_email@example.com"},
]
dependencies = [
    "fastapi>=0.100.0", # Keep FastAPI version reasonably up-to-date
    "uvicorn[standard]>=0.20.0", # For ASGI server, [standard] includes websockets and http-tools
    "pydantic>=2.0.0", # Using Pydantic v2 features
    "pydantic-settings>=2.0.0",
    # python-dotenv is usually a sub-dependency of pydantic-settings, but explicit can be fine
    # "python-dotenv>=1.0.0",
    "sse-starlette>=1.0.0", # For Server-Sent Events
    "llama-cpp-python>=0.2.50", # Specify a recent version, check latest compatible
    "requests>=2.20.0", # General HTTP requests
    "huggingface_hub>=0.18.0", # For Hugging Face Hub interactions
    "smolagents>=0.1.0", # Placeholder version, adjust when integrating
    "httpx>=0.24.0", # For async HTTP requests (can also be used for testing)
    "aiofiles>=23.0.0" # For async file operations if needed by FastAPI (e.g. FileUploads)
]
requires-python = ">=3.10" # Recommended Python version
readme = "README.md" 
license = {text = "Proprietary"} 

[project.urls]
Homepage = "[https://github.com/your_username/aicockpit-backend](https://github.com/your_username/aicockpit-backend)" # Replace with your actual repo URL
Repository = "[https://github.com/your_username/aicockpit-backend](https://github.com/your_username/aicockpit-backend)"

# [project.optional-dependencies]
# # Example: if you have features that require extra dependencies
# # gpu_cuda = ["llama-cpp-python[cuda]"] 
# # gpu_rocm = ["llama-cpp-python[rocm]"]

[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"

[tool.ruff]
# Ruff linter configuration ([https://beta.ruff.rs/docs/configuration/](https://beta.ruff.rs/docs/configuration/))
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # Pyflakes
    "I",  # isort
    "C",  # flake8-comprehensions
    "B",  # flake8-bugbear
]
ignore = [
    "E501", # Line too long, handled by black
]
line-length = 120 

[tool.ruff.isort]
known-first-party = ["acp_backend"]

[tool.black]
# Black code formatter configuration
line-length = 120
# target-version = ['py310'] 

[tool.pytest.ini_options]
minversion = "6.0"
addopts = "-ra -q" 
testpaths = [
    "tests",
]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
pythonpath = ["."] # <-- ADDED THIS LINE

[tool.pdm.scripts]
# Define common scripts for convenience
lint = "ruff check acp_backend tests && black --check acp_backend tests"
format = "ruff --fix acp_backend tests && black acp_backend tests"
test = "pytest"
run = "uvicorn acp_backend.main:app --reload --port 8000"
dev = "uvicorn acp_backend.main:app --reload --port 8000 --log-level debug" 


[dependency-groups]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.20.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "pre-commit>=3.0.0",
    "pytest-mock>=3.14.0",
]
-------

agent_executor.py

import logging
import asyncio # For async sleep, simulating work
import uuid
from typing import List, Dict, Any, Optional, AsyncGenerator

from acp_backend.models.agent_models import AgentConfig, RunAgentRequest, AgentRunStatus, AgentOutputChunk
from acp_backend.core.llm_manager import llm_manager # To potentially verify LLM model ID
from acp_backend.config import settings
# (Planned) from smolagents import CodeAgent, ToolCallingAgent, etc.

logger = logging.getLogger(__name__)

# In-memory cache for agent configurations.
# TODO: Replace with persistent storage (e.g., database or file-based) - Category 2
AGENT_CONFIGS_CACHE: Dict[str, AgentConfig] = {}

class AgentExecutor:
    def __init__(self):
        logger.info("AgentExecutor initialized.")
        # (Planned) Initialize smolagents or other agent frameworks here if needed globally

    async def _get_agent_config_from_cache(self, agent_id: str) -> Optional[AgentConfig]: # Renamed for clarity
        """Retrieves an agent configuration from the cache."""
        return AGENT_CONFIGS_CACHE.get(agent_id)

    async def _initialize_smol_agent(self, agent_config: AgentConfig, task_input: str):
        """
        Placeholder for initializing a smolagent instance based on config.
        This would involve selecting the agent type, setting up its LLM, tools, etc.
        """
        logger.warning("_initialize_smol_agent is a placeholder and not fully implemented.")
        await asyncio.sleep(0.1) # Simulate some async work
        return None # Placeholder

    async def run_agent_task(self, request: RunAgentRequest) -> AgentRunStatus:
        """
        Runs a configured agent for a given task (non-streaming).
        This is currently a placeholder for actual smolagent execution.
        """
        run_id = str(uuid.uuid4())
        start_time_iso = datetime.datetime.now(datetime.timezone.utc).isoformat() # For AgentRunStatus
        logger.info(f"Attempting to run agent_id: {request.agent_id} for run_id: {run_id} with input: '{request.input_prompt[:50]}...'")

        if not settings.ENABLE_AGENT_MODULE:
            logger.warning("Agent module is disabled. Agent run aborted.")
            return AgentRunStatus(run_id=run_id, agent_id=request.agent_id, status="failed", error_message="Agent module is disabled.", start_time=start_time_iso)

        agent_config = await self._get_agent_config_from_cache(request.agent_id)
        if not agent_config:
            logger.error(f"Agent configuration for agent_id '{request.agent_id}' not found.")
            return AgentRunStatus(run_id=run_id, agent_id=request.agent_id, status="failed", error_message=f"Agent configuration for '{request.agent_id}' not found.", start_time=start_time_iso)

        # LLM Model Verification
        if settings.ENABLE_LLM_MODULE and llm_manager:
            if not agent_config.llm_model_id: # Check if an LLM model ID is even configured for the agent
                error_msg = f"Agent '{request.agent_id}' has no LLM model configured (llm_model_id is missing or empty)."
                logger.error(error_msg)
                return AgentRunStatus(run_id=run_id, agent_id=request.agent_id, status="failed", error_message=error_msg, start_time=start_time_iso)

            model_info = await llm_manager.get_model_details(agent_config.llm_model_id)
            if not model_info or not model_info.loaded:
                error_msg = f"LLM model '{agent_config.llm_model_id}' for agent '{request.agent_id}' is not loaded or does not exist."
                logger.error(error_msg)
                return AgentRunStatus(run_id=run_id, agent_id=request.agent_id, status="failed", error_message=error_msg, start_time=start_time_iso)
        elif not settings.ENABLE_LLM_MODULE and agent_config.llm_model_id: # Agent needs LLM but module disabled
            error_msg = f"Agent '{request.agent_id}' requires LLM '{agent_config.llm_model_id}', but the LLM module is disabled."
            logger.error(error_msg)
            return AgentRunStatus(run_id=run_id, agent_id=request.agent_id, status="failed", error_message=error_msg, start_time=start_time_iso)
        # If no llm_model_id is set in agent_config, and LLM module is disabled, it might be fine if agent doesn't need LLM.

        try:
            # (Planned - Category 2) Actual agent execution logic using smolagents
            # agent_instance = await self._initialize_smol_agent(agent_config, request.input_prompt)
            # if not agent_instance:
            #     return AgentRunStatus(run_id=run_id, agent_id=request.agent_id, status="failed", error_message="Failed to initialize agent.", start_time=start_time_iso)
            # result = await agent_instance.run(request.input_prompt)

            # Placeholder execution:
            logger.info(f"Placeholder execution for agent '{request.agent_id}'. Simulating work...")
            await asyncio.sleep(1) # Simulate agent processing time (reduced for quicker tests)
            output_message = f"Agent '{request.agent_id}' processed input: '{request.input_prompt}' (Placeholder Output)"
            placeholder_logs_ref = f"/sessions/placeholder_session/agents/{request.agent_id}/runs/{run_id}/memory_trace.jsonl"

            return AgentRunStatus(
                run_id=run_id,
                agent_id=request.agent_id,
                status="completed",
                output=output_message,
                memory_trace_ref=placeholder_logs_ref,
                error_message=None,
                start_time=start_time_iso,
                end_time=datetime.datetime.now(datetime.timezone.utc).isoformat()
            )

        except Exception as e:
            logger.error(f"Error running agent {request.agent_id} (run_id: {run_id}): {e}", exc_info=True)
            return AgentRunStatus(
                run_id=run_id,
                agent_id=request.agent_id,
                status="failed",
                error_message=str(e),
                start_time=start_time_iso,
                end_time=datetime.datetime.now(datetime.timezone.utc).isoformat()
            )

    async def stream_agent_task_outputs(self, request: RunAgentRequest) -> AsyncGenerator[AgentOutputChunk, None]:
        run_id = str(uuid.uuid4())
        logger.info(f"Attempting to stream agent_id: {request.agent_id} for run_id: {run_id} with input: '{request.input_prompt[:50]}...'")

        if not settings.ENABLE_AGENT_MODULE:
            logger.warning("Agent module is disabled. Agent stream aborted.")
            yield AgentOutputChunk(run_id=run_id, type="error", data={"message": "Agent module is disabled."})
            return

        agent_config = await self._get_agent_config_from_cache(request.agent_id)
        if not agent_config:
            logger.error(f"Agent configuration for agent_id '{request.agent_id}' not found for streaming.")
            yield AgentOutputChunk(run_id=run_id, type="error", data={"message": f"Agent configuration for '{request.agent_id}' not found."})
            return

        # LLM Model Verification (similar to run_agent_task)
        if settings.ENABLE_LLM_MODULE and llm_manager:
            if not agent_config.llm_model_id:
                error_msg = f"Agent '{request.agent_id}' has no LLM model configured (llm_model_id is missing or empty) for streaming."
                logger.error(error_msg)
                yield AgentOutputChunk(run_id=run_id, type="error", data={"message": error_msg})
                return
            model_info = await llm_manager.get_model_details(agent_config.llm_model_id)
            if not model_info or not model_info.loaded:
                error_msg = f"LLM model '{agent_config.llm_model_id}' for agent '{request.agent_id}' is not loaded or does not exist for streaming."
                logger.error(error_msg)
                yield AgentOutputChunk(run_id=run_id, type="error", data={"message": error_msg})
                return
        elif not settings.ENABLE_LLM_MODULE and agent_config.llm_model_id:
            error_msg = f"Agent '{request.agent_id}' requires LLM '{agent_config.llm_model_id}' for streaming, but the LLM module is disabled."
            logger.error(error_msg)
            yield AgentOutputChunk(run_id=run_id, type="error", data={"message": error_msg})
            return

        # (Planned - Category 2) Actual agent streaming logic using smolagents
        # try:
        #     # ...
        # except Exception as e:
        #     # ... yield error chunk ...

        # Placeholder streaming:
        try:
            yield AgentOutputChunk(run_id=run_id, type="status_update", data={"status": "starting", "message": f"Agent {request.agent_id} starting..."})
            await asyncio.sleep(0.2)
            yield AgentOutputChunk(run_id=run_id, type="thought", data="Planning initial approach... (Placeholder)")
            await asyncio.sleep(0.2)
            yield AgentOutputChunk(run_id=run_id, type="log", data="Performing step 1... (Placeholder)")
            await asyncio.sleep(0.3)
            yield AgentOutputChunk(run_id=run_id, type="partial_output", data="Found some preliminary results. (Placeholder)")
            await asyncio.sleep(0.2)
            yield AgentOutputChunk(run_id=run_id, type="log", data="Performing step 2... (Placeholder)")
            await asyncio.sleep(0.3)
            final_output_message = f"Final output for agent {request.agent_id} on input: '{request.input_prompt}' (Placeholder)"
            yield AgentOutputChunk(run_id=run_id, type="final_output", data=final_output_message)
            yield AgentOutputChunk(run_id=run_id, type="status_update", data={"status": "completed"})
        except Exception as e: # Catches errors during the placeholder streaming itself
            logger.error(f"Error during placeholder streaming for agent {request.agent_id} (run_id: {run_id}): {e}", exc_info=True)
            yield AgentOutputChunk(run_id=run_id, type="error", data={"message": f"Streaming error: {str(e)}"})


# Initialize a global instance of the AgentExecutor
agent_executor = AgentExecutor()

# --- Agent Configuration Management (In-Memory Cache - Placeholder for Category 1) ---
# These functions directly manipulate the global cache.
# TODO: Refactor this into AgentExecutor methods interacting with persistent storage (Category 2).

def update_agent_configs_cache(configs: Dict[str, AgentConfig]):
    global AGENT_CONFIGS_CACHE
    AGENT_CONFIGS_CACHE = configs
    logger.info(f"Agent configs cache updated with {len(configs)} configurations.")

def add_agent_config_to_cache(config: AgentConfig):
    global AGENT_CONFIGS_CACHE
    AGENT_CONFIGS_CACHE[config.agent_id] = config
    logger.info(f"Agent config '{config.agent_id}' added/updated in cache.")

def get_agent_config_from_cache(agent_id: str) -> Optional[AgentConfig]:
    return AGENT_CONFIGS_CACHE.get(agent_id)

def list_agent_configs_from_cache() -> List[AgentConfig]:
    return list(AGENT_CONFIGS_CACHE.values())

def delete_agent_config_from_cache(agent_id: str) -> bool:
    global AGENT_CONFIGS_CACHE
    if agent_id in AGENT_CONFIGS_CACHE:
        del AGENT_CONFIGS_CACHE[agent_id]
        logger.info(f"Agent config '{agent_id}' deleted from cache.")
        return True
    return False

# Need datetime for AgentRunStatus timestamps
import datetime
------
fs_manager.py

import os
import logging
import datetime
import shutil # For rmtree (deleting directories) and move
from typing import List, Optional # Union removed as not used

from acp_backend.models.work_board_models import FileNode, ReadFileResponse, WriteFileRequest
from acp_backend.core.session_handler import session_handler # To get session data root

logger = logging.getLogger(__name__)

class FileSystemManager:
    def __init__(self):
        logger.info("FileSystemManager initialized.")

    def _get_session_data_root(self, session_id: str) -> str: # Changed to raise error instead of Optional[str]
        """
        Gets the absolute path to the 'data' subdirectory for a given session.
        This is the root for all WorkBoard operations within that session.
        Raises FileNotFoundError if session data path cannot be determined or is not a directory.
        """
        # session_handler._get_session_data_path can raise ValueError for bad session_id
        try:
            session_data_path = session_handler._get_session_data_path(session_id)
        except ValueError as e: # Propagate error from session_handler
            raise FileNotFoundError(f"Invalid session ID format for WorkBoard operation: {session_id}. Details: {e}") from e

        if os.path.exists(session_data_path) and os.path.isdir(session_data_path):
            return os.path.abspath(session_data_path)
        
        # Log specific reason for failure
        if not os.path.exists(session_data_path):
            log_msg = f"Session data path for session_id '{session_id}' does not exist: {session_data_path}"
        else: # Exists but not a directory
            log_msg = f"Session data path for session_id '{session_id}' is not a directory: {session_data_path}"
        logger.error(log_msg)
        raise FileNotFoundError(f"Work session '{session_id}' data directory is not accessible. {log_msg}")


    def _resolve_path_within_session(self, session_id: str, relative_path: str) -> str: # Changed to raise error
        """
        Resolves a relative path to an absolute path within the session's data directory.
        Ensures the path does not escape the session's data directory (path traversal protection).
        Raises FileNotFoundError if session root cannot be determined or path is outside session root.
        """
        # _get_session_data_root will raise FileNotFoundError if session root is problematic
        session_root = self._get_session_data_root(session_id) 

        # Normalize the relative path: remove leading slashes, handle '..' etc.
        # os.path.normpath handles '..' by attempting to resolve them.
        # We need to ensure that after normalization, it's still within the root.
        # An empty relative_path should resolve to the session_root itself.
        if not relative_path or relative_path == ".":
            normalized_relative_path = "" # Will resolve to session_root
        else:
            normalized_relative_path = os.path.normpath(relative_path.lstrip('/\\'))
            # Disallow paths that try to go above the root after normalization (e.g. ".." or "../something")
            if normalized_relative_path.startswith("..") or "/../" in normalized_relative_path or "\\../" in normalized_relative_path:
                 logger.error(f"Path traversal attempt (post-normalization): session_id='{session_id}', relative_path='{relative_path}', normalized='{normalized_relative_path}'.")
                 raise FileNotFoundError(f"Invalid path: '{relative_path}' attempts to traverse outside allowed directory.")


        absolute_path = os.path.abspath(os.path.join(session_root, normalized_relative_path))

        # Security check: Ensure the resolved path is still within or same as the session_root
        if not absolute_path.startswith(session_root): # os.path.commonprefix is another way
            logger.error(f"Path traversal attempt: session_id='{session_id}', relative_path='{relative_path}'. Resolved to '{absolute_path}' outside session root '{session_root}'.")
            raise FileNotFoundError(f"Access denied: Path '{relative_path}' is outside the session's data directory.")
        return absolute_path

    async def list_dir(self, session_id: str, relative_path: str = ".") -> List[FileNode]:
        logger.debug(f"Listing directory for session_id='{session_id}', relative_path='{relative_path}'")
        # _resolve_path_within_session will raise FileNotFoundError for bad session or path traversal
        absolute_dir_path = self._resolve_path_within_session(session_id, relative_path)

        if not os.path.exists(absolute_dir_path):
            logger.warning(f"Directory to list does not exist: {absolute_dir_path}")
            raise FileNotFoundError(f"Directory not found: {relative_path}")
        if not os.path.isdir(absolute_dir_path):
            logger.warning(f"Path to list is not a directory: {absolute_dir_path}")
            raise NotADirectoryError(f"Not a directory: {relative_path}")

        nodes: List[FileNode] = []
        session_data_root_path = self._get_session_data_root(session_id) # Get once

        try:
            for item_name in os.listdir(absolute_dir_path):
                item_abs_path = os.path.join(absolute_dir_path, item_name)
                item_rel_path_from_session_data_root = os.path.relpath(item_abs_path, session_data_root_path)

                try:
                    stat_info = os.stat(item_abs_path)
                    is_dir = os.path.isdir(item_abs_path)
                    nodes.append(FileNode(
                        name=item_name,
                        path=item_rel_path_from_session_data_root.replace(os.path.sep, '/'),
                        is_dir=is_dir,
                        size_bytes=stat_info.st_size if not is_dir else None,
                        modified_at=datetime.datetime.fromtimestamp(stat_info.st_mtime, tz=datetime.timezone.utc).isoformat()
                    ))
                except OSError as e_stat:
                    logger.error(f"Error stating item {item_abs_path}: {e_stat}", exc_info=True)
                    # Continue to list other items if one fails to stat
            
            nodes.sort(key=lambda x: (not x.is_dir, x.name.lower()))
            return nodes
        except OSError as e_listdir:
            logger.error(f"Error listing directory {absolute_dir_path}: {e_listdir}", exc_info=True)
            raise IOError(f"Could not read directory contents for '{relative_path}': {e_listdir}") from e_listdir


    async def read_file(self, session_id: str, relative_path: str) -> ReadFileResponse:
        logger.debug(f"Reading file for session_id='{session_id}', relative_path='{relative_path}'")
        absolute_file_path = self._resolve_path_within_session(session_id, relative_path)

        if not os.path.exists(absolute_file_path):
            logger.warning(f"File to read does not exist: {absolute_file_path}")
            raise FileNotFoundError(f"File not found: {relative_path}")
        if not os.path.isfile(absolute_file_path):
            logger.warning(f"Path to read is not a file: {absolute_file_path}")
            raise IsADirectoryError(f"Path is a directory, not a file: {relative_path}")

        try:
            with open(absolute_file_path, "r", encoding="utf-8") as f:
                content = f.read()
            return ReadFileResponse(
                path=relative_path.replace(os.path.sep, '/'),
                content=content,
                encoding="utf-8"
            )
        except UnicodeDecodeError as e_decode:
            logger.error(f"Unicode decode error reading file {absolute_file_path}: {e_decode}", exc_info=True)
            raise ValueError(f"Cannot decode file '{relative_path}' with UTF-8. It may be binary or use different encoding.") from e_decode
        except IOError as e_read:
            logger.error(f"Error reading file {absolute_file_path}: {e_read}", exc_info=True)
            raise IOError(f"Could not read file '{relative_path}': {e_read}") from e_read


    async def write_file(self, session_id: str, request: WriteFileRequest) -> FileNode:
        logger.debug(f"Writing file for session_id='{session_id}', relative_path='{request.path}'")
        absolute_file_path = self._resolve_path_within_session(session_id, request.path)

        try:
            parent_dir = os.path.dirname(absolute_file_path)
            if os.path.exists(parent_dir) and not os.path.isdir(parent_dir):
                logger.error(f"Cannot write file. Parent path '{parent_dir}' exists but is not a directory.")
                raise NotADirectoryError(f"Cannot create file, parent path '{os.path.dirname(request.path)}' is a file.")

            os.makedirs(parent_dir, exist_ok=True)
            logger.info(f"Ensured parent directory exists: {parent_dir}")
            
            with open(absolute_file_path, "w", encoding=request.encoding) as f:
                f.write(request.content)
            
            stat_info = os.stat(absolute_file_path)
            return FileNode(
                name=os.path.basename(absolute_file_path),
                path=request.path.replace(os.path.sep, '/'),
                is_dir=False,
                size_bytes=stat_info.st_size,
                modified_at=datetime.datetime.fromtimestamp(stat_info.st_mtime, tz=datetime.timezone.utc).isoformat()
            )
        except IOError as e_write:
            logger.error(f"Error writing file {absolute_file_path}: {e_write}", exc_info=True)
            raise IOError(f"Could not write file '{request.path}': {e_write}") from e_write


    async def delete_item(self, session_id: str, relative_path: str) -> bool:
        logger.debug(f"Deleting item for session_id='{session_id}', relative_path='{relative_path}'")
        # _resolve_path_within_session will raise FileNotFoundError for bad session or path traversal
        absolute_item_path = self._resolve_path_within_session(session_id, relative_path)

        if not os.path.exists(absolute_item_path):
            logger.warning(f"Item to delete not found: {absolute_item_path}")
            return True # Idempotent: if it doesn't exist, it's "deleted"

        try:
            if os.path.isdir(absolute_item_path):
                shutil.rmtree(absolute_item_path)
                logger.info(f"Recursively deleted directory: {absolute_item_path}")
            else:
                os.remove(absolute_item_path)
                logger.info(f"Deleted file: {absolute_item_path}")
            return True
        except OSError as e_delete: # OSError is base for IOError, PermissionError etc.
            logger.error(f"Error deleting item {absolute_item_path}: {e_delete}", exc_info=True)
            raise IOError(f"Could not delete item '{relative_path}': {e_delete}") from e_delete


    async def create_directory(self, session_id: str, relative_path: str) -> FileNode:
        logger.debug(f"Creating directory for session_id='{session_id}', relative_path='{relative_path}'")
        absolute_dir_path = self._resolve_path_within_session(session_id, relative_path)

        if os.path.exists(absolute_dir_path):
            if os.path.isdir(absolute_dir_path):
                logger.info(f"Directory already exists: {absolute_dir_path}")
                stat_info = os.stat(absolute_dir_path)
                return FileNode(
                    name=os.path.basename(absolute_dir_path),
                    path=relative_path.replace(os.path.sep, '/'),
                    is_dir=True,
                    modified_at=datetime.datetime.fromtimestamp(stat_info.st_mtime, tz=datetime.timezone.utc).isoformat()
                )
            else: 
                logger.error(f"Cannot create directory. Path exists and is not a directory: {absolute_dir_path}")
                raise FileExistsError(f"Cannot create directory, path exists as a file: {relative_path}")
        try:
            os.makedirs(absolute_dir_path, exist_ok=True) 
            logger.info(f"Created directory: {absolute_dir_path}")
            stat_info = os.stat(absolute_dir_path)
            return FileNode(
                name=os.path.basename(absolute_dir_path),
                path=relative_path.replace(os.path.sep, '/'),
                is_dir=True,
                modified_at=datetime.datetime.fromtimestamp(stat_info.st_mtime, tz=datetime.timezone.utc).isoformat()
            )
        except OSError as e_mkdir:
            logger.error(f"Error creating directory {absolute_dir_path}: {e_mkdir}", exc_info=True)
            raise IOError(f"Could not create directory '{relative_path}': {e_mkdir}") from e_mkdir


    async def move_item(self, session_id: str, source_relative_path: str, destination_relative_path: str) -> FileNode:
        logger.debug(f"Moving item for session_id='{session_id}' from '{source_relative_path}' to '{destination_relative_path}'")
        abs_source_path = self._resolve_path_within_session(session_id, source_relative_path)
        abs_destination_path = self._resolve_path_within_session(session_id, destination_relative_path)
        
        if not os.path.exists(abs_source_path):
            logger.error(f"Source path for move does not exist: {abs_source_path}")
            raise FileNotFoundError(f"Source path not found: {source_relative_path}")

        if os.path.exists(abs_destination_path):
            logger.error(f"Destination path for move already exists: {abs_destination_path}.")
            raise FileExistsError(f"Destination path already exists: {destination_relative_path}")
        
        try:
            dest_parent_dir = os.path.dirname(abs_destination_path)
            if os.path.exists(dest_parent_dir) and not os.path.isdir(dest_parent_dir):
                 raise NotADirectoryError(f"Cannot move item, parent of destination '{os.path.dirname(destination_relative_path)}' is a file.")
            
            os.makedirs(dest_parent_dir, exist_ok=True) # Ensure parent of destination exists
            logger.info(f"Ensured parent directory for move destination exists: {dest_parent_dir}")

            shutil.move(abs_source_path, abs_destination_path)
            logger.info(f"Moved item from {abs_source_path} to {abs_destination_path}")

            stat_info = os.stat(abs_destination_path)
            is_dir = os.path.isdir(abs_destination_path)
            return FileNode(
                name=os.path.basename(abs_destination_path),
                path=destination_relative_path.replace(os.path.sep, '/'),
                is_dir=is_dir,
                size_bytes=stat_info.st_size if not is_dir else None,
                modified_at=datetime.datetime.fromtimestamp(stat_info.st_mtime, tz=datetime.timezone.utc).isoformat()
            )
        except (OSError, IOError) as e_move: # Catch common FS errors
            logger.error(f"Error moving item from {abs_source_path} to {abs_destination_path}: {e_move}", exc_info=True)
            raise IOError(f"Could not move item from '{source_relative_path}' to '{destination_relative_path}': {e_move}") from e_move

# Initialize a global instance of the FileSystemManager
fs_manager = FileSystemManager()
------
llm_manager.py

import logging
from typing import List, Dict, Any, Optional, AsyncGenerator

from acp_backend.config import settings
from acp_backend.llm_backends.base import BaseLLMBackend
from acp_backend.llm_backends.llama_cpp import LlamaCppBackend
from acp_backend.llm_backends.pie import PIEBackend # Placeholder
from acp_backend.models.llm_models import (
    LLMModelInfo, LoadModelRequest,
    ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk
)

logger = logging.getLogger(__name__)

class LLMManager:
    def __init__(self):
        self.backend: BaseLLMBackend
        backend_type = settings.LLM_BACKEND_TYPE.lower()
        logger.info(f"Initializing LLMManager with backend type: {backend_type}")

        if backend_type == "llama_cpp":
            self.backend = LlamaCppBackend()
            logger.info("LlamaCppBackend selected.")
        elif backend_type == "pie":
            self.backend = PIEBackend() # Placeholder
            logger.info("PIEBackend (Placeholder) selected.")
        else:
            logger.error(f"Unsupported LLM_BACKEND_TYPE: {settings.LLM_BACKEND_TYPE}")
            raise ValueError(f"Unsupported LLM_BACKEND_TYPE: {settings.LLM_BACKEND_TYPE}")
        logger.info("LLMManager initialized successfully.")

    async def list_available_models(self) -> List[LLMModelInfo]:
        """Lists models discoverable by the backend in the configured MODELS_DIR."""
        logger.debug(f"LLMManager forwarding list_available_models to backend. Models_dir: {settings.MODELS_DIR}")
        return await self.backend.discover_models(settings.MODELS_DIR)

    async def load_model(self, request: LoadModelRequest) -> LLMModelInfo:
        """Loads a model using the backend."""
        logger.debug(f"LLMManager forwarding load_model for {request.model_id or request.model_path} to backend.")
        return await self.backend.load_model(request)

    async def unload_model(self, model_id: str) -> LLMModelInfo:
        """Unloads a model using the backend."""
        logger.debug(f"LLMManager forwarding unload_model for {model_id} to backend.")
        return await self.backend.unload_model(model_id)

    async def get_loaded_models_info(self) -> List[LLMModelInfo]:
        """Gets information about all currently loaded models from the backend."""
        logger.debug("LLMManager forwarding get_loaded_models_info to backend.")
        return await self.backend.get_loaded_models()

    async def get_model_details(self, model_id: str) -> Optional[LLMModelInfo]:
        """Gets detailed information for a specific loaded model from the backend."""
        logger.debug(f"LLMManager forwarding get_model_details for {model_id} to backend.")
        return await self.backend.get_model_info(model_id)

    async def process_chat_completion(self, request: ChatCompletionRequest) -> ChatCompletionResponse:
        """Processes a chat completion request (non-streaming) via the backend."""
        logger.debug(f"LLMManager forwarding process_chat_completion for {request.model_id} to backend.")
        return await self.backend.chat_completion(request)

    async def stream_process_chat_completion(self, request: ChatCompletionRequest) -> AsyncGenerator[ChatCompletionChunk, None]:
        """Processes a chat completion request (streaming) via the backend."""
        logger.debug(f"LLMManager forwarding stream_process_chat_completion for {request.model_id} to backend.")
        async for chunk in self.backend.stream_chat_completion(request):
            yield chunk

# Initialize a global instance of the LLMManager
# This might raise an error if config is bad (e.g., unsupported backend type)
try:
    llm_manager = LLMManager()
except ValueError as e:
    logger.critical(f"Failed to initialize LLMManager: {e}. LLM functionalities unavailable.", exc_info=True)
    # Depending on desired behavior, you might re-raise or have a non-functional llm_manager
    raise # Re-raise to prevent app startup if LLM manager is critical
------

session_handler.py

import os
import json
import uuid
import datetime
import logging
import shutil 
from typing import List, Optional # Ensure List and Optional are imported

from acp_backend.config import settings
from acp_backend.models.work_session_models import WorkSession, CreateWorkSessionRequest, UpdateWorkSessionRequest

logger = logging.getLogger(__name__)

SESSION_MANIFEST_FILENAME = "session_manifest.json"
SESSION_DATA_DIRNAME = "data" 

class SessionHandler:
    def __init__(self):
        self.sessions_base_dir = settings.WORK_SESSIONS_DIR
        if not self.sessions_base_dir: 
            _critical_error_msg = "WORK_SESSIONS_DIR is not configured in settings. SessionHandler cannot operate."
            logger.critical(_critical_error_msg)
            raise ValueError(_critical_error_msg)
        try:
            os.makedirs(self.sessions_base_dir, exist_ok=True)
            logger.info(f"SessionHandler initialized. Work sessions directory: {self.sessions_base_dir}")
        except OSError as e:
            logger.critical(f"Failed to create base sessions directory {self.sessions_base_dir}: {e}", exc_info=True)
            raise RuntimeError(f"SessionHandler critical failure: Cannot create base directory {self.sessions_base_dir}. Error: {e}") from e

    def _validate_session_id_format(self, session_id: str):
        if not session_id or ".." in session_id or "/" in session_id or "\\" in session_id or not session_id.strip():
            logger.error(f"Invalid session_id format attempt: '{session_id}'")
            raise ValueError(f"Invalid session_id format: '{session_id}' contains disallowed characters or is empty.")
        if len(session_id) > 255: 
             raise ValueError(f"Invalid session_id format: '{session_id}' is too long.")

    def _get_session_folder_path(self, session_id: str) -> str:
        self._validate_session_id_format(session_id) 
        return os.path.join(self.sessions_base_dir, session_id)

    def _get_session_manifest_path(self, session_id: str) -> str:
        return os.path.join(self._get_session_folder_path(session_id), SESSION_MANIFEST_FILENAME)

    def _get_session_data_path(self, session_id: str) -> str:
        return os.path.join(self._get_session_folder_path(session_id), SESSION_DATA_DIRNAME)

    async def create_session(self, request: CreateWorkSessionRequest) -> WorkSession:
        session_id = str(uuid.uuid4()) 
        now_iso = datetime.datetime.now(datetime.timezone.utc).isoformat()
        
        session_folder = self._get_session_folder_path(session_id) 
        session_data_dir = self._get_session_data_path(session_id)

        try:
            os.makedirs(session_folder, exist_ok=False)
            os.makedirs(session_data_dir, exist_ok=True) 
            logger.info(f"Created directory structure for new session: {session_folder} and data subdir: {session_data_dir}")
        except FileExistsError as e_fe: 
            logger.error(f"Session folder {session_folder} already exists for new UUID {session_id}. This is highly unlikely.", exc_info=True)
            raise RuntimeError(f"Failed to create session: A directory for session ID {session_id} already exists (UUID collision?).") from e_fe
        except OSError as e_os:
            logger.error(f"Failed to create directory for session {session_id} at {session_folder}: {e_os}", exc_info=True)
            raise RuntimeError(f"Could not create session directory structure: {e_os}") from e_os

        session_data = WorkSession(
            session_id=session_id,
            name=request.name,
            description=request.description,
            created_at=now_iso,
            last_accessed=now_iso
        )
        
        manifest_path = self._get_session_manifest_path(session_id)
        try:
            with open(manifest_path, "w", encoding="utf-8") as f:
                json.dump(session_data.model_dump(mode="json"), f, indent=4)
            logger.info(f"Created new work session '{request.name}' with ID: {session_id}")
            return session_data
        except IOError as e_io:
            logger.error(f"Failed to write session manifest for {session_id} at {manifest_path}: {e_io}", exc_info=True)
            if os.path.exists(session_folder):
                try:
                    shutil.rmtree(session_folder)
                    logger.info(f"Cleaned up session folder {session_folder} after manifest write failure.")
                except OSError as cleanup_e:
                    logger.error(f"Failed to cleanup session folder {session_folder}: {cleanup_e}", exc_info=True)
            raise RuntimeError(f"Could not write session manifest: {e_io}") from e_io

    async def list_sessions(self) -> List[WorkSession]:
        sessions: List[WorkSession] = []
        if not os.path.exists(self.sessions_base_dir) or not os.path.isdir(self.sessions_base_dir):
            logger.warning(f"Sessions base directory {self.sessions_base_dir} does not exist or not a directory. Returning empty list.")
            return [] 
            
        for item_name in os.listdir(self.sessions_base_dir):
            try:
                self._validate_session_id_format(item_name) 
            except ValueError:
                logger.debug(f"Skipping item in sessions directory (not a valid session ID format): '{item_name}'")
                continue

            session_folder_path = os.path.join(self.sessions_base_dir, item_name)
            if os.path.isdir(session_folder_path):
                session_id = item_name 
                manifest_path = os.path.join(session_folder_path, SESSION_MANIFEST_FILENAME)
                
                if os.path.exists(manifest_path) and os.path.isfile(manifest_path):
                    try:
                        with open(manifest_path, "r", encoding="utf-8") as f:
                            data = json.load(f)
                        if data.get("session_id") != session_id:
                            logger.warning(f"Manifest session_id '{data.get('session_id')}' does not match folder name '{session_id}' in {manifest_path}. Skipping.")
                            continue
                        sessions.append(WorkSession(**data))
                    except json.JSONDecodeError as e_json:
                        logger.error(f"Error decoding JSON for session manifest {manifest_path} (session {session_id}): {e_json}", exc_info=True)
                    except Exception as e_load: 
                        logger.error(f"Error loading session manifest {manifest_path} (session {session_id}): {e_load}", exc_info=True)
        
        sessions.sort(key=lambda s: s.last_accessed, reverse=True)
        logger.debug(f"Listed {len(sessions)} work sessions.")
        return sessions

    async def get_session(self, session_id: str) -> Optional[WorkSession]:
        manifest_path = self._get_session_manifest_path(session_id) 

        if not os.path.exists(manifest_path) or not os.path.isfile(manifest_path):
            logger.debug(f"Session manifest not found or not a file for session_id {session_id} at {manifest_path}")
            return None
        try:
            with open(manifest_path, "r+", encoding="utf-8") as f: 
                data = json.load(f)
                if data.get("session_id") != session_id:
                    logger.error(f"Manifest session_id '{data.get('session_id')}' does not match requested session_id '{session_id}' in {manifest_path}. Data corruption suspected.")
                    return None 

                original_last_accessed = data.get("last_accessed")
                new_last_accessed = datetime.datetime.now(datetime.timezone.utc).isoformat()
                data['last_accessed'] = new_last_accessed
                
                f.seek(0) 
                json.dump(data, f, indent=4)
                f.truncate() 
                
                logger.debug(f"Accessed session '{session_id}'. Updated last_accessed from {original_last_accessed} to {new_last_accessed}.")
                return WorkSession(**data)
        except json.JSONDecodeError as e_json:
            logger.error(f"Error decoding JSON for session manifest {manifest_path} (session {session_id}): {e_json}", exc_info=True)
            return None
        except Exception as e_io: 
            logger.error(f"Error reading or updating session manifest {manifest_path} for session {session_id}: {e_io}", exc_info=True)
            return None

    async def update_session(self, session_id: str, update_data: UpdateWorkSessionRequest) -> Optional[WorkSession]:
        manifest_path = self._get_session_manifest_path(session_id) 

        if not os.path.exists(manifest_path) or not os.path.isfile(manifest_path):
            logger.warning(f"Cannot update session. Manifest not found for session_id: {session_id}")
            return None
        
        try:
            with open(manifest_path, "r+", encoding="utf-8") as f:
                data = json.load(f)
                if data.get("session_id") != session_id:
                    logger.error(f"Manifest session_id '{data.get('session_id')}' does not match requested session_id '{session_id}' for update. Data corruption suspected.")
                    return None

                current_session = WorkSession(**data)
                update_dict = update_data.model_dump(exclude_unset=True)
                updated_session_data = current_session.model_copy(update=update_dict)
                updated_session_data.last_accessed = datetime.datetime.now(datetime.timezone.utc).isoformat()
                
                f.seek(0)
                json.dump(updated_session_data.model_dump(mode="json"), f, indent=4)
                f.truncate()
                
                logger.info(f"Updated session '{session_id}'. Changes: {update_dict}")
                return updated_session_data
        except json.JSONDecodeError as e_json:
            logger.error(f"Error decoding JSON for session manifest {manifest_path} (session {session_id}) during update: {e_json}", exc_info=True)
            return None
        except Exception as e_io: 
            logger.error(f"Error updating session manifest {manifest_path} for session {session_id}: {e_io}", exc_info=True)
            return None

    async def delete_session(self, session_id: str) -> bool:
        session_folder = self._get_session_folder_path(session_id) 

        if not os.path.exists(session_folder):
            logger.warning(f"Attempted to delete session {session_id}, but folder {session_folder} does not exist.")
            return True 

        if not os.path.isdir(session_folder): 
            logger.error(f"Attempted to delete session {session_id}, but path {session_folder} is not a directory.")
            raise IOError(f"Path '{session_folder}' is not a directory, cannot delete as session.")

        try:
            shutil.rmtree(session_folder)
            logger.info(f"Successfully deleted work session '{session_id}' and its directory: {session_folder}")
            return True
        except OSError as e_os:
            logger.error(f"Error deleting session folder {session_folder} for session {session_id}: {e_os}", exc_info=True)
            return False 

try:
    session_handler = SessionHandler()
except (RuntimeError, ValueError) as e: 
    logger.critical(f"SessionHandler could not be initialized: {e}. Session management will be unavailable.", exc_info=True)
    if settings.ENABLE_WORK_SESSION_MODULE: 
        raise 
    else:
        session_handler = None # type: ignore 
        logger.info("SessionHandler initialization skipped as WORK_SESSION_MODULE is disabled or due to non-critical error during init.")
------

test_session_handler.py


# <PROJECT_ROOT>/tests/unit/core/test_session_handler.py
import pytest
import os
import shutil
import json 
import uuid 
import datetime 
from unittest import mock

from acp_backend.config import settings as app_settings
from acp_backend.core.session_handler import SessionHandler, SESSION_MANIFEST_FILENAME, SESSION_DATA_DIRNAME
from acp_backend.models.work_session_models import CreateWorkSessionRequest

TEST_SESSIONS_BASE_DIR = "./test_acp_work_sessions"

@pytest.fixture(autouse=True)
def manage_test_sessions_dir_auto():
    """Fixture to set WORK_SESSIONS_DIR and clean up TEST_SESSIONS_BASE_DIR."""
    # Clean up before test run starts for this module/session
    if os.path.exists(TEST_SESSIONS_BASE_DIR):
        shutil.rmtree(TEST_SESSIONS_BASE_DIR)
    # Individual tests will handle creation of TEST_SESSIONS_BASE_DIR if SessionHandler.__init__ is SUT
    
    with mock.patch.object(app_settings, 'WORK_SESSIONS_DIR', TEST_SESSIONS_BASE_DIR):
        yield 

    # Clean up after all tests in the module might have run
    if os.path.exists(TEST_SESSIONS_BASE_DIR):
        shutil.rmtree(TEST_SESSIONS_BASE_DIR)

# Test __init__
def test_session_handler_init_success():
    """Test SessionHandler initialization successfully creates base directory."""
    if os.path.exists(TEST_SESSIONS_BASE_DIR): # Ensure clean slate for this specific test
        shutil.rmtree(TEST_SESSIONS_BASE_DIR)
    
    handler = SessionHandler() # This should create TEST_SESSIONS_BASE_DIR
    assert os.path.exists(TEST_SESSIONS_BASE_DIR)
    assert handler.sessions_base_dir == TEST_SESSIONS_BASE_DIR
    print(f"\n[PASSED] test_session_handler_init_success")

@mock.patch('acp_backend.core.session_handler.os.makedirs')
def test_session_handler_init_failure_os_error(mock_os_makedirs_init):
    """Test SessionHandler init raises RuntimeError if base directory creation fails."""
    mock_os_makedirs_init.side_effect = OSError("Init Permission denied")
    
    with pytest.raises(RuntimeError) as excinfo:
        SessionHandler() 
    
    assert "SessionHandler critical failure" in str(excinfo.value)
    assert "Init Permission denied" in str(excinfo.value)
    mock_os_makedirs_init.assert_called_once_with(TEST_SESSIONS_BASE_DIR, exist_ok=True)
    print(f"\n[PASSED] test_session_handler_init_failure_os_error")

# --- create_session Tests ---

@mock.patch('acp_backend.core.session_handler.datetime')
@mock.patch('acp_backend.core.session_handler.uuid')
@mock.patch('acp_backend.core.session_handler.os.makedirs') 
@mock.patch('acp_backend.core.session_handler.open', new_callable=mock.mock_open)
@mock.patch('acp_backend.core.session_handler.json.dump')
async def test_create_session_success(
    mock_json_dump, mock_file_open, mock_os_makedirs_cs, mock_uuid_cs, mock_datetime_cs
):
    """Test successful session creation."""
    # Ensure TEST_SESSIONS_BASE_DIR exists for SessionHandler() init to use real os.makedirs
    # or a non-failing mock if SessionHandler.__init__ itself is not the SUT for this test.
    if not os.path.exists(TEST_SESSIONS_BASE_DIR):
        os.makedirs(TEST_SESSIONS_BASE_DIR, exist_ok=True)

    mock_uuid_cs.uuid4.return_value = uuid.UUID('12345678-1234-5678-1234-567812345678')
    fixed_now_dt = datetime.datetime(2023, 1, 1, 10, 0, 0, tzinfo=datetime.timezone.utc)
    mock_datetime_cs.datetime.now.return_value = fixed_now_dt 
    
    # The mock_os_makedirs_cs is active due to the decorator.
    # The call in SessionHandler's __init__ will use it.
    # The calls in create_session() will also use it.
    # We reset the mock after SessionHandler() instantiation to isolate calls for create_session.
    
    handler = SessionHandler() 
    mock_os_makedirs_cs.reset_mock() # Reset after __init__ call

    request_data = CreateWorkSessionRequest(name="Test Session", description="A test description.")
    created_session = await handler.create_session(request_data)

    expected_session_id = "12345678-1234-5678-1234-567812345678"
    expected_session_folder = os.path.join(TEST_SESSIONS_BASE_DIR, expected_session_id)
    expected_data_folder = os.path.join(expected_session_folder, SESSION_DATA_DIRNAME)
    expected_manifest_path = os.path.join(expected_session_folder, SESSION_MANIFEST_FILENAME)

    assert created_session.session_id == expected_session_id
    assert created_session.name == "Test Session"
    assert created_session.description == "A test description."
    assert created_session.created_at == fixed_now_dt.isoformat() 
    assert created_session.last_accessed == fixed_now_dt.isoformat()

    # These are the calls expected from within create_session()
    expected_makedirs_calls_in_create = [
        mock.call(expected_session_folder, exist_ok=False),
        mock.call(expected_data_folder, exist_ok=True)
    ]
    mock_os_makedirs_cs.assert_has_calls(expected_makedirs_calls_in_create, any_order=False)
    assert mock_os_makedirs_cs.call_count == 2 # Only count calls from create_session

    mock_file_open.assert_called_once_with(expected_manifest_path, "w", encoding="utf-8")
    mock_json_dump.assert_called_once()
    dumped_data = mock_json_dump.call_args[0][0]
    assert dumped_data["session_id"] == expected_session_id
    assert dumped_data["name"] == "Test Session"
    
    print(f"\n[PASSED] test_create_session_success")

@mock.patch('acp_backend.core.session_handler.datetime')
@mock.patch('acp_backend.core.session_handler.uuid')
@mock.patch('acp_backend.core.session_handler.os.makedirs') 
@mock.patch('acp_backend.core.session_handler.open', new_callable=mock.mock_open)
@mock.patch('acp_backend.core.session_handler.json.dump', side_effect=IOError("Failed to write manifest"))
@mock.patch('acp_backend.core.session_handler.shutil.rmtree')
@mock.patch('acp_backend.core.session_handler.os.path.exists') 
async def test_create_session_manifest_write_failure_cleanup(
    mock_os_path_exists_cl, mock_shutil_rmtree_cl, mock_json_dump_failure_cl, 
    mock_file_open_cl, mock_os_makedirs_cl, mock_uuid_cl, mock_datetime_cl
):
    """Test cleanup occurs if writing session manifest fails."""
    if not os.path.exists(TEST_SESSIONS_BASE_DIR): # Ensure base dir for SessionHandler init
         os.makedirs(TEST_SESSIONS_BASE_DIR, exist_ok=True)

    mock_uuid_cl.uuid4.return_value = uuid.UUID('12345678-1234-1234-1234-1234567890ab') 
    mock_datetime_cl.datetime.now.return_value = datetime.datetime(2023,1,1,12,0,0, tzinfo=datetime.timezone.utc)
    
    handler = SessionHandler() # __init__ call to os.makedirs uses mock_os_makedirs_cl
    mock_os_makedirs_cl.reset_mock() # Reset for calls within create_session

    # Configure os.path.exists for the cleanup check
    expected_session_id = "12345678-1234-1234-1234-1234567890ab"
    expected_session_folder = os.path.join(TEST_SESSIONS_BASE_DIR, expected_session_id)
    # Make os.path.exists return True specifically for the session folder to test rmtree call
    mock_os_path_exists_cl.side_effect = lambda path_arg: path_arg == expected_session_folder

    request_data = CreateWorkSessionRequest(name="Cleanup Test", description="Test manifest failure.")

    with pytest.raises(RuntimeError) as excinfo:
        await handler.create_session(request_data)
    
    assert "Could not write session manifest" in str(excinfo.value)
    assert "Failed to write manifest" in str(excinfo.value)
    
    mock_shutil_rmtree_cl.assert_called_once_with(expected_session_folder)
    print(f"\n[PASSED] test_create_session_manifest_write_failure_cleanup")

@mock.patch('acp_backend.core.session_handler.uuid') 
async def test_create_session_folder_already_exists(mock_uuid_fe):
    """Test behavior if session folder (UUID collision) already exists."""
    # SessionHandler.__init__ should succeed using real os.makedirs.
    # The fixture ensures TEST_SESSIONS_BASE_DIR is clean, then __init__ creates it.
    if not os.path.exists(TEST_SESSIONS_BASE_DIR):
         os.makedirs(TEST_SESSIONS_BASE_DIR, exist_ok=True)
        
    handler = SessionHandler() 
    
    fixed_uuid_str = 'abcdef01-abcd-ef01-abcd-ef0123456789'
    mock_uuid_fe.uuid4.return_value = uuid.UUID(fixed_uuid_str)
    expected_session_folder = os.path.join(TEST_SESSIONS_BASE_DIR, fixed_uuid_str)

    request_data = CreateWorkSessionRequest(name="Collision Test", description="Test folder exists.")

    # We want to mock os.makedirs ONLY for the scope of handler.create_session
    # and specifically for the call that creates the session folder.
    with mock.patch('acp_backend.core.session_handler.os.makedirs') as mock_makedirs_inner_create:
        # 1st call (session folder) from create_session: Raise FileExistsError
        # 2nd call (data_folder) from create_session: Should not happen, but mock to allow.
        mock_makedirs_inner_create.side_effect = [
            FileExistsError("Session folder already exists"), 
            None 
        ]

        with pytest.raises(RuntimeError) as excinfo:
            await handler.create_session(request_data)
            
    assert "already exists (UUID collision?)" in str(excinfo.value)
    
    # Check the calls to the inner mock (mock_makedirs_inner_create)
    # This mock is *only* active during the create_session call.
    mock_makedirs_inner_create.assert_called_once_with(expected_session_folder, exist_ok=False)
    print(f"\n[PASSED] test_create_session_folder_already_exists")
------

base.py

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, AsyncGenerator # Dict, Any not directly used

from acp_backend.models.llm_models import (
    LLMModelInfo, LoadModelRequest,
    ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk
)

class BaseLLMBackend(ABC):
    """
    Abstract Base Class for LLM backend implementations.
    Defines the interface that all LLM backends must adhere to.
    """

    @abstractmethod
    async def discover_models(self, models_dir: str) -> List[LLMModelInfo]:
        """
        Discovers available GGUF (or other format) models in the specified directory.
        Args:
            models_dir: The directory to scan for models.
        Returns:
            A list of LLMModelInfo objects for discovered models.
        Raises:
            IOError: If the models_dir cannot be accessed or read.
            # Other backend-specific exceptions might occur.
        """
        pass

    @abstractmethod
    async def load_model(self, request: LoadModelRequest) -> LLMModelInfo:
        """
        Loads a specific LLM model into memory.
        Args:
            request: LoadModelRequest containing model_id or model_path and load parameters.
        Returns:
            LLMModelInfo for the loaded model.
        Raises:
            FileNotFoundError: If the model file cannot be found or path is invalid.
            ValueError: If parameters are invalid (e.g., model_id not found and path not given, or other invalid load params).
            RuntimeError: If the model fails to load for other reasons (e.g., backend internal error, resource exhaustion).
        """
        pass

    @abstractmethod
    async def unload_model(self, model_id: str) -> LLMModelInfo:
        """
        Unloads a specific LLM model from memory.
        Args:
            model_id: The ID of the model to unload.
        Returns:
            LLMModelInfo for the unloaded model (with loaded=False).
        Raises:
            ValueError: If the model_id is not found or not currently loaded.
        """
        pass

    @abstractmethod
    async def get_loaded_models(self) -> List[LLMModelInfo]:
        """
        Retrieves information about all currently loaded LLM models.
        Returns:
            A list of LLMModelInfo objects for loaded models.
        """
        pass

    @abstractmethod
    async def get_model_info(self, model_id: str) -> Optional[LLMModelInfo]:
        """
        Retrieves information for a specific loaded LLM model.
        Args:
            model_id: The ID of the model.
        Returns:
            LLMModelInfo if the model is loaded, None otherwise.
        """
        pass

    @abstractmethod
    async def chat_completion(self, request: ChatCompletionRequest) -> ChatCompletionResponse:
        """
        Processes a chat completion request (non-streaming).
        Args:
            request: ChatCompletionRequest object.
        Returns:
            ChatCompletionResponse object.
        Raises:
            ValueError: If the specified model_id is not loaded or request is invalid.
            RuntimeError: If the completion fails due to a backend or model issue.
        """
        pass

    @abstractmethod
    async def stream_chat_completion(self, request: ChatCompletionRequest) -> AsyncGenerator[ChatCompletionChunk, None]:
        """
        Processes a chat completion request and streams the response.
        Args:
            request: ChatCompletionRequest object (stream flag should ideally be True).
        Yields:
            ChatCompletionChunk objects.
        Raises:
            ValueError: If the specified model_id is not loaded or request is invalid (during setup).
            RuntimeError: If the streaming completion fails due to a backend or model issue (during setup or potentially mid-stream if not handled by yielding an error chunk).
        """
        if False: # pragma: no cover
            yield # type: ignore 
------

llama_cpp.py

import os
import glob
import time
import uuid
import logging
import asyncio # For asyncio.to_thread
from typing import List, Dict, Any, Optional, AsyncGenerator

try:
    from llama_cpp import Llama, LlamaGrammar # type: ignore
    # llama_chat_format might be a module or an object with methods
    import llama_cpp.llama_chat_format as llama_chat_format_module # type: ignore
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    # Define dummy classes if llama_cpp is not available
    class Llama: # type: ignore
        def __init__(self, *args: Any, **kwargs: Any):
            raise NotImplementedError("llama-cpp-python is not installed. Please install it to use LlamaCppBackend.")
        def create_chat_completion(self, *args: Any, **kwargs: Any) -> Any:
            raise NotImplementedError("llama-cpp-python is not installed.")
        def tokenize(self, *args: Any, **kwargs: Any) -> Any:
            raise NotImplementedError("llama-cpp-python is not installed.")
        @property
        def chat_format(self) -> str: return "not_available"
        @property
        def n_ctx(self) -> int: return 0
        # Mock params if accessed, e.g., for n_gpu_layers
        class MockParams:
            n_gpu_layers: int = 0
        params = MockParams()


    class LlamaGrammar: # type: ignore
        @staticmethod
        def from_string(*args: Any, **kwargs: Any) -> Any:
            raise NotImplementedError("llama-cpp-python is not installed.")

    class DummyLlamaChatFormatModule: # type: ignore
        def format_messages(self, messages: List[Dict[str,str]], chat_format_name: str, **kwargs: Any) -> Dict[str, Any]:
            raise NotImplementedError("llama-cpp-python's llama_chat_format.format_messages is not available.")
    
    llama_chat_format_module = DummyLlamaChatFormatModule()


from acp_backend.llm_backends.base import BaseLLMBackend
from acp_backend.models.llm_models import (
    LLMModelInfo, LoadModelRequest,
    ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk,
    ChatMessageInput, ChatCompletionChunkChoice, ChatCompletionChunkDelta,
    ChatCompletionResponseChoice, UsageInfo
)
from acp_backend.config import settings

logger = logging.getLogger(__name__)

class LlamaCppBackend(BaseLLMBackend):
    def __init__(self):
        if not LLAMA_CPP_AVAILABLE:
            logger.error("llama-cpp-python package is not installed. LlamaCppBackend will be non-functional.")
        self.loaded_llms: Dict[str, Llama] = {}
        self.model_configs: Dict[str, LLMModelInfo] = {}
        logger.info("LlamaCppBackend initialized.")

    def _generate_model_id(self, path: str) -> str:
        return os.path.splitext(os.path.basename(path))[0]

    async def discover_models(self, models_dir: str) -> List[LLMModelInfo]:
        discovered: List[LLMModelInfo] = []
        if not await asyncio.to_thread(os.path.isdir, models_dir): # Async check
            logger.warning(f"Models directory not found or not a directory: {models_dir}")
            raise IOError(f"Models directory not found or not a directory: {models_dir}")
        
        logger.info(f"Discovering models in: {models_dir}")
        try:
            model_file_paths = await asyncio.to_thread(glob.glob, os.path.join(models_dir, "*.gguf"))
        except Exception as e_glob:
            logger.error(f"Error during glob operation in {models_dir}: {e_glob}", exc_info=True)
            raise IOError(f"Could not scan models directory {models_dir}: {e_glob}") from e_glob

        for model_file_path in model_file_paths:
            try:
                model_id = self._generate_model_id(model_file_path)
                size_bytes = await asyncio.to_thread(os.path.getsize, model_file_path)
                size_gb = round(size_bytes / (1024**3), 2)
                
                filename_lower = os.path.basename(model_file_path).lower()
                quantization = "unknown"
                # More specific quantization pattern matching
                q_patterns = { 
                    # Order matters for more specific matches, e.g. q3_k_s before q3_k
                    "q2_k": "Q2_K", "q3_k_s": "Q3_K_S", "q3_k_m": "Q3_K_M", "q3_k_l": "Q3_K_L",
                    "q4_0": "Q4_0", "q4_1": "Q4_1", "q4_k_s": "Q4_K_S", "q4_k_m": "Q4_K_M",
                    "q5_0": "Q5_0", "q5_1": "Q5_1", "q5_k_s": "Q5_K_S", "q5_k_m": "Q5_K_M",
                    "q6_k": "Q6_K", "q8_0": "Q8_0", "f16": "F16", "f32": "F32",
                    # Imatrix quants often have "iq" prefix or within the name
                    "iq4_nl": "IQ4_NL", "iq4_xs": "IQ4_XS", "iq3_s": "IQ3_S", "iq3_m": "IQ3_M", 
                    "iq3_xs": "IQ3_XS", "iq3_xxs": "IQ3_XXS", "iq2_s": "IQ2_S", "iq2_m": "IQ2_M",
                    "iq2_xs": "IQ2_XS", "iq2_xxs": "IQ2_XXS", "iq1_s": "IQ1_S", "iq1_m": "IQ1_M"
                }
                fn_base = os.path.splitext(filename_lower)[0] # Filename without .gguf
                for pattern, q_value in q_patterns.items():
                    # Check for patterns like -key, .key, _key, or key as a whole segment
                    if any(f"{sep}{pattern}" in fn_base for sep in ["-", ".", "_"]) or \
                       pattern == fn_base or fn_base.endswith(f"_{pattern}") or fn_base.startswith(f"{pattern}_"):
                        quantization = q_value
                        break
                
                discovered.append(LLMModelInfo(
                    id=model_id, name=os.path.basename(model_file_path), path=model_file_path,
                    size_gb=size_gb, quantization=quantization, loaded=(model_id in self.loaded_llms),
                    backend="llama_cpp"
                ))
            except FileNotFoundError: 
                logger.warning(f"Model file {model_file_path} found by glob but then not accessible.", exc_info=True)
            except Exception as e:
                logger.error(f"Error discovering model {model_file_path}: {e}", exc_info=True)
        logger.info(f"Discovered {len(discovered)} models.")
        return discovered

    async def load_model(self, request: LoadModelRequest) -> LLMModelInfo:
        if not LLAMA_CPP_AVAILABLE:
            raise RuntimeError("llama-cpp-python is not installed. Cannot load model.")

        model_path_to_load = request.model_path
        model_id_to_load = request.model_id

        if model_id_to_load and not model_path_to_load:
            discovered_models = await self.discover_models(settings.MODELS_DIR)
            found_model = next((m for m in discovered_models if m.id == model_id_to_load), None)
            if found_model and found_model.path:
                model_path_to_load = found_model.path
            else:
                raise ValueError(f"Model ID '{model_id_to_load}' not found in discovered models and no model_path provided.")
        
        if not model_path_to_load:
            raise ValueError("No model_path could be determined for loading. Provide a valid model_id or model_path.")

        abs_model_path = await asyncio.to_thread(os.path.abspath, model_path_to_load)
        abs_models_dir = await asyncio.to_thread(os.path.abspath, settings.MODELS_DIR)

        if not abs_model_path.startswith(abs_models_dir):
            logger.error(f"Security Alert: Attempt to load model outside of configured MODELS_DIR. Requested: {abs_model_path}, Allowed base: {abs_models_dir}")
            raise FileNotFoundError(f"Access denied: Model path '{model_path_to_load}' is outside the configured models directory.")

        if not await asyncio.to_thread(os.path.exists, abs_model_path):
            raise FileNotFoundError(f"Model file not found: {abs_model_path}")
        if not await asyncio.to_thread(os.path.isfile, abs_model_path):
            raise FileNotFoundError(f"Model path is not a file: {abs_model_path}")

        final_model_id = model_id_to_load or self._generate_model_id(abs_model_path)

        if final_model_id in self.loaded_llms:
            logger.info(f"Model '{final_model_id}' is already loaded.")
            existing_info = self.model_configs.get(final_model_id)
            if existing_info: return existing_info
            
            logger.warning(f"Model '{final_model_id}' in loaded_llms but not model_configs. Reconstructing info.")
            llm_instance_existing = self.loaded_llms[final_model_id]
            reconstructed_info = LLMModelInfo(
                id=final_model_id, name=os.path.basename(abs_model_path), path=abs_model_path, loaded=True, backend="llama_cpp",
                context_length=getattr(llm_instance_existing, 'n_ctx', 0) # Get n_ctx from existing instance
            )
            self.model_configs[final_model_id] = reconstructed_info
            return reconstructed_info

        logger.info(f"Loading model '{final_model_id}' from {abs_model_path}...")
        try:
            # Llama instantiation is blocking, run in thread
            llm_instance = await asyncio.to_thread(
                Llama, model_path=abs_model_path,
                n_gpu_layers=request.n_gpu_layers if request.n_gpu_layers is not None else settings.LLAMA_CPP_N_GPU_LAYERS,
                n_ctx=request.n_ctx if request.n_ctx is not None else settings.LLAMA_CPP_N_CTX,
                n_batch=request.n_batch if request.n_batch is not None else settings.LLAMA_CPP_N_BATCH,
                chat_format=request.chat_format if request.chat_format is not None else settings.LLAMA_CPP_CHAT_FORMAT,
                verbose=settings.LLAMA_CPP_VERBOSE
            )
            self.loaded_llms[final_model_id] = llm_instance
            
            size_bytes_val = await asyncio.to_thread(os.path.getsize, abs_model_path)
            size_gb = round(size_bytes_val / (1024**3), 2)
            context_length = getattr(llm_instance, 'n_ctx', 0)
            
            # Get quantization info (best effort)
            quantization_value = "unknown"
            try: # Discover again to get potentially updated quantization string from filename pattern matching
                all_discovered = await self.discover_models(settings.MODELS_DIR)
                discovered_model = next((m for m in all_discovered if m.id == final_model_id), None)
                if discovered_model: quantization_value = discovered_model.quantization
            except Exception: pass # Ignore errors in re-discovering for quantization

            model_info = LLMModelInfo(
                id=final_model_id, name=os.path.basename(abs_model_path), path=abs_model_path,
                size_gb=size_gb, quantization=quantization_value, loaded=True, backend="llama_cpp",
                context_length=context_length
            )
            self.model_configs[final_model_id] = model_info
            n_gpu_layers_loaded = getattr(getattr(llm_instance, 'params', None), 'n_gpu_layers', 'N/A')
            logger.info(f"Model '{final_model_id}' loaded. Context: {context_length}, GPU Layers: {n_gpu_layers_loaded}")
            return model_info
        except Exception as e:
            logger.error(f"Failed to load model '{final_model_id}' from {abs_model_path}: {e}", exc_info=True)
            if final_model_id in self.loaded_llms: del self.loaded_llms[final_model_id]
            if final_model_id in self.model_configs: del self.model_configs[final_model_id]
            raise RuntimeError(f"Llama.cpp failed to load model '{final_model_id}': {e}") from e

    async def unload_model(self, model_id: str) -> LLMModelInfo:
        if model_id in self.loaded_llms:
            logger.info(f"Unloading model '{model_id}'...")
            del self.loaded_llms[model_id] 
            model_info_to_return: LLMModelInfo
            if model_id in self.model_configs:
                cached_info = self.model_configs.pop(model_id)
                cached_info.loaded = False
                model_info_to_return = cached_info
            else:
                logger.warning(f"Model '{model_id}' was in loaded_llms but not model_configs during unload.")
                # Fallback: try to find from discovered models
                try:
                    discovered_models = await self.discover_models(settings.MODELS_DIR)
                    found_model = next((m for m in discovered_models if m.id == model_id), None)
                    if found_model:
                        model_info_to_return = LLMModelInfo(id=model_id, name=found_model.name, path=found_model.path, size_gb=found_model.size_gb, quantization=found_model.quantization, loaded=False, backend="llama_cpp")
                    else:
                        model_info_to_return = LLMModelInfo(id=model_id, name=model_id, loaded=False, backend="llama_cpp", path="unknown")
                except Exception: # If discover_models fails
                     model_info_to_return = LLMModelInfo(id=model_id, name=model_id, loaded=False, backend="llama_cpp", path="unknown")
            logger.info(f"Model '{model_id}' unloaded.")
            return model_info_to_return
        else:
            raise ValueError(f"Model '{model_id}' not found or not currently loaded.")

    async def get_loaded_models(self) -> List[LLMModelInfo]:
        return list(self.model_configs.values())

    async def get_model_info(self, model_id: str) -> Optional[LLMModelInfo]:
        return self.model_configs.get(model_id)

    def _get_prompt_tokens_sync(self, llm_instance: Llama, messages: List[Dict[str, str]]) -> int:
        if not (LLAMA_CPP_AVAILABLE and hasattr(llm_instance, 'tokenize') and hasattr(llama_chat_format_module, 'format_messages')):
            logger.warning("_get_prompt_tokens_sync: llama_cpp or required methods missing.")
            return 0 
        try:
            formatted_prompt_obj = llama_chat_format_module.format_messages(
                messages=messages, chat_format_name=llm_instance.chat_format
            )
            prompt_str_for_tokenization = formatted_prompt_obj.get("prompt", "") if isinstance(formatted_prompt_obj, dict) else str(formatted_prompt_obj)
            
            if not prompt_str_for_tokenization:
                logger.warning(f"Could not determine prompt string for tokenization (chat_format: {llm_instance.chat_format}). Using concatenated content.")
                concatenated_content = " ".join([m["content"] for m in messages if m.get("content")])
                return len(llm_instance.tokenize(concatenated_content.encode("utf-8", errors="ignore")))
            return len(llm_instance.tokenize(prompt_str_for_tokenization.encode("utf-8", errors="ignore")))
        except Exception as e:
            logger.error(f"Error tokenizing prompt (chat_format: {llm_instance.chat_format}): {e}", exc_info=True)
            return 0 

    async def chat_completion(self, request: ChatCompletionRequest) -> ChatCompletionResponse:
        if not LLAMA_CPP_AVAILABLE: raise RuntimeError("llama-cpp-python is not installed.")
        if request.model_id not in self.loaded_llms: raise ValueError(f"Model '{request.model_id}' not loaded.")
        
        llm = self.loaded_llms[request.model_id]
        messages_for_llm = [{"role": msg.role, "content": msg.content} for msg in request.messages]
        grammar_instance: Optional[LlamaGrammar] = None
        if request.grammar:
            try:
                grammar_instance = await asyncio.to_thread(LlamaGrammar.from_string, request.grammar, verbose=settings.LLAMA_CPP_VERBOSE)
            except Exception as e: raise ValueError(f"Invalid GBNF grammar: {e}") from e
        
        prompt_tokens_calculated = await asyncio.to_thread(self._get_prompt_tokens_sync, llm, messages_for_llm)

        try:
            completion_result = await asyncio.to_thread(
                llm.create_chat_completion, messages=messages_for_llm, temperature=request.temperature,
                max_tokens=request.max_tokens if request.max_tokens and request.max_tokens > 0 else None,
                stop=request.stop, stream=False, top_p=request.top_p, top_k=request.top_k,
                repeat_penalty=request.repeat_penalty, grammar=grammar_instance
            )
            
            response_choices = [
                ChatCompletionResponseChoice(
                    message=ChatMessageInput(role=choice['message']['role'], content=choice['message']['content']),
                    finish_reason=choice.get('finish_reason'), index=choice.get('index', 0)
                ) for choice in completion_result.get('choices', [])
            ]
            
            usage_from_llm = completion_result.get('usage', {})
            prompt_tokens_llm = usage_from_llm.get('prompt_tokens', prompt_tokens_calculated) 
            completion_tokens_llm = usage_from_llm.get('completion_tokens', 0)
            if not completion_tokens_llm and response_choices:
                content_to_tokenize = response_choices[0].message.content
                completion_tokens_llm = await asyncio.to_thread(lambda: len(llm.tokenize(content_to_tokenize.encode("utf-8", errors="ignore"))))

            total_tokens_llm = usage_from_llm.get('total_tokens', prompt_tokens_llm + completion_tokens_llm)
            usage_info = UsageInfo(prompt_tokens=prompt_tokens_llm, completion_tokens=completion_tokens_llm, total_tokens=total_tokens_llm)

            return ChatCompletionResponse(
                id=completion_result.get('id', f"chatcmpl-{uuid.uuid4()}"), choices=response_choices,
                created=completion_result.get('created', int(time.time())), model=completion_result.get('model', request.model_id), 
                usage=usage_info, object="chat.completion"
            )
        except Exception as e:
            logger.error(f"Chat completion error for model '{request.model_id}': {e}", exc_info=True)
            raise RuntimeError(f"Chat completion failed for model '{request.model_id}': {e}") from e

    async def stream_chat_completion(self, request: ChatCompletionRequest) -> AsyncGenerator[ChatCompletionChunk, None]:
        if not LLAMA_CPP_AVAILABLE: raise RuntimeError("llama-cpp-python is not installed.")
        if request.model_id not in self.loaded_llms: raise ValueError(f"Model '{request.model_id}' not loaded for streaming.")

        llm = self.loaded_llms[request.model_id]
        messages_for_llm = [{"role": msg.role, "content": msg.content} for msg in request.messages]
        grammar_instance: Optional[LlamaGrammar] = None
        if request.grammar:
            try:
                grammar_instance = await asyncio.to_thread(LlamaGrammar.from_string, request.grammar, verbose=settings.LLAMA_CPP_VERBOSE)
            except Exception as e: raise ValueError(f"Invalid GBNF grammar for streaming: {e}") from e

        stream_id = f"chatcmpl-stream-{uuid.uuid4()}"
        stream_created_time = int(time.time())

        # This is the Category 3 item: making this truly async.
        # For Category 1, we acknowledge the sync generator iteration will block.
        logger.warning("LlamaCppBackend.stream_chat_completion is currently using a synchronous generator loop which will block the event loop per chunk. This is a Category 3 item to make fully asynchronous.")
        try:
            completion_stream_sync_gen = llm.create_chat_completion(
                messages=messages_for_llm, temperature=request.temperature,
                max_tokens=request.max_tokens if request.max_tokens and request.max_tokens > 0 else None,
                stop=request.stop, stream=True, top_p=request.top_p, top_k=request.top_k,
                repeat_penalty=request.repeat_penalty, grammar=grammar_instance
            )
            for chunk_dict in completion_stream_sync_gen: # This loop is synchronous
                chunk_choices = []
                for choice_chunk_dict in chunk_dict.get('choices', []):
                    delta_dict = choice_chunk_dict.get('delta', {})
                    chunk_choices.append(ChatCompletionChunkChoice(
                        delta=ChatCompletionChunkDelta(role=delta_dict.get('role'), content=delta_dict.get('content')),
                        finish_reason=choice_chunk_dict.get('finish_reason'), index=choice_chunk_dict.get('index', 0)
                    ))
                if chunk_choices: 
                    yield ChatCompletionChunk(
                        id=chunk_dict.get('id', stream_id), choices=chunk_choices,
                        created=chunk_dict.get('created', stream_created_time), model=chunk_dict.get('model', request.model_id),
                        object="chat.completion.chunk"
                    )
        except Exception as e:
            logger.error(f"Streaming chat completion error for model '{request.model_id}': {e}", exc_info=True)
            raise RuntimeError(f"Streaming chat completion failed for model '{request.model_id}': {e}") from e
------

pie.py

(PLace holder)

------

agent_models.py

from pydantic import BaseModel, Field, ConfigDict # Import ConfigDict
from typing import List, Dict, Optional, Any
import datetime

class AgentToolConfig(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    tool_id: str = Field(..., description="Identifier of the tool (e.g., 'web_search', 'code_interpreter').")
    params: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Tool-specific configuration parameters.")

class AgentConfig(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated
    # json_schema_extra can be added here if needed:
    # model_config = ConfigDict(
    #     extra="forbid",
    #     json_schema_extra = {
    #         "example": {
    #             "agent_id": "my-coder-agent-v1", "name": "Expert Python Coder",
    #             # ... rest of example
    #         }
    #     }
    # )

    agent_id: str = Field(..., description="Unique identifier for this agent configuration.")
    name: str = Field(..., min_length=1, description="User-friendly name for the agent.")
    description: Optional[str] = Field(None, description="Detailed description of the agent's purpose and capabilities.")
    agent_type: str = Field("CodeAgent", description="Type of smolagent (e.g., 'CodeAgent', 'ToolCallingAgent', 'PlannerAgent').")
    system_prompt: Optional[str] = Field(None, description="Custom system prompt to guide the agent's behavior.")
    llm_model_id: str = Field(..., description="ID of the loaded LLM model to be used by this agent.")
    llm_config_overrides: Dict[str, Any] = Field(default_factory=dict, description="Specific LLM parameter overrides for this agent.")
    tools: List[str] = Field(default_factory=list, description="List of tool IDs available to the agent.")
    max_steps: Optional[int] = Field(None, gt=0, description="Maximum number of steps. Must be positive if set.")
    additional_authorized_imports: List[str] = Field(default_factory=list, description="For CodeAgent: Python modules this agent is allowed to import.")
    created_at: str = Field(default_factory=lambda: datetime.datetime.now(datetime.timezone.utc).isoformat(), description="ISO timestamp of creation.")
    updated_at: str = Field(default_factory=lambda: datetime.datetime.now(datetime.timezone.utc).isoformat(), description="ISO timestamp of last update.")

class RunAgentRequest(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    agent_id: str = Field(..., description="ID of the configured agent to run.")
    input_prompt: str = Field(..., min_length=1, description="Primary input, question, or task for the agent.")

class AgentRunStatus(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    run_id: str = Field(..., description="Unique identifier for this agent run.")
    agent_id: str = Field(..., description="ID of the agent that was run.")
    status: str = Field(..., description="Current status of the run.")
    output: Optional[Any] = Field(None, description="Final output or result from the agent.")
    error_message: Optional[str] = Field(None, description="Error message if the run failed.")
    memory_trace_ref: Optional[str] = Field(None, description="Reference to a detailed memory trace or log.")
    start_time: str = Field(default_factory=lambda: datetime.datetime.now(datetime.timezone.utc).isoformat(), description="ISO timestamp of when the run started.")
    end_time: Optional[str] = Field(None, description="ISO timestamp of when the run ended.")

class AgentOutputChunk(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    run_id: str = Field(..., description="Identifier of the agent run this chunk belongs to.")
    type: str = Field(..., description="Type of the output chunk.")
    data: Any = Field(..., description="The actual content of the chunk.")
    timestamp: str = Field(default_factory=lambda: datetime.datetime.now(datetime.timezone.utc).isoformat(), description="ISO timestamp of when the chunk was generated.")
------

common.py

from pydantic import BaseModel, Field, ConfigDict # Import ConfigDict
from typing import Optional, Dict, Any, Literal
import datetime

class StatusResponse(BaseModel):
    model_config = ConfigDict(extra="forbid") # USE ConfigDict

    status: str
    message: Optional[str] = None
    details: Optional[Dict[str, Any]] = None
    # json_schema_extra example removed for brevity, can be added back if needed

class PingResponse(BaseModel):
    model_config = ConfigDict(extra="forbid") # USE ConfigDict

    ping: Literal["pong"] = Field("pong", description="Always returns 'pong'.")
    timestamp: str = Field(default_factory=lambda: datetime.datetime.now(datetime.timezone.utc).isoformat(), description="ISO timestamp of when the ping was processed.")
    # json_schema_extra example removed for brevity
------

llm_models.py

from pydantic import BaseModel, Field, ConfigDict # Import ConfigDict
from typing import List, Optional, Dict, Any, Union, Literal
import time 
import uuid 

class LLMModelInfo(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    id: str = Field(..., description="Unique identifier for the model.")
    name: str = Field(..., description="Display name of the model.")
    path: Optional[str] = Field(None, description="Filesystem path to the model file, if applicable.")
    size_gb: Optional[float] = Field(None, ge=0, description="Size of the model file in GB. Must be non-negative if set.")
    quantization: Optional[str] = Field(None, description="Quantization type (e.g., 'Q4_K_M', 'F16').")
    loaded: bool = Field(False, description="Indicates if the model is currently loaded in memory.")
    backend: str = Field(..., description="Identifier of the backend managing this model.")
    architecture: Optional[str] = Field(None, description="Model architecture (e.g., 'Llama', 'Mistral').")
    context_length: Optional[int] = Field(None, gt=0, description="Maximum context length. Must be positive if set.")
    parameters: Optional[str] = Field(None, description="Number of parameters (e.g., '7B', '70B').")

class LoadModelRequest(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    model_id: Optional[str] = Field(None, description="ID of a previously discovered model to load.")
    model_path: Optional[str] = Field(None, description="Path to the GGUF model file.")
    n_gpu_layers: Optional[int] = Field(None, ge=-1, description="Layers to offload to GPU (-1 for all). Llama.cpp specific.")
    n_ctx: Optional[int] = Field(None, gt=0, description="Context size override. Must be positive. Llama.cpp specific.")
    n_batch: Optional[int] = Field(None, gt=0, description="Batch size for prompt processing. Must be positive. Llama.cpp specific.")
    chat_format: Optional[str] = Field(None, description="Chat format string (e.g., 'llama-2'). Llama.cpp specific.")

class ChatMessageInput(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    role: Literal["system", "user", "assistant"] = Field(..., description="Role of the message sender.")
    content: str = Field(..., description="Content of the message.")

class UsageInfo(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    prompt_tokens: int = Field(..., ge=0)
    completion_tokens: Optional[int] = Field(None, ge=0)
    total_tokens: int = Field(..., ge=0)

class ChatCompletionRequest(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    model_id: str = Field(..., description="ID of the loaded LLM model to use.")
    messages: List[ChatMessageInput] = Field(..., min_length=1, description="Conversation history.")
    temperature: float = Field(0.7, ge=0.0, le=2.0, description="Controls randomness.")
    max_tokens: Optional[int] = Field(512, gt=0, description="Max tokens to generate. Must be positive if set.")
    stream: bool = Field(False, description="If True, stream response via SSE.")
    stop: Optional[Union[str, List[str]]] = Field(None, description="Stop sequence(s).")
    top_p: Optional[float] = Field(None, ge=0.0, le=1.0, description="Nucleus sampling cutoff.")
    top_k: Optional[int] = Field(None, ge=0, description="Consider k most likely tokens.")
    repeat_penalty: Optional[float] = Field(None, ge=0.0, description="Penalty for repeating tokens (1.0 is neutral).")
    grammar: Optional[str] = Field(None, description="GBNF grammar string (Llama.cpp specific).")

class ChatCompletionResponseChoice(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    index: int = Field(0, ge=0)
    message: ChatMessageInput 
    finish_reason: Optional[Literal["stop", "length", "tool_calls", "content_filter", "function_call", "error"]] = Field(None, description="Reason generation stopped.")

class ChatCompletionResponse(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4()}", description="Unique completion ID.")
    object: Literal["chat.completion"] = Field("chat.completion", description="Object type.")
    created: int = Field(default_factory=lambda: int(time.time()), description="Unix timestamp of creation.")
    model: str = Field(..., description="ID of the model used.")
    choices: List[ChatCompletionResponseChoice] = Field(..., min_length=1)
    usage: Optional[UsageInfo] = None

class ChatCompletionChunkDelta(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    role: Optional[Literal["system", "user", "assistant"]] = None
    content: Optional[str] = None 

class ChatCompletionChunkChoice(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    index: int = Field(0, ge=0)
    delta: ChatCompletionChunkDelta
    finish_reason: Optional[Literal["stop", "length", "tool_calls", "content_filter", "function_call", "error"]] = Field(None, description="Reason generation stopped (final chunk).")

class ChatCompletionChunk(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    id: str = Field(..., description="ID of the overall stream.")
    object: Literal["chat.completion.chunk"] = Field("chat.completion.chunk", description="Object type.")
    created: int = Field(..., description="Timestamp of creation.")
    model: str = Field(..., description="ID of the model used.")
    choices: List[ChatCompletionChunkChoice] 
------

work_board_models.py

from pydantic import BaseModel, Field, ConfigDict # Import ConfigDict
from typing import List, Optional
import datetime 

class FileNode(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    name: str = Field(..., min_length=1, description="Name of the file or directory.")
    path: str = Field(..., description="Relative path from the session's data root, using forward slashes.")
    is_dir: bool = Field(..., description="True if this node is a directory, False if it's a file.")
    size_bytes: Optional[int] = Field(None, ge=0, description="Size of the file in bytes. Must be non-negative if set.")
    modified_at: str = Field(..., description="ISO timestamp of the last modification time.")

class ListDirRequest(BaseModel): 
    model_config = ConfigDict(extra="forbid") # Updated

    path: str = Field(".", description="Relative path of the directory to list. Defaults to the root.")

class ReadFileResponse(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    path: str = Field(..., description="Relative path of the file read.")
    content: str = Field(..., description="Content of the file (assumes text).")
    encoding: str = Field("utf-8", description="Encoding of the file content.")

class WriteFileRequest(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    path: str = Field(..., description="Relative path of the file to write/overwrite.")
    content: str = Field(..., description="Content to write to the file.")
    encoding: str = Field("utf-8", description="Encoding to use when writing the file.")

class CreateDirectoryRequest(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    path: str = Field(..., description="Relative path of the directory to create.")

class MoveItemRequest(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    source_path: str = Field(..., description="Current relative path of the item to move/rename.")
    destination_path: str = Field(..., description="New relative path for the item.")
------

work_session_models.py

from pydantic import BaseModel, Field, ConfigDict # Import ConfigDict
from typing import Optional # List, Dict, Any, uuid removed
import datetime 

class WorkSession(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    session_id: str = Field(..., description="Unique identifier for the work session.")
    name: str = Field(..., min_length=1, max_length=100, description="User-defined name for the session.")
    description: Optional[str] = Field(None, max_length=500, description="Brief description of the session's purpose or content.")
    created_at: str = Field(..., description="ISO timestamp of when the session was created.")
    last_accessed: str = Field(..., description="ISO timestamp of when the session was last accessed or modified.")

class CreateWorkSessionRequest(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    name: str = Field(..., min_length=1, max_length=100, description="Name for the new session.")
    description: Optional[str] = Field(None, max_length=500, description="Optional description for the new session.")

class UpdateWorkSessionRequest(BaseModel):
    model_config = ConfigDict(extra="forbid") # Updated

    name: Optional[str] = Field(None, min_length=1, max_length=100, description="New name for the session. Must not be empty if provided.")
    description: Optional[str] = Field(None, max_length=500, description="New description for the session. Can be empty string to clear.")
------

agents.py

import logging
import json 
import datetime # For setting timestamps on agent configs
from typing import List, AsyncGenerator # Dict, Any, Optional removed as not directly used
from fastapi import APIRouter, HTTPException, Body, Path, status
from sse_starlette.sse import EventSourceResponse

from acp_backend.config import settings
from acp_backend.core.agent_executor import (
    agent_executor,
    add_agent_config_to_cache, 
    get_agent_config_from_cache,
    list_agent_configs_from_cache,
    delete_agent_config_from_cache
)
from acp_backend.models.agent_models import (
    AgentConfig, RunAgentRequest, AgentRunStatus, AgentOutputChunk
)
from acp_backend.models.common import StatusResponse

logger = logging.getLogger(__name__)
router = APIRouter()
MODULE_NAME = "Agents Service"
TAG_AGENT_CONFIG = "Agent Configuration"
TAG_AGENT_EXECUTION = "Agent Execution"

def _check_module_enabled():
    if not settings.ENABLE_AGENT_MODULE:
        logger.warning(f"{MODULE_NAME} is disabled in configuration.")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"{MODULE_NAME} is currently disabled."
        )

@router.post(
    "/configs", 
    response_model=AgentConfig, 
    status_code=status.HTTP_201_CREATED,
    summary="Create Agent Configuration",
    tags=[TAG_AGENT_CONFIG]
)
async def create_agent_configuration(config_payload: AgentConfig = Body(...)): # Renamed for clarity
    _check_module_enabled()
    logger.info(f"Request to create agent configuration for agent_id: {config_payload.agent_id}")
    if get_agent_config_from_cache(config_payload.agent_id): 
        logger.warning(f"Agent configuration creation failed: ID '{config_payload.agent_id}' already exists.")
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=f"Agent configuration ID '{config_payload.agent_id}' already exists."
        )
    
    now_iso = datetime.datetime.now(datetime.timezone.utc).isoformat()
    config_payload.created_at = now_iso
    config_payload.updated_at = now_iso
    
    add_agent_config_to_cache(config_payload) 
    logger.info(f"Agent configuration '{config_payload.agent_id}' created and cached.")
    return config_payload

@router.get(
    "/configs/{agent_id}", 
    response_model=AgentConfig, 
    summary="Get Agent Configuration",
    tags=[TAG_AGENT_CONFIG]
)
async def get_agent_configuration(agent_id: str = Path(..., description="The ID of the agent configuration to retrieve.")):
    _check_module_enabled()
    logger.debug(f"Request for agent configuration: {agent_id}")
    config = get_agent_config_from_cache(agent_id)
    if not config:
        logger.warning(f"Agent configuration '{agent_id}' not found in cache.")
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Agent configuration ID '{agent_id}' not found."
        )
    return config

@router.get(
    "/configs", 
    response_model=List[AgentConfig], 
    summary="List All Agent Configurations",
    tags=[TAG_AGENT_CONFIG]
)
async def list_all_agent_configurations():
    _check_module_enabled()
    logger.debug("Request to list all agent configurations.")
    return list_agent_configs_from_cache()

@router.put(
    "/configs/{agent_id}", 
    response_model=AgentConfig, 
    summary="Update Agent Configuration",
    tags=[TAG_AGENT_CONFIG]
)
async def update_agent_configuration(
    agent_id: str = Path(..., description="The ID of the agent configuration to update."),
    config_update_payload: AgentConfig = Body(...)
):
    _check_module_enabled()
    logger.info(f"Request to update agent configuration: {agent_id}")
    if agent_id != config_update_payload.agent_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Path agent_id does not match agent_id in request body."
        )

    existing_config = get_agent_config_from_cache(agent_id)
    if not existing_config:
        logger.warning(f"Agent configuration update failed: ID '{agent_id}' not found.")
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, 
            detail=f"Agent configuration ID '{agent_id}' not found."
        )
    
    config_update_payload.updated_at = datetime.datetime.now(datetime.timezone.utc).isoformat()
    config_update_payload.created_at = existing_config.created_at # Preserve original created_at

    add_agent_config_to_cache(config_update_payload)
    logger.info(f"Agent configuration '{agent_id}' updated in cache.")
    return config_update_payload

@router.delete(
    "/configs/{agent_id}", 
    response_model=None, # For 204 No Content
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Delete Agent Configuration",
    tags=[TAG_AGENT_CONFIG]
)
async def delete_agent_configuration(agent_id: str = Path(..., description="The ID of the agent configuration to delete.")):
    _check_module_enabled()
    logger.info(f"Request to delete agent configuration: {agent_id}")
    if not get_agent_config_from_cache(agent_id):
        logger.warning(f"Agent configuration deletion failed: ID '{agent_id}' not found.")
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Agent configuration ID '{agent_id}' not found."
        )
    delete_agent_config_from_cache(agent_id)
    logger.info(f"Agent configuration '{agent_id}' deleted from cache.")
    return # FastAPI returns 204 No Content by default

@router.post(
    "/run", 
    response_model=AgentRunStatus, 
    summary="Run Agent Task (Non-Streaming)",
    tags=[TAG_AGENT_EXECUTION]
)
async def run_agent_task_endpoint(request: RunAgentRequest = Body(...)): # Renamed
    _check_module_enabled()
    logger.info(f"Request to run agent: {request.agent_id} with input: '{request.input_prompt[:100]}...'")
    try:
        run_status_result = await agent_executor.run_agent_task(request) # Renamed variable
        if run_status_result.status == "failed":
            error_detail = run_status_result.error_message or "Agent task failed with an unspecified error."
            if "not found" in error_detail.lower() and "configuration" in error_detail.lower():
                 logger.warning(f"Agent run failed for '{request.agent_id}': {error_detail}")
                 raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=error_detail)
            elif "not loaded" in error_detail.lower() or "does not exist" in error_detail.lower() and "model" in error_detail.lower():
                 logger.warning(f"Agent run failed for '{request.agent_id}': {error_detail}")
                 raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=error_detail)
            elif "disabled" in error_detail.lower() and "module" in error_detail.lower():
                 logger.warning(f"Agent run failed for '{request.agent_id}': {error_detail}")
                 raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=error_detail)
            else:
                 logger.error(f"Agent run failed for '{request.agent_id}' (run_id: {run_status_result.run_id}): {error_detail}")
                 raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_detail)
        
        logger.info(f"Agent run for '{request.agent_id}' (run_id: {run_status_result.run_id}) completed with status: {run_status_result.status}")
        return run_status_result
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error running agent task for '{request.agent_id}': {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=f"An unexpected error occurred: {str(e)}"
        )

@router.post(
    "/run/stream", 
    summary="Run Agent Task (Streaming SSE)",
    tags=[TAG_AGENT_EXECUTION]
    # response_model=None for EventSourceResponse
)
async def stream_agent_task_outputs_endpoint(request: RunAgentRequest = Body(...)): # Renamed
    _check_module_enabled()
    logger.info(f"Request to stream agent: {request.agent_id} with input: '{request.input_prompt[:100]}...'")

    # Initial checks (moved from agent_executor to router for direct HTTP response)
    agent_config_instance = get_agent_config_from_cache(request.agent_id) # Renamed variable
    if not agent_config_instance:
        logger.warning(f"Agent configuration '{request.agent_id}' not found for streaming.")
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f"Agent configuration for '{request.agent_id}' not found.")

    if settings.ENABLE_LLM_MODULE and llm_manager:
        if not agent_config_instance.llm_model_id:
            error_msg = f"Agent '{request.agent_id}' has no LLM model configured for streaming."
            logger.error(error_msg)
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=error_msg)
        model_info = await llm_manager.get_model_details(agent_config_instance.llm_model_id)
        if not model_info or not model_info.loaded:
            error_msg = f"LLM model '{agent_config_instance.llm_model_id}' for agent '{request.agent_id}' is not loaded or does not exist for streaming."
            logger.error(error_msg)
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=error_msg)
    elif not settings.ENABLE_LLM_MODULE and agent_config_instance.llm_model_id:
        error_msg = f"Agent '{request.agent_id}' requires LLM for streaming, but the LLM module is disabled."
        logger.error(error_msg)
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=error_msg)

    async def event_generator() -> AsyncGenerator[str, None]:
        try:
            async for chunk in agent_executor.stream_agent_task_outputs(request):
                try:
                    json_data = json.dumps(chunk.data)
                except TypeError as te:
                    logger.error(f"Failed to serialize chunk data for SSE (run_id: {chunk.run_id}, type: {chunk.type}): {te}", exc_info=True)
                    error_payload = {"message": f"Data serialization error for chunk type {chunk.type}", "details": str(te)}
                    yield f"event: error\ndata: {json.dumps(error_payload)}\n\n"
                    continue 
                yield f"event: {chunk.type}\ndata: {json_data}\n\n"
            logger.debug(f"Agent SSE stream completed for agent_id: {request.agent_id}")
        except Exception as e_stream: 
            logger.error(f"Error during agent SSE stream execution for '{request.agent_id}': {e_stream}", exc_info=True)
            error_payload = {"message": str(e_stream), "type": "agent_stream_error"}
            yield f"event: error\ndata: {json.dumps(error_payload)}\n\n"
            
    return EventSourceResponse(event_generator(), media_type="text/event-stream")
------

llm_service.py

import logging
import json 
from typing import List, AsyncGenerator # Union, Optional removed
from fastapi import APIRouter, HTTPException, Body, Path, status
from sse_starlette.sse import EventSourceResponse

from acp_backend.config import settings
from acp_backend.core.llm_manager import llm_manager
from acp_backend.models.llm_models import (
    LLMModelInfo, LoadModelRequest,
    ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk
)

logger = logging.getLogger(__name__)
router = APIRouter()
MODULE_NAME = "LLM Service"
TAG_LLM_MODEL_MGMT = "LLM Model Management"
TAG_LLM_CHAT = "LLM Chat Completions"


def _check_module_enabled():
    if not settings.ENABLE_LLM_MODULE:
        logger.warning(f"{MODULE_NAME} is disabled in configuration.")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE, 
            detail=f"{MODULE_NAME} is currently disabled."
        )

@router.get(
    "/models/available", 
    response_model=List[LLMModelInfo], 
    summary="List Discoverable LLM Models",
    tags=[TAG_LLM_MODEL_MGMT]
)
async def list_available_models_endpoint():
    _check_module_enabled()
    try:
        logger.info("Request to list available LLM models.")
        models = await llm_manager.list_available_models()
        logger.info(f"Found {len(models)} available models in '{settings.MODELS_DIR}'.")
        return models
    except IOError as e: 
        logger.error(f"I/O error listing available models: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to access models directory: {str(e)}")
    except Exception as e:
        logger.error(f"Error listing available models: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to list available models: {str(e)}")

@router.get(
    "/models/loaded", 
    response_model=List[LLMModelInfo], 
    summary="List Currently Loaded LLM Models",
    tags=[TAG_LLM_MODEL_MGMT]
)
async def get_loaded_models_endpoint():
    _check_module_enabled()
    try:
        logger.info("Request to list loaded LLM models.")
        loaded_models = await llm_manager.get_loaded_models_info()
        logger.info(f"Found {len(loaded_models)} loaded models.")
        return loaded_models
    except Exception as e:
        logger.error(f"Error getting loaded models: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to get loaded models: {str(e)}")

@router.post(
    "/models/load", 
    response_model=LLMModelInfo, 
    status_code=status.HTTP_200_OK, 
    summary="Load an LLM Model",
    tags=[TAG_LLM_MODEL_MGMT]
)
async def load_llm_model_endpoint(request: LoadModelRequest = Body(...)):
    _check_module_enabled()
    model_identifier = request.model_id or request.model_path or "Unknown"
    logger.info(f"Request to load LLM model: {model_identifier}")
    try:
        loaded_model_info = await llm_manager.load_model(request)
        logger.info(f"Model '{loaded_model_info.id}' load operation completed. Loaded status: {loaded_model_info.loaded}")
        return loaded_model_info
    except FileNotFoundError as e:
        logger.error(f"Model file not found for '{model_identifier}': {e}", exc_info=settings.DEBUG_MODE)
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except ValueError as e: 
        logger.error(f"Value error loading model '{model_identifier}': {e}", exc_info=settings.DEBUG_MODE)
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except RuntimeError as e: 
        logger.error(f"Runtime error loading model '{model_identifier}': {e}", exc_info=settings.DEBUG_MODE)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
    except Exception as e: 
        logger.error(f"Unexpected error loading model '{model_identifier}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"An unexpected error occurred: {str(e)}")

@router.post(
    "/models/unload/{model_id}", 
    response_model=LLMModelInfo, 
    summary="Unload an LLM Model",
    tags=[TAG_LLM_MODEL_MGMT]
)
async def unload_llm_model_endpoint(model_id: str = Path(..., description="The ID of the model to unload.")):
    _check_module_enabled()
    logger.info(f"Request to unload LLM model: {model_id}")
    try:
        unloaded_model_info = await llm_manager.unload_model(model_id)
        logger.info(f"Model '{model_id}' unload operation completed. Loaded status: {unloaded_model_info.loaded}")
        return unloaded_model_info
    except ValueError as e: 
        logger.warning(f"ValueError unloading model '{model_id}': {e}", exc_info=settings.DEBUG_MODE)
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e)) # Model not found or not loaded
    except Exception as e:
        logger.error(f"Error unloading model '{model_id}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to unload model: {str(e)}")

@router.get(
    "/models/{model_id}", 
    response_model=LLMModelInfo, 
    summary="Get Details of a Specific LLM Model",
    tags=[TAG_LLM_MODEL_MGMT]
)
async def get_model_details_endpoint(model_id: str = Path(..., description="The ID of the model.")):
    _check_module_enabled()
    logger.debug(f"Request for model details: {model_id}")
    
    details = await llm_manager.get_model_details(model_id)
    if details:
        logger.debug(f"Model '{model_id}' found in loaded models.")
        return details

    logger.debug(f"Model '{model_id}' not loaded, checking discoverable models.")
    try:
        available_models = await llm_manager.list_available_models()
        discoverable_model = next((m for m in available_models if m.id == model_id), None)
        if discoverable_model:
            logger.debug(f"Model '{model_id}' found in discoverable (but not loaded) models.")
            return discoverable_model
    except IOError: # If list_available_models itself fails
        logger.error(f"Could not check discoverable models for '{model_id}' due to an I/O error.", exc_info=True)
        # Fall through to 404, or raise 500 if this check is critical
    except Exception as e:
        logger.error(f"Error while checking discoverable models for '{model_id}': {e}", exc_info=True)
    
    logger.warning(f"Model ID '{model_id}' not found (neither loaded nor discoverable).")
    raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f"Model ID '{model_id}' not found.")

@router.post(
    "/chat/completions",
    response_model=None, 
    summary="Create Chat Completion (Streaming or Non-Streaming)",
    tags=[TAG_LLM_CHAT]
)
async def create_chat_completion_endpoint(request: ChatCompletionRequest = Body(...)):
    """
    Creates a chat completion for the given conversation messages.
    Supports both streaming (Server-Sent Events) and non-streaming responses.
    Set `stream: true` in the request for streaming.

    - **If `stream: false` (default):** Returns a `ChatCompletionResponse` object.
    - **If `stream: true`:** Returns an `EventSourceResponse` for Server-Sent Events,
      where each event payload is a JSON string of a `ChatCompletionChunk`.
    """
    _check_module_enabled()
    logger.info(f"Chat completion request for model: {request.model_id}, stream: {request.stream}")

    try:
        model_info = await llm_manager.get_model_details(request.model_id)
        if not model_info or not model_info.loaded:
            logger.warning(f"Chat completion failed: Model '{request.model_id}' is not loaded.")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Model '{request.model_id}' is not loaded. Please load the model first."
            )
    except HTTPException: 
        raise
    except Exception as e_check: 
        logger.error(f"Error during pre-check for chat completion with model '{request.model_id}': {e_check}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Initial check failed: {str(e_check)}")

    if request.stream:
        logger.debug(f"Processing streaming chat completion for model '{request.model_id}'.")
        async def event_generator() -> AsyncGenerator[str, None]:
            try:
                async for chunk in llm_manager.stream_process_chat_completion(request):
                    yield f"data: {chunk.model_dump_json()}\n\n"
                logger.debug(f"SSE stream completed for model: {request.model_id}")
            except Exception as e_stream:
                logger.error(f"Error during SSE stream generation for model '{request.model_id}': {e_stream}", exc_info=True)
                error_payload = {"error": {"message": str(e_stream), "type": "stream_error", "code": "STREAM_GENERATION_ERROR"}}
                yield f"event: error\ndata: {json.dumps(error_payload)}\n\n"
        return EventSourceResponse(event_generator(), media_type="text/event-stream")
    else: 
        logger.debug(f"Processing non-streaming chat completion for model '{request.model_id}'.")
        try:
            response_data = await llm_manager.process_chat_completion(request)
            logger.info(f"Non-streaming chat completion successful for model: {request.model_id}, response ID: {response_data.id}")
            return response_data 
        except ValueError as e: 
            logger.warning(f"ValueError during non-streaming chat completion for model '{request.model_id}': {e}", exc_info=settings.DEBUG_MODE)
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
        except RuntimeError as e: 
            logger.error(f"RuntimeError during non-streaming chat completion for model '{request.model_id}': {e}", exc_info=settings.DEBUG_MODE)
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
        except Exception as e: 
            logger.error(f"Unexpected error during non-streaming chat completion for model '{request.model_id}': {e}", exc_info=True)
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"An unexpected error occurred: {str(e)}")
------

system.py

import logging
from fastapi import APIRouter, HTTPException, status
from acp_backend.config import settings, Settings 
from acp_backend.models.common import StatusResponse, PingResponse

logger = logging.getLogger(__name__)
router = APIRouter()
MODULE_NAME = "System Service"
TAG_SYSTEM_INFO = "System Information"

def _check_module_enabled():
    if not settings.ENABLE_SYSTEM_MODULE:
        logger.warning(f"{MODULE_NAME} is disabled in configuration.")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"{MODULE_NAME} is currently disabled."
        )

@router.get(
    "/status", 
    response_model=StatusResponse, 
    summary="Get System Status and Basic Info",
    tags=[TAG_SYSTEM_INFO]
)
async def get_system_status():
    _check_module_enabled()
    logger.debug("Request for system status.")
    
    enabled_modules_status = {
        "llm_service": {"enabled": settings.ENABLE_LLM_MODULE, "status": "ok" if settings.ENABLE_LLM_MODULE else "disabled"},
        "agents_service": {"enabled": settings.ENABLE_AGENT_MODULE, "status": "ok" if settings.ENABLE_AGENT_MODULE else "disabled"},
        "work_sessions_service": {"enabled": settings.ENABLE_WORK_SESSION_MODULE, "status": "ok" if settings.ENABLE_WORK_SESSION_MODULE else "disabled"},
        "work_board_service": {"enabled": settings.ENABLE_WORK_BOARD_MODULE, "status": "ok" if settings.ENABLE_WORK_BOARD_MODULE else "disabled"}
    }
    
    application_status = "ok" 

    return StatusResponse(
        status=application_status, 
        message=f"{settings.APP_NAME} v{settings.APP_VERSION} is running.",
        details={
            "application_name": settings.APP_NAME,
            "application_version": settings.APP_VERSION,
            "debug_mode": settings.DEBUG_MODE,
            "configured_llm_backend": settings.LLM_BACKEND_TYPE if settings.ENABLE_LLM_MODULE else "N/A (LLM Module Disabled)",
            "modules": enabled_modules_status
        }
    )

@router.get(
    "/ping", 
    response_model=PingResponse, 
    summary="Ping System for Liveness Check",
    tags=[TAG_SYSTEM_INFO]
)
async def ping_system():
    _check_module_enabled()
    logger.debug("Ping request received, sending pong.")
    return PingResponse()

@router.get(
    "/config/view", 
    response_model=Settings, 
    summary="View System Configuration (Review for Production)",
    tags=[TAG_SYSTEM_INFO]
)
async def get_system_config_view():
    """
    Displays the current system configuration settings loaded from environment
    variables and defaults. 
    
    **Caution**: This endpoint returns the full `Settings` object. 
    In a production environment, ensure that sensitive values (e.g., API keys like 
    `SERPER_API_KEY`) are not inadvertently exposed. Consider using a separate response 
    model that excludes sensitive fields or implementing proper authorization.
    """
    _check_module_enabled()
    # (Future - Category 3) Add authorization check here if this endpoint is sensitive
    # if not current_user.is_admin: 
    #     raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not authorized.")

    logger.info("Request to view system configuration.")
    return settings
------


work_board.py

import logging
from typing import List # Optional removed
from fastapi import APIRouter, HTTPException, Body, Path, Query, status

from acp_backend.config import settings
from acp_backend.core.fs_manager import fs_manager
from acp_backend.core.session_handler import session_handler
from acp_backend.models.work_board_models import (
    FileNode, ReadFileResponse, WriteFileRequest,
    CreateDirectoryRequest, MoveItemRequest
)
# StatusResponse not used if DELETE returns 204

logger = logging.getLogger(__name__)
router = APIRouter()
MODULE_NAME = "WorkBoard Service"
TAG_WORKBOARD = "WorkBoard File System"

async def _check_module_and_session(session_id: str):
    if not settings.ENABLE_WORK_BOARD_MODULE:
        logger.warning(f"{MODULE_NAME} is disabled in configuration.")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE, 
            detail=f"{MODULE_NAME} is currently disabled."
        )
    try:
        session = await session_handler.get_session(session_id) 
    except ValueError as ve:
        logger.warning(f"WorkBoard: Invalid session ID format '{session_id}': {ve}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"Invalid session ID format: {session_id}")

    if not session:
        logger.warning(f"WorkBoard: Session ID '{session_id}' not found.")
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f"Session ID '{session_id}' not found.")

@router.get(
    "/{session_id}/files", 
    response_model=List[FileNode], 
    summary="List Files and Directories",
    tags=[TAG_WORKBOARD]
)
async def list_files_in_work_board(
    session_id: str = Path(..., description="The ID of the work session."),
    path: str = Query(".", description="Relative directory path. Defaults to root ('.').")
):
    await _check_module_and_session(session_id)
    logger.debug(f"List files for session='{session_id}', path='{path}'")
    try:
        return await fs_manager.list_dir(session_id, path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except NotADirectoryError as e:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except IOError as e:
        logger.error(f"I/O error listing files for session '{session_id}', path '{path}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Could not list files: {str(e)}")
    except Exception as e: 
        logger.error(f"Unexpected error listing files for session '{session_id}', path '{path}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Unexpected error: {str(e)}")

@router.get(
    "/{session_id}/files/content", 
    response_model=ReadFileResponse, 
    summary="Read File Content",
    tags=[TAG_WORKBOARD]
)
async def read_file_content_from_work_board(
    session_id: str = Path(..., description="The ID of the work session."),
    path: str = Query(..., description="Relative path of the file to read.")
):
    await _check_module_and_session(session_id)
    logger.debug(f"Read file for session='{session_id}', path='{path}'")
    try:
        return await fs_manager.read_file(session_id, path)
    except FileNotFoundError as e:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except IsADirectoryError as e:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except ValueError as e: # E.g., UnicodeDecodeError
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except IOError as e:
        logger.error(f"I/O error reading file for session '{session_id}', path '{path}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Could not read file: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error reading file for session '{session_id}', path '{path}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Unexpected error: {str(e)}")

@router.post(
    "/{session_id}/files/content", 
    response_model=FileNode, 
    status_code=status.HTTP_201_CREATED, 
    summary="Write or Overwrite File Content",
    tags=[TAG_WORKBOARD]
)
async def write_file_content_to_work_board(
    session_id: str = Path(..., description="The ID of the work session."),
    request: WriteFileRequest = Body(...)
):
    await _check_module_and_session(session_id)
    logger.info(f"Write file for session='{session_id}', path='{request.path}'")
    try:
        return await fs_manager.write_file(session_id, request)
    except FileNotFoundError as e: # Path resolution failed (e.g. bad session part)
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except NotADirectoryError as e: # Parent path is a file
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except IOError as e:
        logger.error(f"I/O error writing file for session '{session_id}', path '{request.path}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Could not write file: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error writing file for session '{session_id}', path '{request.path}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Unexpected error: {str(e)}")

@router.delete(
    "/{session_id}/files", 
    response_model=None, 
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Delete a File or Directory",
    tags=[TAG_WORKBOARD]
)
async def delete_work_board_item(
    session_id: str = Path(..., description="The ID of the work session."),
    path: str = Query(..., description="Relative path of the item to delete.")
):
    await _check_module_and_session(session_id)
    logger.info(f"Delete item for session='{session_id}', path='{path}'")
    try:
        await fs_manager.delete_item(session_id, path) # Returns True or raises error
        return # FastAPI returns 204
    except FileNotFoundError as e: # Path resolution failed
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except IOError as e: # Actual error during deletion
        logger.error(f"I/O error deleting item for session '{session_id}', path '{path}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Could not delete item: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error deleting item for session '{session_id}', path '{path}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Unexpected error: {str(e)}")

@router.post(
    "/{session_id}/directories", 
    response_model=FileNode, 
    status_code=status.HTTP_201_CREATED, 
    summary="Create a Directory",
    tags=[TAG_WORKBOARD]
)
async def create_work_board_directory(
    session_id: str = Path(..., description="The ID of the work session."),
    request: CreateDirectoryRequest = Body(...)
):
    await _check_module_and_session(session_id)
    logger.info(f"Create directory for session='{session_id}', path='{request.path}'")
    try:
        return await fs_manager.create_directory(session_id, request.path)
    except FileNotFoundError as e: # Path resolution failed
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except FileExistsError as e: # Path exists as a file
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=str(e))
    except IOError as e:
        logger.error(f"I/O error creating directory for session '{session_id}', path '{request.path}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Could not create directory: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error creating directory for session '{session_id}', path '{request.path}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Unexpected error: {str(e)}")

@router.post(
    "/{session_id}/move", 
    response_model=FileNode, 
    summary="Move or Rename a File/Directory",
    tags=[TAG_WORKBOARD]
)
async def move_work_board_item(
    session_id: str = Path(..., description="The ID of the work session."),
    request: MoveItemRequest = Body(...)
):
    await _check_module_and_session(session_id)
    logger.info(f"Move item for session='{session_id}', from '{request.source_path}' to '{request.destination_path}'")
    try:
        return await fs_manager.move_item(session_id, request.source_path, request.destination_path)
    except FileNotFoundError as e: # Source not found, or path resolution failed
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except FileExistsError as e: # Destination exists
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=str(e))
    except NotADirectoryError as e: # Parent of destination is a file
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except IOError as e: 
        logger.error(f"I/O error moving item for session '{session_id}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Could not move item: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error moving item for session '{session_id}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Unexpected error: {str(e)}")
------

work_sessions.py

import logging
from typing import List # Optional, Query removed
from fastapi import APIRouter, HTTPException, Body, Path, status

from acp_backend.config import settings
from acp_backend.core.session_handler import session_handler
from acp_backend.models.work_session_models import (
    WorkSession, CreateWorkSessionRequest, UpdateWorkSessionRequest
)
# StatusResponse not used if DELETE returns 204

logger = logging.getLogger(__name__)
router = APIRouter()
MODULE_NAME = "Work Sessions Service"
TAG_SESSIONS = "Work Session Management"

def _check_module_enabled():
    if not settings.ENABLE_WORK_SESSION_MODULE:
        logger.warning(f"{MODULE_NAME} is disabled in configuration.")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE, 
            detail=f"{MODULE_NAME} is currently disabled."
        )

@router.post(
    "/", 
    response_model=WorkSession, 
    status_code=status.HTTP_201_CREATED, 
    summary="Create a New Work Session",
    tags=[TAG_SESSIONS]
)
async def create_new_work_session(request: CreateWorkSessionRequest = Body(...)):
    _check_module_enabled()
    logger.info(f"Request to create new work session with name: '{request.name}'")
    try:
        new_session = await session_handler.create_session(request)
        logger.info(f"Work session '{new_session.name}' (ID: {new_session.session_id}) created successfully.")
        return new_session
    except RuntimeError as e: 
        logger.error(f"Runtime error creating work session '{request.name}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
    except Exception as e: 
        logger.error(f"Unexpected error creating work session '{request.name}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"An unexpected error occurred: {str(e)}")

@router.get(
    "/", 
    response_model=List[WorkSession], 
    summary="List All Work Sessions",
    tags=[TAG_SESSIONS]
)
async def list_all_work_sessions():
    _check_module_enabled()
    logger.debug("Request to list all work sessions.")
    try:
        sessions = await session_handler.list_sessions()
        logger.debug(f"Returning {len(sessions)} work sessions.")
        return sessions
    except IOError as e: 
        logger.error(f"I/O error listing work sessions: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to list sessions due to I/O error: {str(e)}")
    except Exception as e:
        logger.error(f"Error listing work sessions: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to list work sessions: {str(e)}")

@router.get(
    "/{session_id}", 
    response_model=WorkSession, 
    summary="Get Details of a Specific Work Session",
    tags=[TAG_SESSIONS]
)
async def get_work_session_details(session_id: str = Path(..., description="The unique ID of the work session.")):
    _check_module_enabled()
    logger.debug(f"Request for work session details: {session_id}")
    try:
        session = await session_handler.get_session(session_id)
    except ValueError as ve: 
        logger.warning(f"Get session failed: Invalid session ID format '{session_id}': {ve}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"Invalid session ID format: {session_id}")
    
    if not session:
        logger.warning(f"Work session ID '{session_id}' not found.")
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f"Work Session ID '{session_id}' not found.")
    return session

@router.put(
    "/{session_id}", 
    response_model=WorkSession, 
    summary="Update an Existing Work Session",
    tags=[TAG_SESSIONS]
)
async def update_work_session_details(
    session_id: str = Path(..., description="The ID of the work session to update."),
    update_data: UpdateWorkSessionRequest = Body(...)
):
    _check_module_enabled()
    logger.info(f"Request to update work session: {session_id} with data: {update_data.model_dump(exclude_unset=True)}")
    try:
        updated_session = await session_handler.update_session(session_id, update_data)
    except ValueError as ve: 
        logger.warning(f"Update session failed: Invalid session ID format '{session_id}': {ve}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"Invalid session ID format: {session_id}")

    if not updated_session:
        logger.warning(f"Failed to update work session '{session_id}'. It might not exist or an error occurred.")
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f"Work Session ID '{session_id}' not found or could not be updated.")
    logger.info(f"Work session '{session_id}' updated successfully.")
    return updated_session

@router.delete(
    "/{session_id}", 
    response_model=None, 
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Delete a Work Session",
    tags=[TAG_SESSIONS]
)
async def delete_work_session_by_id(session_id: str = Path(..., description="The ID of the work session to delete.")):
    _check_module_enabled()
    logger.info(f"Request to delete work session: {session_id}")
    try:
        deleted_successfully = await session_handler.delete_session(session_id)
        if not deleted_successfully: 
            # This implies an OSError occurred during deletion, as "not found" returns True.
            # session_handler.delete_session now raises IOError if path is not a dir.
            logger.error(f"Failed to delete work session '{session_id}' due to an internal storage error.")
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Could not delete session '{session_id}'.")
        logger.info(f"Work session '{session_id}' processed for deletion.")
        return # FastAPI returns 204
    except ValueError as ve: 
        logger.warning(f"Delete session failed: Invalid session ID format '{session_id}': {ve}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"Invalid session ID format: {session_id}")
    except IOError as ioe: # Raised by session_handler if path is not a dir before rmtree
        logger.error(f"Delete session failed for '{session_id}': {ioe}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(ioe))
    except Exception as e: 
        logger.error(f"Unexpected error deleting session '{session_id}': {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"An unexpected error occurred: {str(e)}")
------

__init__.py

# This file makes 'acp_backend' a Python package.
# It can be empty.
------

